----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Generate Sketches		#s: 1
Sample Iter: 5	#Pop: 31	#Target: 50	fail_ct: 10209	Time elapsed: 4.05
Sample Initial Population	#s: 51	fail_ct: 16333	Time elapsed: 6.50
GA Iter: 0	Max score: 0.9369	Min score: 0.0391	#Pop: 51	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9999	Min score: 0.9767	#Pop: 128	#M+: 1397	#M-: 0
EvolutionarySearch		#s: 128	Time elapsed: 13.83
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
..............................................................(16, 32, 224, 224, 1, 5, 5, 2, 1, 1)
(16, 32, 224, 224, 4, 3, 3, 2, 1, 1)
# of tasks = 2
Get devices for measurement successfully!
Begin tuning...
|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                                Conv5x5_origin |            - |              - |      0 |
|    1 |                                                   Conv5x5_opt |            - |              - |      0 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: - ms	Trials: 0	Used time : 0 s	Next ID: 0	
.T.T**************************************************************==================================================
No: 1	GFLOPS: 2093.98 / 2093.98	results: MeasureResult(cost:[0.0006], error_no:0, all_cost:5.64, Tstamp:1669889446.46)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,448)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,448)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,8)
        for rx.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,448)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,17)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,448)
              pad_temp.shared = ...
          for rc.1 (0,2)
            for ry.1 (0,5)
              for rc.2 (0,2)
                conv2d_nchw.local = ...
      conv2d_nchw = ...

==================================================
No: 2	GFLOPS: 0.00 / 2093.98	results: MeasureResult(error_type:BuildTimeoutError, error_msg:, all_cost:15.00, Tstamp:1669889446.47)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,98)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,8)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,180)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.3 (0,4)
              for xx_c.3 (0,2)
                for ry.2 (0,5)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,8)
          for xx.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 3	GFLOPS: 142.17 / 2093.98	results: MeasureResult(cost:[0.0090], error_no:0, all_cost:3.41, Tstamp:1669889448.11)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,448)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,28)
      conv2d_nchw.local auto_unroll: 16
      for rc.0 (0,8)
        for ry.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,28)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,293)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,28)
              pad_temp.shared = ...
          for nn_c.3 (0,2)
            for yy_c.3 (0,4)
              for xx_c.3 (0,4)
                for rc.2 (0,4)
                  for rx.2 (0,5)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,4)
          for xx.3 (0,4)
            conv2d_nchw = ...

==================================================
No: 4	GFLOPS: 2878.00 / 2878.00	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:5.30, Tstamp:1669889450.20)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,224)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,112)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,112)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,47)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,112)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,5)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          for xx.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 5	GFLOPS: 1731.58 / 2878.00	results: MeasureResult(cost:[0.0007], error_no:0, all_cost:6.20, Tstamp:1669889452.31)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,224)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,56)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,131)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for yy_c.3 (0,2)
            for rx.2 (0,5)
              for yy_c.4 (0,8)
                conv2d_nchw.local = ...
      for yy.3 (0,16)
        conv2d_nchw = ...

==================================================
No: 6	GFLOPS: 354.82 / 2878.00	results: MeasureResult(cost:[0.0036], error_no:0, all_cost:3.13, Tstamp:1669889453.95)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,784)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        for ax0@ax1@ax2@ax3@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,216)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for ry.2 (0,5)
              for nn_c.4 (0,4)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 7	GFLOPS: 226.23 / 2878.00	results: MeasureResult(cost:[0.0057], error_no:0, all_cost:3.48, Tstamp:1669889455.59)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,448)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,4)
      for rc.0 (0,16)
        for ax0@ax1@ax2@ax3@.0.0 (0,13)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,4)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,1856)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,4)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for yy_c.3 (0,56)
            for ry.2 (0,5)
              for rx.2 (0,5)
                for yy_c.4 (0,2)
                  for xx_c.4 (0,2)
                    conv2d_nchw.local = ...
      for yy.3 (0,112)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 8	GFLOPS: 1357.97 / 2878.00	results: MeasureResult(cost:[0.0009], error_no:0, all_cost:9.11, Tstamp:1669889457.66)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,784)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        for ry.0 (0,5)
          for rx.0 (0,5)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
              vectorize ax0@ax1@ax2@ax3@.1 (0,2)
                kernel.shared = ...
            for ax0@ax1@ax2@ax3@.0.0 (0,16)
              threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
                pad_temp.shared = ...
            for rc.1 (0,2)
              for yy_c.3 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 9	GFLOPS: 904.64 / 2878.00	results: MeasureResult(cost:[0.0014], error_no:0, all_cost:4.06, Tstamp:1669889459.90)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,448)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,16)
      conv2d_nchw.local auto_unroll: 16
      for rc.0 (0,16)
        for ax0@ax1@ax2@ax3@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,16)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,464)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,16)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for yy_c.3 (0,7)
            for ry.2 (0,5)
              for rx.2 (0,5)
                for yy_c.4 (0,2)
                  for xx_c.4 (0,4)
                    conv2d_nchw.local = ...
      for yy.3 (0,14)
        for xx.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 10	GFLOPS: 2803.37 / 2878.00	results: MeasureResult(cost:[0.0005], error_no:0, all_cost:5.48, Tstamp:1669889461.68)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        for ry.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,24)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
              pad_temp.shared = ...
          for rc.1 (0,2)
            for rx.1 (0,5)
              for yy_c.3 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 11	GFLOPS: 1000.46 / 2878.00	results: MeasureResult(cost:[0.0013], error_no:0, all_cost:3.15, Tstamp:1669889463.38)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,448)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,224)
      conv2d_nchw.local auto_unroll: 16
      for rc.0 (0,8)
        for ry.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,224)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,37)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,224)
              pad_temp.shared = ...
          for rx.1 (0,5)
            for xx_c.3 (0,4)
              for rc.2 (0,4)
                conv2d_nchw.local = ...
      for xx.3 (0,4)
        conv2d_nchw = ...

==================================================
No: 12	GFLOPS: 1152.50 / 2878.00	results: MeasureResult(cost:[0.0011], error_no:0, all_cost:3.46, Tstamp:1669889465.49)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,896)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,56)
      conv2d_nchw.local auto_unroll: 16
      for rc.0 (0,8)
        for ax0@ax1@ax2@ax3@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,103)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
            pad_temp.shared = ...
        for rc.1 (0,4)
          for ry.1 (0,5)
            for yy_c.3 (0,2)
              for xx_c.3 (0,2)
                for rx.2 (0,5)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 13	GFLOPS: 809.35 / 2878.00	results: MeasureResult(cost:[0.0016], error_no:0, all_cost:3.39, Tstamp:1669889467.59)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,448)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,112)
      conv2d_nchw.local auto_unroll: 16
      for rc.0 (0,32)
        for ry.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,112)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,17)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,112)
              pad_temp.shared = ...
          for rx.1 (0,5)
            for nn_c.4 (0,4)
              conv2d_nchw.local = ...
      for nn.3 (0,4)
        conv2d_nchw = ...

==================================================
No: 14	GFLOPS: 36.19 / 2878.00	results: MeasureResult(cost:[0.0355], error_no:0, all_cost:3.68, Tstamp:1669889469.41)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,112)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,16)
      conv2d_nchw.local auto_unroll: 16
      for rc.0 (0,32)
        for ry.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,16)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,672)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,16)
              pad_temp.shared = ...
          for nn_c.3 (0,2)
            for yy_c.3 (0,14)
              for rx.2 (0,5)
                for nn_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,8)
        for yy.3 (0,14)
          conv2d_nchw = ...

==================================================
No: 15	GFLOPS: 366.26 / 2878.00	results: MeasureResult(cost:[0.0035], error_no:0, all_cost:3.46, Tstamp:1669889471.01)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,196)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        for ry.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,36)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
              vectorize ax0@ax1@ax2@ax3@.1 (0,2)
                pad_temp.shared = ...
          for nn_c.3 (0,2)
            for rx.2 (0,5)
              for nn_c.4 (0,2)
                for xx_c.4 (0,8)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for xx.3 (0,8)
          conv2d_nchw = ...

==================================================
No: 16	GFLOPS: 660.52 / 2878.00	results: MeasureResult(cost:[0.0019], error_no:0, all_cost:3.33, Tstamp:1669889472.64)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,448)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,28)
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,28)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,92)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,28)
            pad_temp.shared = ...
        for yy_c.3 (0,2)
          for xx_c.3 (0,4)
            for ry.2 (0,5)
              for rx.2 (0,5)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,8)
          conv2d_nchw = ...

==================================================
No: 17	GFLOPS: 1704.59 / 2878.00	results: MeasureResult(cost:[0.0008], error_no:0, all_cost:8.17, Tstamp:1669889474.75)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,112)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,224)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        for ry.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,224)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,18)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,224)
              vectorize ax0@ax1@ax2@ax3@.1 (0,2)
                pad_temp.shared = ...
          for rx.1 (0,5)
            for nn_c.3 (0,2)
              for yy_c.3 (0,2)
                for xx_c.3 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          for xx.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 18	GFLOPS: 1572.55 / 2878.00	results: MeasureResult(cost:[0.0008], error_no:0, all_cost:4.71, Tstamp:1669889476.88)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,784)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,8)
        for ax0@ax1@ax2@ax3@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            vectorize ax0@ax1@ax2@ax3@.1 (0,24)
              pad_temp.shared = ...
        for rc.1 (0,4)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for yy_c.3 (0,2)
                for xx_c.4 (0,4)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 19	GFLOPS: 217.03 / 2878.00	results: MeasureResult(cost:[0.0059], error_no:0, all_cost:4.34, Tstamp:1669889478.53)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,448)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,8)
      conv2d_nchw.local auto_unroll: 16
      for rc.0 (0,8)
        for ry.0 (0,5)
          for ax0@ax1@ax2@ax3@.0.0 (0,3)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,8)
              kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,128)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,8)
              vectorize ax0@ax1@ax2@ax3@.1 (0,8)
                pad_temp.shared = ...
          for yy_c.3 (0,4)
            for xx_c.3 (0,4)
              for rc.2 (0,4)
                for rx.2 (0,5)
                  for xx_c.4 (0,7)
                    conv2d_nchw.local = ...
      for yy.3 (0,4)
        for xx.3 (0,28)
          conv2d_nchw = ...

==================================================
No: 20	GFLOPS: 2852.95 / 2878.00	results: MeasureResult(cost:[0.0005], error_no:0, all_cost:5.18, Tstamp:1669889480.64)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,224)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,112)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,112)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,47)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,112)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for xx_c.3 (0,2)
                for ry.2 (0,5)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          for xx.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 21	GFLOPS: 624.33 / 2878.00	results: MeasureResult(cost:[0.0021], error_no:0, all_cost:4.17, Tstamp:1669889482.87)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,392)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,8)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        for ry.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,4)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
              vectorize ax0@ax1@ax2@ax3@.1 (0,20)
                pad_temp.shared = ...
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for rx.2 (0,5)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 22	GFLOPS: 1864.70 / 2878.00	results: MeasureResult(cost:[0.0007], error_no:0, all_cost:3.53, Tstamp:1669889485.11)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,112)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,448)
      conv2d_nchw.local auto_unroll: 16
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,448)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,21)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,448)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.4 (0,4)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,4)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 23	GFLOPS: 650.16 / 2878.00	results: MeasureResult(cost:[0.0020], error_no:0, all_cost:3.18, Tstamp:1669889486.87)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,784)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
      for rc.0 (0,8)
        for ax0@ax1@ax2@ax3@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,216)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
            pad_temp.shared = ...
        for rc.1 (0,4)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for nn_c.3 (0,2)
                for yy_c.3 (0,2)
                  for xx_c.4 (0,4)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          for xx.3 (0,4)
            conv2d_nchw = ...

==================================================
No: 24	GFLOPS: 2023.25 / 2878.00	results: MeasureResult(cost:[0.0006], error_no:0, all_cost:3.50, Tstamp:1669889488.97)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,512)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,784)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        for ry.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,784)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,5)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,784)
              pad_temp.shared = ...
          for rx.1 (0,5)
            for rc.2 (0,2)
              conv2d_nchw.local = ...
      conv2d_nchw = ...

==================================================
No: 25	GFLOPS: 409.31 / 2878.00	results: MeasureResult(cost:[0.0031], error_no:0, all_cost:4.79, Tstamp:1669889490.60)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,4)
      for ry.0 (0,5)
        for rx.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,16)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
              vectorize ax0@ax1@ax2@ax3@.1 (0,8)
                pad_temp.shared = ...
          for rc.1 (0,4)
            for xx_c.3 (0,2)
              for rc.2 (0,2)
                for xx_c.4 (0,8)
                  conv2d_nchw.local = ...
    for xx.3 (0,16)
      conv2d_nchw = ...

==================================================
No: 26	GFLOPS: 1351.72 / 2878.00	results: MeasureResult(cost:[0.0010], error_no:0, all_cost:3.92, Tstamp:1669889492.70)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,448)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,448)
      conv2d_nchw.local auto_unroll: 16
      for rc.0 (0,16)
        for ry.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,448)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,11)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,448)
              pad_temp.shared = ...
          for rc.1 (0,2)
            for rx.1 (0,5)
              for yy_c.3 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 27	GFLOPS: 425.84 / 2878.00	results: MeasureResult(cost:[0.0030], error_no:0, all_cost:4.21, Tstamp:1669889494.35)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,448)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,16)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        for ax0@ax1@ax2@ax3@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,16)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,78)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,16)
            vectorize ax0@ax1@ax2@ax3@.1 (0,6)
              pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for nn_c.3 (0,2)
              for yy_c.3 (0,7)
                for ry.2 (0,5)
                  for yy_c.4 (0,2)
                    for xx_c.4 (0,2)
                      conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,14)
          for xx.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 28	GFLOPS: 956.14 / 2878.00	results: MeasureResult(cost:[0.0013], error_no:0, all_cost:4.06, Tstamp:1669889496.44)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,392)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
      conv2d_nchw.local auto_unroll: 16
      for rc.0 (0,16)
        for ax0@ax1@ax2@ax3@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,60)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
            vectorize ax0@ax1@ax2@ax3@.1 (0,3)
              pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for nn_c.3 (0,2)
                for yy_c.3 (0,2)
                  for xx_c.3 (0,4)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          for xx.3 (0,4)
            conv2d_nchw = ...

==================================================
No: 29	GFLOPS: 4157.88 / 4157.88	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:6.37, Tstamp:1669889498.29)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,196)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        for rx.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,160)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
              pad_temp.shared = ...
          for rc.1 (0,2)
            for nn_c.3 (0,2)
              for ry.2 (0,5)
                for yy_c.4 (0,4)
                  for xx_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,4)
          for xx.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 30	GFLOPS: 1532.82 / 4157.88	results: MeasureResult(cost:[0.0008], error_no:0, all_cost:12.24, Tstamp:1669889499.89)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1792)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,56)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,8)
      for rx.0 (0,5)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,37)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for nn_c.3 (0,2)
              for rc.2 (0,2)
                for nn_c.4 (0,4)
                  conv2d_nchw.local = ...
    for nn.3 (0,8)
      conv2d_nchw = ...

==================================================
No: 31	GFLOPS: 1869.17 / 4157.88	results: MeasureResult(cost:[0.0007], error_no:0, all_cost:8.65, Tstamp:1669889501.99)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,3136)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,2)
      for ax0@ax1@ax2@ax3@.0.0 (0,7)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,4)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,32)
            pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for rc.2 (0,16)
            for xx_c.4 (0,4)
              conv2d_nchw.local = ...
    for xx.3 (0,4)
      conv2d_nchw = ...

==================================================
No: 32	GFLOPS: 1078.42 / 4157.88	results: MeasureResult(cost:[0.0012], error_no:0, all_cost:8.98, Tstamp:1669889503.64)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        for ry.0 (0,5)
          for rx.0 (0,5)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
              vectorize ax0@ax1@ax2@ax3@.1 (0,2)
                kernel.shared = ...
            for ax0@ax1@ax2@ax3@.0.0 (0,16)
              threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
                pad_temp.shared = ...
            for rc.1 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 33	GFLOPS: 295.02 / 4157.88	results: MeasureResult(cost:[0.0044], error_no:0, all_cost:3.03, Tstamp:1669889505.27)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,448)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,56)
      conv2d_nchw.local auto_unroll: 16
      for rc.0 (0,8)
        for ry.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,147)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
              pad_temp.shared = ...
          for rc.1 (0,4)
            for rx.1 (0,5)
              for yy_c.3 (0,4)
                for xx_c.3 (0,4)
                  conv2d_nchw.local = ...
      for yy.3 (0,4)
        for xx.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 34	GFLOPS: 2000.05 / 4157.88	results: MeasureResult(cost:[0.0006], error_no:0, all_cost:6.16, Tstamp:1669889507.38)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,112)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,68)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for xx_c.3 (0,2)
            for ry.2 (0,5)
              for xx_c.4 (0,7)
                conv2d_nchw.local = ...
      for xx.3 (0,14)
        conv2d_nchw = ...

==================================================
No: 35	GFLOPS: 0.00 / 4157.88	results: MeasureResult(error_type:BuildTimeoutError, error_msg:, all_cost:15.00, Tstamp:1669889507.39)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,98)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,8)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,360)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.1 (0,5)
            for nn_c.3 (0,2)
              for yy_c.3 (0,4)
                for xx_c.3 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,8)
          for xx.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 36	GFLOPS: 5906.61 / 5906.61	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.30, Tstamp:1669889509.03)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for ry.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 37	GFLOPS: 1694.10 / 5906.61	results: MeasureResult(cost:[0.0008], error_no:0, all_cost:5.41, Tstamp:1669889511.13)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,224)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,56)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        for rx.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,66)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
              pad_temp.shared = ...
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for yy_c.4 (0,8)
                conv2d_nchw.local = ...
      for yy.3 (0,16)
        conv2d_nchw = ...

==================================================
No: 38	GFLOPS: 3983.10 / 5906.61	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:3.93, Tstamp:1669889513.22)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,896)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,116)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for yy_c.3 (0,7)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
    for yy.3 (0,14)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 39	GFLOPS: 1841.09 / 5906.61	results: MeasureResult(cost:[0.0007], error_no:0, all_cost:3.97, Tstamp:1669889515.33)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,448)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,56)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,5)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
          vectorize ax0@ax1@ax2@ax3@.1 (0,20)
            pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for nn_c.3 (0,2)
              for xx_c.3 (0,2)
                for xx_c.4 (0,8)
                  conv2d_nchw.local = ...
    for nn.3 (0,2)
      for xx.3 (0,16)
        conv2d_nchw = ...

==================================================
No: 40	GFLOPS: 875.38 / 5906.61	results: MeasureResult(cost:[0.0015], error_no:0, all_cost:3.84, Tstamp:1669889517.46)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,224)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,112)
      conv2d_nchw.local auto_unroll: 16
      for rc.0 (0,32)
        for ry.0 (0,5)
          for rx.0 (0,5)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,112)
              kernel.shared = ...
            for ax0@ax1@ax2@ax3@.0.0 (0,16)
              threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,112)
                vectorize ax0@ax1@ax2@ax3@.1 (0,2)
                  pad_temp.shared = ...
            for nn_c.3 (0,2)
              for yy_c.3 (0,2)
                for xx_c.3 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          for xx.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 41	GFLOPS: 885.18 / 5906.61	results: MeasureResult(cost:[0.0015], error_no:0, all_cost:3.18, Tstamp:1669889519.30)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,392)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      for rc.0 (0,8)
        for ax0@ax1@ax2@ax3@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            vectorize ax0@ax1@ax2@ax3@.1 (0,24)
              pad_temp.shared = ...
        for rc.1 (0,4)
          for rx.1 (0,5)
            for nn_c.3 (0,2)
              for yy_c.3 (0,2)
                for ry.2 (0,5)
                  for xx_c.4 (0,4)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          for xx.3 (0,4)
            conv2d_nchw = ...

==================================================
No: 42	GFLOPS: 3006.84 / 5906.61	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:8.24, Tstamp:1669889521.61)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,512)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,14)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,28)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,8)
        for rx.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,28)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,288)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,28)
              pad_temp.shared = ...
          for rc.1 (0,4)
            for ry.1 (0,5)
              for nn_c.3 (0,2)
                for xx_c.3 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 43	GFLOPS: 495.59 / 5906.61	results: MeasureResult(cost:[0.0026], error_no:0, all_cost:4.13, Tstamp:1669889523.45)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,392)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,16)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        for ax0@ax1@ax2@ax3@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,16)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,60)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,16)
            vectorize ax0@ax1@ax2@ax3@.1 (0,3)
              pad_temp.shared = ...
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for xx_c.3 (0,4)
                for ry.2 (0,5)
                  for nn_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for yy.3 (0,2)
          for xx.3 (0,4)
            conv2d_nchw = ...

==================================================
No: 44	GFLOPS: 2438.01 / 5906.61	results: MeasureResult(cost:[0.0005], error_no:0, all_cost:11.25, Tstamp:1669889525.53)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,392)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,8)
        for ax0@ax1@ax2@ax3@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,12)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            vectorize ax0@ax1@ax2@ax3@.1 (0,16)
              pad_temp.shared = ...
        for rc.1 (0,4)
          for ry.1 (0,5)
            for yy_c.3 (0,2)
              for rx.2 (0,5)
                for nn_c.4 (0,2)
                  for xx_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          for xx.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 45	GFLOPS: 1727.58 / 5906.61	results: MeasureResult(cost:[0.0007], error_no:0, all_cost:5.40, Tstamp:1669889527.67)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,256)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,112)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        for ry.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,112)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,64)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,112)
              pad_temp.shared = ...
          for yy_c.3 (0,14)
            for rc.2 (0,2)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
      for yy.3 (0,14)
        conv2d_nchw = ...

==================================================
No: 46	GFLOPS: 222.64 / 5906.61	results: MeasureResult(cost:[0.0058], error_no:0, all_cost:4.09, Tstamp:1669889529.31)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,112)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
      conv2d_nchw.local auto_unroll: 16
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,13)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
            vectorize ax0@ax1@ax2@ax3@.1 (0,28)
              pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.1 (0,5)
            for nn_c.3 (0,2)
              for yy_c.3 (0,14)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for yy.3 (0,14)
          conv2d_nchw = ...

==================================================
No: 47	GFLOPS: 631.16 / 5906.61	results: MeasureResult(cost:[0.0020], error_no:0, all_cost:3.18, Tstamp:1669889530.95)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,16)
      conv2d_nchw.local auto_unroll: 16
      for rc.0 (0,32)
        for ax0@ax1@ax2@ax3@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,16)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,16)
            vectorize ax0@ax1@ax2@ax3@.1 (0,20)
              pad_temp.shared = ...
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for xx_c.3 (0,4)
              for ry.2 (0,5)
                conv2d_nchw.local = ...
      for nn.3 (0,2)
        for xx.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 48	GFLOPS: 1399.36 / 5906.61	results: MeasureResult(cost:[0.0009], error_no:0, all_cost:5.23, Tstamp:1669889533.19)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,112)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        for rx.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,63)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
              pad_temp.shared = ...
          for xx_c.3 (0,2)
            for ry.2 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,7)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,14)
          conv2d_nchw = ...

==================================================
No: 49	GFLOPS: 11.37 / 5906.61	results: MeasureResult(cost:[0.1130], error_no:0, all_cost:3.91, Tstamp:1669889535.23)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,896)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,4)
      conv2d_nchw.local auto_unroll: 16
      for rc.0 (0,32)
        for ry.0 (0,5)
          for ax0@ax1@ax2@ax3@.0.0 (0,2)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,4)
              kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,1120)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,4)
              pad_temp.shared = ...
          for nn_c.3 (0,2)
            for yy_c.3 (0,14)
              for rx.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for yy.3 (0,14)
          conv2d_nchw = ...

==================================================
No: 50	GFLOPS: 3668.88 / 5906.61	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:6.96, Tstamp:1669889537.08)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,392)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,8)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,8)
        for ry.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,144)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
              pad_temp.shared = ...
          for rc.1 (0,4)
            for yy_c.3 (0,2)
              for rx.2 (0,5)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 51	GFLOPS: 2108.64 / 5906.61	results: MeasureResult(cost:[0.0006], error_no:0, all_cost:5.33, Tstamp:1669889539.20)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,448)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,448)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,8)
        for rx.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,448)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,17)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,448)
              pad_temp.shared = ...
          for rc.1 (0,2)
            for rc.2 (0,2)
              for ry.2 (0,5)
                conv2d_nchw.local = ...
      conv2d_nchw = ...

==================================================
No: 52	GFLOPS: 3517.90 / 5906.61	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:4.61, Tstamp:1669889541.31)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,448)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,112)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,112)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,28)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,112)
          pad_temp.shared = ...
      for nn_c.3 (0,2)
        for xx_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for nn_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,8)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 53	GFLOPS: 3358.90 / 5906.61	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:3.99, Tstamp:1669889543.40)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,512)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,784)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,8)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,784)
          kernel.shared = ...
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,784)
          vectorize ax0@ax1@ax2@ax3@.1 (0,20)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for rc.2 (0,4)
            for ry.2 (0,5)
              conv2d_nchw.local = ...
      conv2d_nchw = ...

==================================================
No: 54	GFLOPS: 1596.69 / 5906.61	results: MeasureResult(cost:[0.0008], error_no:0, all_cost:3.57, Tstamp:1669889545.51)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,896)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,56)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,8)
        for ax0@ax1@ax2@ax3@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,34)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
            vectorize ax0@ax1@ax2@ax3@.1 (0,4)
              pad_temp.shared = ...
        for rc.1 (0,4)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for yy_c.3 (0,2)
                for xx_c.3 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for yy.3 (0,4)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 55	GFLOPS: 866.47 / 5906.61	results: MeasureResult(cost:[0.0015], error_no:0, all_cost:9.62, Tstamp:1669889547.13)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,196)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      for ry.0 (0,5)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,288)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for nn_c.3 (0,2)
              for yy_c.3 (0,8)
                for xx_c.3 (0,2)
                  for yy_c.4 (0,2)
                    for xx_c.4 (0,2)
                      conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,16)
        for xx.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 56	GFLOPS: 2305.00 / 5906.61	results: MeasureResult(cost:[0.0006], error_no:0, all_cost:5.17, Tstamp:1669889549.25)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,224)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,112)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,112)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,47)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,112)
            pad_temp.shared = ...
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              for ry.2 (0,5)
                for rx.2 (0,5)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          for xx.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 57	GFLOPS: 626.64 / 5906.61	results: MeasureResult(cost:[0.0020], error_no:0, all_cost:2.93, Tstamp:1669889550.97)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,196)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      for rc.0 (0,16)
        for rx.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,10)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
              vectorize ax0@ax1@ax2@ax3@.1 (0,16)
                pad_temp.shared = ...
          for rc.1 (0,2)
            for nn_c.3 (0,2)
              for ry.2 (0,5)
                for yy_c.4 (0,4)
                  for xx_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,4)
          for xx.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 58	GFLOPS: 1607.37 / 5906.61	results: MeasureResult(cost:[0.0008], error_no:0, all_cost:6.51, Tstamp:1669889552.60)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,784)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,8)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        for ry.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,3)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
              vectorize ax0@ax1@ax2@ax3@.1 (0,10)
                pad_temp.shared = ...
          for yy_c.3 (0,2)
            for rx.2 (0,5)
              conv2d_nchw.local = ...
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 59	GFLOPS: 1693.63 / 5906.61	results: MeasureResult(cost:[0.0008], error_no:0, all_cost:4.35, Tstamp:1669889554.70)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,8)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,16)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        for ry.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,16)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,40)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,16)
              pad_temp.shared = ...
          for yy_c.3 (0,2)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,4)
        conv2d_nchw = ...

==================================================
No: 60	GFLOPS: 538.56 / 5906.61	results: MeasureResult(cost:[0.0024], error_no:0, all_cost:7.08, Tstamp:1669889556.82)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,512)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,98)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,8)
        for rx.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,98)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,6)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,98)
              vectorize ax0@ax1@ax2@ax3@.1 (0,16)
                pad_temp.shared = ...
          for rc.1 (0,4)
            for ry.1 (0,5)
              for nn_c.3 (0,4)
                for xx_c.3 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 61	GFLOPS: 1988.27 / 5906.61	results: MeasureResult(cost:[0.0006], error_no:0, all_cost:7.27, Tstamp:1669889558.93)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,224)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,8)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,112)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        for rx.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,112)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,42)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,112)
              pad_temp.shared = ...
          for ry.1 (0,5)
            for nn_c.3 (0,2)
              for xx_c.3 (0,2)
                conv2d_nchw.local = ...
      for nn.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 62	GFLOPS: 3149.47 / 5906.61	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:3.06, Tstamp:1669889561.03)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for nn_c.3 (0,4)
            for ry.2 (0,5)
              conv2d_nchw.local = ...
      for nn.3 (0,4)
        conv2d_nchw = ...

==================================================
No: 63	GFLOPS: 492.87 / 5906.61	results: MeasureResult(cost:[0.0026], error_no:0, all_cost:2.51, Tstamp:1669889562.65)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,784)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      for rc.0 (0,4)
        for ry.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,144)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
              pad_temp.shared = ...
          for rc.1 (0,8)
            for nn_c.3 (0,4)
              for rx.2 (0,5)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 64	GFLOPS: 907.57 / 5906.61	results: MeasureResult(cost:[0.0014], error_no:0, all_cost:3.19, Tstamp:1669889564.95)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
      conv2d_nchw.local auto_unroll: 16
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,48)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for xx_c.3 (0,4)
            for ry.2 (0,5)
              conv2d_nchw.local = ...
      for xx.3 (0,4)
        conv2d_nchw = ...

Time elapsed for measurement: 141.40 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 0.46 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Generate Sketches		#s: 1
Sample Initial Population	#s: 77	fail_ct: 4019	Time elapsed: 2.22
GA Iter: 0	Max score: 0.9899	Min score: 0.0070	#Pop: 77	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9998	Min score: 0.9829	#Pop: 128	#M+: 1395	#M-: 0
EvolutionarySearch		#s: 128	Time elapsed: 12.98
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
...............................................................
|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                                Conv5x5_origin |        0.217 |        5906.61 |     64 |
|    1 |                                                   Conv5x5_opt |            - |              - |      0 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: - ms	Trials: 64	Used time : 163 s	Next ID: 1	
.T***************************************************************==================================================
No: 65	GFLOPS: 735.66 / 735.66	results: MeasureResult(cost:[0.0026], error_no:0, all_cost:3.44, Tstamp:1669889605.03)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1808)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      for rc.0 (0,16)
        for ry.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,48)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
              pad_temp.shared = ...
          for rx.1 (0,3)
            for yy_c.3 (0,2)
              for rc.2 (0,2)
                for nn_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 66	GFLOPS: 2644.33 / 2644.33	results: MeasureResult(cost:[0.0007], error_no:0, all_cost:3.64, Tstamp:1669889607.17)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,226)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        for rx.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,3)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
              vectorize ax0@ax1@ax2@ax3@.1 (0,3)
                pad_temp.shared = ...
          for rc.1 (0,2)
            for ry.1 (0,3)
              for nn_c.3 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 67	GFLOPS: 908.69 / 2644.33	results: MeasureResult(cost:[0.0021], error_no:0, all_cost:3.29, Tstamp:1669889608.86)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,7232)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      for ry.0 (0,3)
        for rx.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,2)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
              vectorize ax0@ax1@ax2@ax3@.1 (0,4)
                pad_temp.shared = ...
          for rc.1 (0,2)
            for yy_c.3 (0,2)
              for xx_c.3 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 68	GFLOPS: 406.44 / 2644.33	results: MeasureResult(cost:[0.0046], error_no:0, all_cost:3.04, Tstamp:1669889610.58)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,25538)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
    for rc.0 (0,4)
      for ry.0 (0,3)
        for rx.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
            vectorize ax0@ax1@ax2@ax3@.1 (0,4)
              kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,16)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
              pad_temp.shared = ...
          for rc.1 (0,8)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 69	GFLOPS: 1596.40 / 2644.33	results: MeasureResult(cost:[0.0012], error_no:0, all_cost:2.91, Tstamp:1669889612.27)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,452)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,452)
      conv2d_nchw.local auto_unroll: 16
      for rc.0 (0,32)
        for ry.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,452)
            vectorize ax0@ax1@ax2@ax3@.1 (0,3)
              kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,12)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,452)
              pad_temp.shared = ...
          for rx.2 (0,3)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 70	GFLOPS: 1793.98 / 2644.33	results: MeasureResult(cost:[0.0010], error_no:0, all_cost:3.26, Tstamp:1669889614.57)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,452)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      for rc.0 (0,32)
        for ry.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,32)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
              pad_temp.shared = ...
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for ff_c.3 (0,4)
                for nn_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 71	GFLOPS: 600.95 / 2644.33	results: MeasureResult(cost:[0.0031], error_no:0, all_cost:2.86, Tstamp:1669889616.47)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1808)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      for rc.0 (0,8)
        for ry.0 (0,3)
          for rx.0 (0,3)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
              vectorize ax0@ax1@ax2@ax3@.1 (0,4)
                kernel.shared = ...
            for ax0@ax1@ax2@ax3@.0.0 (0,32)
              threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
                pad_temp.shared = ...
            for rc.1 (0,2)
              for nn_c.3 (0,2)
                for ff_c.3 (0,2)
                  for yy_c.3 (0,2)
                    for rc.2 (0,2)
                      conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 72	GFLOPS: 378.93 / 2644.33	results: MeasureResult(cost:[0.0050], error_no:0, all_cost:2.83, Tstamp:1669889618.19)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,25538)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
    conv2d_nchw.local auto_unroll: 16
    for rc.0 (0,8)
      for ry.0 (0,3)
        for rx.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,8)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
              pad_temp.shared = ...
          for rc.1 (0,4)
            for xx_c.3 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 73	GFLOPS: 1540.66 / 2644.33	results: MeasureResult(cost:[0.0012], error_no:0, all_cost:3.15, Tstamp:1669889620.37)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,452)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,904)
      for rc.0 (0,32)
        for ry.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,904)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,6)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,904)
              pad_temp.shared = ...
          for rx.2 (0,3)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 74	GFLOPS: 3358.60 / 3358.60	results: MeasureResult(cost:[0.0006], error_no:0, all_cost:4.63, Tstamp:1669889622.07)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        for rx.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,17)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
              pad_temp.shared = ...
          for rc.1 (0,2)
            for ry.1 (0,3)
              for nn_c.3 (0,4)
                for xx_c.3 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 75	GFLOPS: 254.17 / 3358.60	results: MeasureResult(cost:[0.0074], error_no:0, all_cost:8.30, Tstamp:1669889623.84)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,452)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      for rx.0 (0,3)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,3)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,113)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for xx_c.3 (0,113)
          for ry.2 (0,3)
            conv2d_nchw.local = ...
    for xx.3 (0,113)
      conv2d_nchw = ...

==================================================
No: 76	GFLOPS: 4320.85 / 4320.85	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:4.19, Tstamp:1669889626.22)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1808)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,452)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,4)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,452)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,25)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,452)
          pad_temp.shared = ...
      for rc.1 (0,8)
        for ry.1 (0,3)
          for ff_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for ff.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 77	GFLOPS: 588.60 / 4320.85	results: MeasureResult(cost:[0.0032], error_no:0, all_cost:3.06, Tstamp:1669889628.15)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      for rc.0 (0,8)
        for ry.0 (0,3)
          for rx.0 (0,3)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
              vectorize ax0@ax1@ax2@ax3@.1 (0,16)
                kernel.shared = ...
            for ax0@ax1@ax2@ax3@.0.0 (0,32)
              threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
                pad_temp.shared = ...
            for rc.1 (0,4)
              for nn_c.3 (0,2)
                for nn_c.4 (0,4)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,8)
        for ff.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 78	GFLOPS: 90.31 / 4320.85	results: MeasureResult(cost:[0.0208], error_no:0, all_cost:2.75, Tstamp:1669889629.92)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,4)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,4)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,1824)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,4)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for yy_c.3 (0,226)
            for rc.2 (0,2)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for ff.3 (0,2)
        for yy.3 (0,226)
          conv2d_nchw = ...

==================================================
No: 79	GFLOPS: 4694.38 / 4694.38	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:3.19, Tstamp:1669889632.08)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,452)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,452)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        for ry.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,452)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,5)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,452)
              pad_temp.shared = ...
          for rx.1 (0,3)
            for ff_c.3 (0,2)
              for nn_c.4 (0,2)
                conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 80	GFLOPS: 281.99 / 4694.38	results: MeasureResult(cost:[0.0067], error_no:0, all_cost:2.51, Tstamp:1669889633.77)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,102152)
  for rc.0 (0,8)
    for ry.0 (0,3)
      for rx.0 (0,3)
        for ax0@ax1@ax2@ax3@.0.0 (0,16)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,32)
          pad_temp.shared = ...
        for rc.1 (0,4)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
  for nn.3 (0,8)
    for ff.3 (0,4)
      conv2d_nchw = ...

==================================================
No: 81	GFLOPS: 931.38 / 4694.38	results: MeasureResult(cost:[0.0020], error_no:0, all_cost:5.38, Tstamp:1669889635.49)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        for ry.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,48)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
              pad_temp.shared = ...
          for rc.1 (0,2)
            for rx.1 (0,3)
              for ff_c.3 (0,2)
                for nn_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 82	GFLOPS: 629.86 / 4694.38	results: MeasureResult(cost:[0.0030], error_no:0, all_cost:5.05, Tstamp:1669889637.44)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1808)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,4)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,4)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,64)
              pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for rc.2 (0,4)
                for ry.2 (0,3)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 83	GFLOPS: 3175.83 / 4694.38	results: MeasureResult(cost:[0.0006], error_no:0, all_cost:3.43, Tstamp:1669889639.60)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,3616)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 64
      for ry.0 (0,3)
        for ax0@ax1@ax2@ax3@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,4)
              pad_temp.shared = ...
        for rc.1 (0,16)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for rc.2 (0,2)
                conv2d_nchw.local = ...
      for nn.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 84	GFLOPS: 2077.31 / 4694.38	results: MeasureResult(cost:[0.0009], error_no:0, all_cost:3.66, Tstamp:1669889641.31)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1808)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,4)
        for rx.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,33)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
              pad_temp.shared = ...
          for rc.1 (0,2)
            for nn_c.3 (0,2)
              for rc.2 (0,4)
                for ry.2 (0,3)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 85	GFLOPS: 4533.01 / 4694.38	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:3.55, Tstamp:1669889642.99)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,452)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,33)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 86	GFLOPS: 5649.61 / 5649.61	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:2.82, Tstamp:1669889644.73)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,226)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,3)
            for rx.1 (0,3)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
      for ff.3 (0,4)
        conv2d_nchw = ...

==================================================
No: 87	GFLOPS: 5054.87 / 5649.61	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:3.41, Tstamp:1669889647.11)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1808)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,226)
      conv2d_nchw.local auto_unroll: 16
      for rc.0 (0,32)
        for ry.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,3)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
              pad_temp.shared = ...
          for rx.1 (0,3)
            for ff_c.3 (0,2)
              for nn_c.4 (0,2)
                conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 88	GFLOPS: 2941.00 / 5649.61	results: MeasureResult(cost:[0.0006], error_no:0, all_cost:2.99, Tstamp:1669889649.09)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,14464)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 16
    for rc.0 (0,4)
      for rx.0 (0,3)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,3)
            for ff_c.3 (0,2)
              for rc.2 (0,4)
                conv2d_nchw.local = ...
    for ff.3 (0,2)
      conv2d_nchw = ...

==================================================
No: 89	GFLOPS: 4347.13 / 5649.61	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:4.24, Tstamp:1669889651.23)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,226)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,452)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        for ry.0 (0,3)
          for rx.0 (0,3)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,452)
              kernel.shared = ...
            for ax0@ax1@ax2@ax3@.0.0 (0,16)
              threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,452)
                pad_temp.shared = ...
            for rc.1 (0,2)
              for nn_c.3 (0,4)
                for ff_c.3 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 90	GFLOPS: 908.60 / 5649.61	results: MeasureResult(cost:[0.0021], error_no:0, all_cost:4.08, Tstamp:1669889653.40)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,226)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,904)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        for ry.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,904)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,2)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,904)
              vectorize ax0@ax1@ax2@ax3@.1 (0,6)
                pad_temp.shared = ...
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for rc.2 (0,2)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        conv2d_nchw = ...

==================================================
No: 91	GFLOPS: 1377.99 / 5649.61	results: MeasureResult(cost:[0.0014], error_no:0, all_cost:3.29, Tstamp:1669889655.57)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,452)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      for rc.0 (0,32)
        for ry.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,12)
              kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,32)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
              pad_temp.shared = ...
          for nn_c.3 (0,2)
            for ff_c.3 (0,4)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 92	GFLOPS: 1741.09 / 5649.61	results: MeasureResult(cost:[0.0011], error_no:0, all_cost:4.39, Tstamp:1669889657.79)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,226)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,226)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
            vectorize ax0@ax1@ax2@ax3@.1 (0,6)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,4)
              for ry.2 (0,3)
                for nn_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,8)
        for ff.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 93	GFLOPS: 853.25 / 5649.61	results: MeasureResult(cost:[0.0022], error_no:0, all_cost:3.31, Tstamp:1669889660.06)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,452)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,226)
    conv2d_nchw.local auto_unroll: 16
    for rc.0 (0,32)
      for ry.0 (0,3)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
          kernel.shared = ...
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
          vectorize ax0@ax1@ax2@ax3@.1 (0,16)
            pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for xx_c.3 (0,2)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 94	GFLOPS: 745.39 / 5649.61	results: MeasureResult(cost:[0.0025], error_no:0, all_cost:2.67, Tstamp:1669889661.84)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1808)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,16)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rc.2 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 95	GFLOPS: 3738.85 / 5649.61	results: MeasureResult(cost:[0.0005], error_no:0, all_cost:2.77, Tstamp:1669889663.54)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,226)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
            pad_temp.shared = ...
        for ff_c.3 (0,2)
          for xx_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,2)
          for xx.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 96	GFLOPS: 693.91 / 5649.61	results: MeasureResult(cost:[0.0027], error_no:0, all_cost:6.63, Tstamp:1669889665.50)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        for ry.0 (0,3)
          for rx.0 (0,3)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
              kernel.shared = ...
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
              vectorize ax0@ax1@ax2@ax3@.1 (0,16)
                pad_temp.shared = ...
            for rc.1 (0,2)
              for nn_c.3 (0,4)
                for xx_c.3 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 97	GFLOPS: 3430.60 / 5649.61	results: MeasureResult(cost:[0.0005], error_no:0, all_cost:3.53, Tstamp:1669889667.87)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,226)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,8)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,452)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        for ry.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,452)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,9)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,452)
              pad_temp.shared = ...
          for rx.1 (0,3)
            for ff_c.3 (0,2)
              for nn_c.4 (0,2)
                conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 98	GFLOPS: 5537.68 / 5649.61	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.79, Tstamp:1669889669.76)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,452)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,226)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,33)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,3)
            for rx.1 (0,3)
              for nn_c.3 (0,4)
                for yy_c.3 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 99	GFLOPS: 370.15 / 5649.61	results: MeasureResult(cost:[0.0051], error_no:0, all_cost:13.16, Tstamp:1669889671.94)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,12769)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,4)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,4)
        for ax0@ax1@ax2@ax3@.0.0 (0,72)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,4)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,512)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,4)
            pad_temp.shared = ...
        for rc.1 (0,4)
          for ry.1 (0,3)
            for rx.1 (0,3)
              for rc.2 (0,2)
                for nn_c.4 (0,16)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,16)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 100	GFLOPS: 344.65 / 5649.61	results: MeasureResult(cost:[0.0055], error_no:0, all_cost:2.92, Tstamp:1669889673.65)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,226)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 16
      for rc.0 (0,16)
        for ry.0 (0,3)
          for rx.0 (0,3)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
              vectorize ax0@ax1@ax2@ax3@.1 (0,2)
                kernel.shared = ...
            for ax0@ax1@ax2@ax3@.0.0 (0,64)
              threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
                pad_temp.shared = ...
            for rc.1 (0,2)
              for ff_c.3 (0,4)
                for nn_c.4 (0,16)
                  conv2d_nchw.local = ...
      for nn.3 (0,16)
        for ff.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 101	GFLOPS: 1661.27 / 5649.61	results: MeasureResult(cost:[0.0011], error_no:0, all_cost:2.75, Tstamp:1669889675.35)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,226)
      for rc.0 (0,16)
        for rx.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,2)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
              vectorize ax0@ax1@ax2@ax3@.1 (0,6)
                pad_temp.shared = ...
          for ry.1 (0,3)
            for rc.2 (0,2)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
      for ff.3 (0,4)
        conv2d_nchw = ...

==================================================
No: 102	GFLOPS: 1889.67 / 5649.61	results: MeasureResult(cost:[0.0010], error_no:0, all_cost:3.26, Tstamp:1669889677.58)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1808)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,226)
      for rc.0 (0,16)
        for rx.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,5)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
              pad_temp.shared = ...
          for ry.1 (0,3)
            for rc.2 (0,2)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
      for ff.3 (0,4)
        conv2d_nchw = ...

==================================================
No: 103	GFLOPS: 2247.27 / 5649.61	results: MeasureResult(cost:[0.0008], error_no:0, all_cost:3.19, Tstamp:1669889679.84)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,3616)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 16
      for rc.0 (0,4)
        for ax0@ax1@ax2@ax3@.0.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,49)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rc.2 (0,4)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 104	GFLOPS: 850.37 / 5649.61	results: MeasureResult(cost:[0.0022], error_no:0, all_cost:4.13, Tstamp:1669889681.64)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,452)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        for ry.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,8)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
              vectorize ax0@ax1@ax2@ax3@.1 (0,6)
                pad_temp.shared = ...
          for rx.1 (0,3)
            for ff_c.3 (0,2)
              for nn_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 105	GFLOPS: 40.18 / 5649.61	results: MeasureResult(cost:[0.0469], error_no:0, all_cost:3.54, Tstamp:1669889683.79)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,2)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        for ry.0 (0,3)
          for ax0@ax1@ax2@ax3@.0.0 (0,3)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,2)
              kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,1808)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,2)
              pad_temp.shared = ...
          for ff_c.3 (0,2)
            for yy_c.3 (0,226)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,2)
          for yy.3 (0,226)
            conv2d_nchw = ...

==================================================
No: 106	GFLOPS: 1215.04 / 5649.61	results: MeasureResult(cost:[0.0015], error_no:0, all_cost:2.91, Tstamp:1669889685.49)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1808)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,8)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,4)
        for ax0@ax1@ax2@ax3@.0.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,97)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for nn_c.3 (0,2)
            for rc.2 (0,4)
              for ry.2 (0,3)
                for rx.2 (0,3)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 107	GFLOPS: 4396.33 / 5649.61	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:3.87, Tstamp:1669889687.79)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,452)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,8)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,452)
        vectorize ax0@ax1@ax2@ax3@.1 (0,6)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,452)
          pad_temp.shared = ...
      for rc.1 (0,4)
        for ry.1 (0,3)
          for rx.1 (0,3)
            for ff_c.3 (0,4)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for ff.3 (0,4)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 108	GFLOPS: 1015.65 / 5649.61	results: MeasureResult(cost:[0.0019], error_no:0, all_cost:4.22, Tstamp:1669889690.03)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,226)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        for ry.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
            vectorize ax0@ax1@ax2@ax3@.1 (0,3)
              kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,16)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
              vectorize ax0@ax1@ax2@ax3@.1 (0,2)
                pad_temp.shared = ...
          for rc.1 (0,2)
            for ff_c.3 (0,2)
              for xx_c.3 (0,2)
                for rx.2 (0,3)
                  for nn_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,2)
          for xx.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 109	GFLOPS: 1737.24 / 5649.61	results: MeasureResult(cost:[0.0011], error_no:0, all_cost:3.48, Tstamp:1669889691.82)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1808)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,8)
        for ry.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,64)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
              pad_temp.shared = ...
          for rc.1 (0,2)
            for nn_c.3 (0,2)
              for ff_c.3 (0,2)
                for yy_c.3 (0,2)
                  for rc.2 (0,2)
                    for rx.2 (0,3)
                      conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 110	GFLOPS: 5706.45 / 5706.45	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:2.47, Tstamp:1669889693.52)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1808)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,226)
      conv2d_nchw.local auto_unroll: 16
      for rc.0 (0,16)
        for ry.0 (0,3)
          for rx.0 (0,3)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
              kernel.shared = ...
            for ax0@ax1@ax2@ax3@.0.0 (0,4)
              threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
                pad_temp.shared = ...
            for rc.1 (0,2)
              for yy_c.3 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 111	GFLOPS: 547.10 / 5706.45	results: MeasureResult(cost:[0.0034], error_no:0, all_cost:2.63, Tstamp:1669889695.39)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,25538)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,16)
    for rc.0 (0,4)
      for rx.0 (0,3)
        for ax0@ax1@ax2@ax3@.0.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,16)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,16)
            vectorize ax0@ax1@ax2@ax3@.1 (0,4)
              pad_temp.shared = ...
        for rc.1 (0,8)
          for ry.1 (0,3)
            for ff_c.4 (0,2)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
    for ff.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 112	GFLOPS: 0.00 / 5706.45	results: MeasureResult(error_type:BuildTimeoutError, error_msg:, all_cost:15.00, Tstamp:1669889695.40)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,226)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      for ry.0 (0,3)
        for rx.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,12)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
              vectorize ax0@ax1@ax2@ax3@.1 (0,19)
                pad_temp.shared = ...
          for ff_c.3 (0,2)
            for xx_c.3 (0,113)
              for rc.2 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
    for ff.3 (0,2)
      for xx.3 (0,226)
        conv2d_nchw = ...

==================================================
No: 113	GFLOPS: 5301.08 / 5706.45	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:3.69, Tstamp:1669889697.77)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,452)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,904)
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,904)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,904)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,3)
          for ff_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,2)
      for ff.3 (0,4)
        conv2d_nchw = ...

==================================================
No: 114	GFLOPS: 1924.37 / 5706.45	results: MeasureResult(cost:[0.0010], error_no:0, all_cost:2.76, Tstamp:1669889699.68)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1808)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      for rc.0 (0,2)
        for ry.0 (0,3)
          for ax0@ax1@ax2@ax3@.0.0 (0,2)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
              kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,66)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
              pad_temp.shared = ...
          for rc.1 (0,8)
            for rc.2 (0,2)
              for rx.2 (0,3)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for ff.3 (0,4)
        conv2d_nchw = ...

==================================================
No: 115	GFLOPS: 806.17 / 5706.45	results: MeasureResult(cost:[0.0023], error_no:0, all_cost:2.81, Tstamp:1669889701.38)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,452)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      for rc.0 (0,32)
        for ry.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,16)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
              vectorize ax0@ax1@ax2@ax3@.1 (0,3)
                pad_temp.shared = ...
          for ff_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 116	GFLOPS: 583.32 / 5706.45	results: MeasureResult(cost:[0.0032], error_no:0, all_cost:3.04, Tstamp:1669889703.09)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    for rc.0 (0,8)
      for ry.0 (0,3)
        for rx.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,32)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
              pad_temp.shared = ...
          for rc.1 (0,4)
            for nn_c.3 (0,2)
              for ff_c.3 (0,2)
                for nn_c.4 (0,4)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,8)
      for ff.3 (0,4)
        conv2d_nchw = ...

==================================================
No: 117	GFLOPS: 1187.18 / 5706.45	results: MeasureResult(cost:[0.0016], error_no:0, all_cost:2.98, Tstamp:1669889705.28)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1808)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,49)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for yy_c.3 (0,2)
              for rc.2 (0,2)
                for nn_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 118	GFLOPS: 2196.51 / 5706.45	results: MeasureResult(cost:[0.0009], error_no:0, all_cost:4.80, Tstamp:1669889707.07)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,25538)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,4)
        for ax0@ax1@ax2@ax3@.0.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,4)
          for ry.1 (0,3)
            for rx.1 (0,3)
              for rc.2 (0,2)
                conv2d_nchw.local = ...
      conv2d_nchw = ...

==================================================
No: 119	GFLOPS: 2567.82 / 5706.45	results: MeasureResult(cost:[0.0007], error_no:0, all_cost:4.89, Tstamp:1669889709.48)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,226)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,226)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,49)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for nn_c.4 (0,4)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,8)
        for ff.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 120	GFLOPS: 5241.66 / 5706.45	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:3.82, Tstamp:1669889711.41)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,226)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,8)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,33)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
            pad_temp.shared = ...
        for rc.1 (0,4)
          for ry.1 (0,3)
            for rx.1 (0,3)
              for ff_c.3 (0,4)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for ff.3 (0,4)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 121	GFLOPS: 2008.63 / 5706.45	results: MeasureResult(cost:[0.0009], error_no:0, all_cost:3.37, Tstamp:1669889713.10)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1808)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,226)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,8)
      for rx.0 (0,3)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,3)
            for nn_c.3 (0,2)
              for ff_c.3 (0,2)
                for yy_c.3 (0,2)
                  for rc.2 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,2)
      for ff.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 122	GFLOPS: 7992.17 / 7992.17	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.99, Tstamp:1669889714.81)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 123	GFLOPS: 1992.86 / 7992.17	results: MeasureResult(cost:[0.0009], error_no:0, all_cost:3.47, Tstamp:1669889717.00)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,226)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        for rx.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
            vectorize ax0@ax1@ax2@ax3@.1 (0,4)
              kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,9)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
              pad_temp.shared = ...
          for rc.1 (0,2)
            for ry.1 (0,3)
              for nn_c.3 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 124	GFLOPS: 920.88 / 7992.17	results: MeasureResult(cost:[0.0020], error_no:0, all_cost:2.61, Tstamp:1669889718.84)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,904)
    for rc.0 (0,32)
      for ry.0 (0,3)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,904)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,904)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              conv2d_nchw.local = ...
    for ff.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 125	GFLOPS: 1044.62 / 7992.17	results: MeasureResult(cost:[0.0018], error_no:0, all_cost:3.23, Tstamp:1669889721.25)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,452)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      for rc.0 (0,16)
        for ry.0 (0,3)
          for rx.0 (0,3)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
              kernel.shared = ...
            for ax0@ax1@ax2@ax3@.0.0 (0,32)
              threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
                pad_temp.shared = ...
            for rc.1 (0,2)
              for nn_c.3 (0,2)
                for ff_c.3 (0,4)
                  for nn_c.4 (0,2)
                    for yy_c.4 (0,2)
                      conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 126	GFLOPS: 1071.59 / 7992.17	results: MeasureResult(cost:[0.0018], error_no:0, all_cost:3.13, Tstamp:1669889723.15)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,452)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      for rc.0 (0,16)
        for ry.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,64)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
              pad_temp.shared = ...
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for ff_c.3 (0,4)
                for rc.2 (0,2)
                  for nn_c.4 (0,2)
                    for yy_c.4 (0,2)
                      conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 127	GFLOPS: 4806.70 / 7992.17	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:3.41, Tstamp:1669889725.33)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,3616)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,226)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,4)
      for ax0@ax1@ax2@ax3@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
          pad_temp.shared = ...
      for rc.1 (0,8)
        for ff_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for ff_c.4 (0,2)
                conv2d_nchw.local = ...
    for ff.3 (0,4)
      conv2d_nchw = ...

==================================================
No: 128	GFLOPS: 1307.19 / 7992.17	results: MeasureResult(cost:[0.0014], error_no:0, all_cost:3.44, Tstamp:1669889727.51)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,51076)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
    conv2d_nchw.local auto_unroll: 16
    for rc.0 (0,4)
      for ry.0 (0,3)
        for ax0@ax1@ax2@ax3@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rc.2 (0,4)
            for rx.2 (0,3)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for xx.3 (0,2)
      conv2d_nchw = ...

Time elapsed for measurement: 146.78 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 0.51 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Iter: 5	#Pop: 26	#Target: 50	fail_ct: 10214	Time elapsed: 4.20
Sample Initial Population	#s: 57	fail_ct: 18375	Time elapsed: 7.60
GA Iter: 0	Max score: 0.9813	Min score: 0.0145	#Pop: 57	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9999	Min score: 0.9809	#Pop: 128	#M+: 1386	#M-: 0
EvolutionarySearch		#s: 128	Time elapsed: 13.94
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************==================================================
No: 129	GFLOPS: 2931.82 / 5906.61	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:4.97, Tstamp:1669889769.66)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,896)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,56)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,8)
        for rx.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,80)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
              pad_temp.shared = ...
          for rc.1 (0,2)
            for yy_c.3 (0,2)
              for rc.2 (0,2)
                for ry.2 (0,5)
                  for nn_c.4 (0,4)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 130	GFLOPS: 429.32 / 5906.61	results: MeasureResult(cost:[0.0030], error_no:0, all_cost:4.32, Tstamp:1669889771.31)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,448)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,16)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        for ax0@ax1@ax2@ax3@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,16)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,78)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,16)
            vectorize ax0@ax1@ax2@ax3@.1 (0,6)
              pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for nn_c.3 (0,2)
              for yy_c.3 (0,7)
                for ry.2 (0,5)
                  for yy_c.4 (0,2)
                    for xx_c.4 (0,2)
                      conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,14)
          for xx.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 131	GFLOPS: 177.25 / 5906.61	results: MeasureResult(cost:[0.0072], error_no:0, all_cost:8.60, Tstamp:1669889773.12)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,98)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,8)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        for ry.0 (0,5)
          for rx.0 (0,5)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
              kernel.shared = ...
            for ax0@ax1@ax2@ax3@.0.0 (0,128)
              threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
                vectorize ax0@ax1@ax2@ax3@.1 (0,2)
                  pad_temp.shared = ...
            for nn_c.3 (0,2)
              for yy_c.3 (0,4)
                for xx_c.3 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,8)
          for xx.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 132	GFLOPS: 465.23 / 5906.61	results: MeasureResult(cost:[0.0028], error_no:0, all_cost:3.16, Tstamp:1669889774.78)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,448)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,8)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,56)
      for rc.0 (0,32)
        for rx.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,33)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
              pad_temp.shared = ...
          for ry.1 (0,5)
            for nn_c.3 (0,2)
              for yy_c.3 (0,2)
                conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 133	GFLOPS: 271.57 / 5906.61	results: MeasureResult(cost:[0.0047], error_no:0, all_cost:3.51, Tstamp:1669889776.43)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,896)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,4)
    conv2d_nchw.local auto_unroll: 16
    for rc.0 (0,16)
      for ax0@ax1@ax2@ax3@.0.0 (0,13)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,4)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,928)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,4)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for yy_c.3 (0,56)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
    for yy.3 (0,112)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 134	GFLOPS: 1227.28 / 5906.61	results: MeasureResult(cost:[0.0010], error_no:0, all_cost:5.22, Tstamp:1669889778.06)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,896)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,224)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      for ry.0 (0,5)
        for rx.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,224)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,8)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,224)
              pad_temp.shared = ...
          for xx_c.3 (0,2)
            for rc.2 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 135	GFLOPS: 3920.27 / 5906.61	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:11.75, Tstamp:1669889780.44)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,448)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,112)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      for ry.0 (0,5)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,112)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,37)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,112)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for xx_c.3 (0,4)
            for rc.2 (0,2)
              for nn_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for xx.3 (0,4)
        conv2d_nchw = ...

==================================================
No: 136	GFLOPS: 414.60 / 5906.61	results: MeasureResult(cost:[0.0031], error_no:0, all_cost:3.24, Tstamp:1669889782.29)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,784)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,16)
      conv2d_nchw.local auto_unroll: 16
      for rc.0 (0,8)
        for ax0@ax1@ax2@ax3@.0.0 (0,7)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,16)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,360)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,16)
            pad_temp.shared = ...
        for yy_c.3 (0,16)
          for xx_c.3 (0,2)
            for rc.2 (0,4)
              for ry.2 (0,5)
                for rx.2 (0,5)
                  conv2d_nchw.local = ...
      for yy.3 (0,16)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 137	GFLOPS: 1126.41 / 5906.61	results: MeasureResult(cost:[0.0011], error_no:0, all_cost:3.46, Tstamp:1669889784.39)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,224)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,448)
    for rc.0 (0,16)
      for rx.0 (0,5)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,448)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,20)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,448)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for nn_c.3 (0,2)
              for yy_c.3 (0,2)
                for xx_c.3 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 138	GFLOPS: 1903.40 / 5906.61	results: MeasureResult(cost:[0.0007], error_no:0, all_cost:3.72, Tstamp:1669889786.52)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1792)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,56)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,32)
      for rx.0 (0,5)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,12)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for nn_c.4 (0,2)
            for yy_c.4 (0,4)
              conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,4)
        conv2d_nchw = ...

==================================================
No: 139	GFLOPS: 1600.30 / 5906.61	results: MeasureResult(cost:[0.0008], error_no:0, all_cost:3.69, Tstamp:1669889788.78)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,896)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,56)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,8)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,34)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
            vectorize ax0@ax1@ax2@ax3@.1 (0,4)
              pad_temp.shared = ...
        for rc.1 (0,4)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for yy_c.3 (0,2)
                for xx_c.3 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for yy.3 (0,4)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 140	GFLOPS: 173.47 / 5906.61	results: MeasureResult(cost:[0.0074], error_no:0, all_cost:4.94, Tstamp:1669889790.60)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,224)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,14)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        for ax0@ax1@ax2@ax3@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,14)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,371)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,14)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,5)
                for xx_c.4 (0,16)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          for xx.3 (0,16)
            conv2d_nchw = ...

==================================================
No: 141	GFLOPS: 338.88 / 5906.61	results: MeasureResult(cost:[0.0038], error_no:0, all_cost:5.94, Tstamp:1669889792.25)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,392)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,8)
        for rx.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,5)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
              vectorize ax0@ax1@ax2@ax3@.1 (0,32)
                pad_temp.shared = ...
          for rc.1 (0,4)
            for ry.1 (0,5)
              for yy_c.3 (0,2)
                for nn_c.4 (0,2)
                  for xx_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          for xx.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 142	GFLOPS: 62.05 / 5906.61	results: MeasureResult(cost:[0.0207], error_no:0, all_cost:3.29, Tstamp:1669889793.95)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,3584)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,14)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,4)
      conv2d_nchw.local auto_unroll: 16
      for rc.0 (0,8)
        for rx.0 (0,5)
          for ax0@ax1@ax2@ax3@.0.0 (0,5)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,4)
              kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,672)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,4)
              pad_temp.shared = ...
          for rc.1 (0,4)
            for ry.1 (0,5)
              for nn_c.3 (0,2)
                for xx_c.3 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 143	GFLOPS: 726.08 / 5906.61	results: MeasureResult(cost:[0.0018], error_no:0, all_cost:2.91, Tstamp:1669889795.58)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,448)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,112)
      for rc.0 (0,32)
        for ry.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,112)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,20)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,112)
              pad_temp.shared = ...
          for rx.1 (0,5)
            for nn_c.3 (0,2)
              for yy_c.3 (0,2)
                conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 144	GFLOPS: 199.17 / 5906.61	results: MeasureResult(cost:[0.0064], error_no:0, all_cost:3.89, Tstamp:1669889797.24)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,512)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,14)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,14)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,8)
        for rx.0 (0,5)
          for ax0@ax1@ax2@ax3@.0.0 (0,2)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,14)
              kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,116)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,14)
              vectorize ax0@ax1@ax2@ax3@.1 (0,5)
                pad_temp.shared = ...
          for rc.1 (0,4)
            for ry.1 (0,5)
              for nn_c.3 (0,4)
                for xx_c.3 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 145	GFLOPS: 381.68 / 5906.61	results: MeasureResult(cost:[0.0034], error_no:0, all_cost:3.49, Tstamp:1669889799.36)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,896)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,56)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,4)
        for rx.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,147)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
              pad_temp.shared = ...
          for rc.1 (0,4)
            for ry.1 (0,5)
              for rc.2 (0,2)
                for nn_c.4 (0,2)
                  for xx_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 146	GFLOPS: 2315.80 / 5906.61	results: MeasureResult(cost:[0.0006], error_no:0, all_cost:7.25, Tstamp:1669889801.48)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,896)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,28)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        for ax0@ax1@ax2@ax3@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,28)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,103)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,28)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for yy_c.4 (0,4)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,4)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 147	GFLOPS: 1808.80 / 5906.61	results: MeasureResult(cost:[0.0007], error_no:0, all_cost:9.66, Tstamp:1669889803.72)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,392)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,16)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        for ax0@ax1@ax2@ax3@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,16)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,60)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,16)
            vectorize ax0@ax1@ax2@ax3@.1 (0,3)
              pad_temp.shared = ...
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for xx_c.3 (0,4)
                for ry.2 (0,5)
                  for nn_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for yy.3 (0,2)
          for xx.3 (0,4)
            conv2d_nchw = ...

==================================================
No: 148	GFLOPS: 344.80 / 5906.61	results: MeasureResult(cost:[0.0037], error_no:0, all_cost:10.94, Tstamp:1669889805.56)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,224)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,28)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      for ry.0 (0,5)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,28)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,165)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,28)
            pad_temp.shared = ...
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for rx.2 (0,5)
              for nn_c.4 (0,4)
                for yy_c.4 (0,8)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for yy.3 (0,16)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 149	GFLOPS: 1011.99 / 5906.61	results: MeasureResult(cost:[0.0013], error_no:0, all_cost:4.49, Tstamp:1669889807.86)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,392)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 16
      for rc.0 (0,8)
        for ax0@ax1@ax2@ax3@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,12)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            vectorize ax0@ax1@ax2@ax3@.1 (0,16)
              pad_temp.shared = ...
        for ry.1 (0,5)
          for yy_c.3 (0,2)
            for rc.2 (0,4)
              for rx.2 (0,5)
                for nn_c.4 (0,2)
                  for xx_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          for xx.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 150	GFLOPS: 1263.79 / 5906.61	results: MeasureResult(cost:[0.0010], error_no:0, all_cost:4.32, Tstamp:1669889810.17)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,6272)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,2)
      for ax0@ax1@ax2@ax3@.0.0 (0,4)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,3)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,32)
            pad_temp.shared = ...
      for rc.1 (0,16)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for xx.3 (0,2)
      conv2d_nchw = ...

==================================================
No: 151	GFLOPS: 66.35 / 5906.61	results: MeasureResult(cost:[0.0194], error_no:0, all_cost:3.68, Tstamp:1669889812.07)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,224)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,7)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
      conv2d_nchw.local auto_unroll: 16
      for rc.0 (0,32)
        for ry.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,9)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
              vectorize ax0@ax1@ax2@ax3@.1 (0,40)
                pad_temp.shared = ...
          for yy_c.3 (0,2)
            for rx.2 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,8)
          conv2d_nchw = ...

==================================================
No: 152	GFLOPS: 1565.69 / 5906.61	results: MeasureResult(cost:[0.0008], error_no:0, all_cost:9.53, Tstamp:1669889814.20)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,392)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      for ry.0 (0,5)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,192)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.2 (0,5)
            for yy_c.4 (0,32)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,32)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 153	GFLOPS: 2096.83 / 5906.61	results: MeasureResult(cost:[0.0006], error_no:0, all_cost:4.41, Tstamp:1669889816.30)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,448)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,448)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        for rx.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,448)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,9)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,448)
              pad_temp.shared = ...
          for ry.1 (0,5)
            for rc.2 (0,2)
              conv2d_nchw.local = ...
      conv2d_nchw = ...

==================================================
No: 154	GFLOPS: 872.12 / 5906.61	results: MeasureResult(cost:[0.0015], error_no:0, all_cost:3.72, Tstamp:1669889818.49)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,448)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,8)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,56)
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,138)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for xx_c.3 (0,2)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 155	GFLOPS: 880.06 / 5906.61	results: MeasureResult(cost:[0.0015], error_no:0, all_cost:3.28, Tstamp:1669889820.28)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,224)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,224)
    for rc.0 (0,16)
      for ry.0 (0,5)
        for rx.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,224)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,32)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,224)
              pad_temp.shared = ...
          for xx_c.3 (0,2)
            for rc.2 (0,2)
              for nn_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 156	GFLOPS: 942.35 / 5906.61	results: MeasureResult(cost:[0.0014], error_no:0, all_cost:3.45, Tstamp:1669889821.92)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,448)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,29)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
          vectorize ax0@ax1@ax2@ax3@.1 (0,8)
            pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for yy_c.3 (0,7)
            for ry.2 (0,5)
              for yy_c.4 (0,4)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
    for yy.3 (0,28)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 157	GFLOPS: 234.95 / 5906.61	results: MeasureResult(cost:[0.0055], error_no:0, all_cost:3.06, Tstamp:1669889823.56)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,7168)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,7)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,8)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,8)
        for rx.0 (0,5)
          for ax0@ax1@ax2@ax3@.0.0 (0,3)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,8)
              kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,112)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,8)
              pad_temp.shared = ...
          for rc.2 (0,4)
            for ry.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 158	GFLOPS: 309.56 / 5906.61	results: MeasureResult(cost:[0.0041], error_no:0, all_cost:3.31, Tstamp:1669889825.19)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,448)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,224)
      conv2d_nchw.local auto_unroll: 16
      for rc.0 (0,8)
        for ry.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,224)
            vectorize ax0@ax1@ax2@ax3@.1 (0,4)
              kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,2)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,224)
              vectorize ax0@ax1@ax2@ax3@.1 (0,32)
                pad_temp.shared = ...
          for rx.1 (0,5)
            for xx_c.3 (0,4)
              for rc.2 (0,4)
                conv2d_nchw.local = ...
      for xx.3 (0,4)
        conv2d_nchw = ...

==================================================
No: 159	GFLOPS: 569.01 / 5906.61	results: MeasureResult(cost:[0.0023], error_no:0, all_cost:3.19, Tstamp:1669889826.83)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,392)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      for rc.0 (0,8)
        for rx.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,18)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
              vectorize ax0@ax1@ax2@ax3@.1 (0,8)
                pad_temp.shared = ...
          for rc.1 (0,4)
            for nn_c.3 (0,2)
              for yy_c.3 (0,2)
                for ry.2 (0,5)
                  for xx_c.4 (0,4)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          for xx.3 (0,4)
            conv2d_nchw = ...

==================================================
No: 160	GFLOPS: 291.21 / 5906.61	results: MeasureResult(cost:[0.0044], error_no:0, all_cost:3.89, Tstamp:1669889828.49)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,224)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,28)
    conv2d_nchw.local auto_unroll: 16
    for rc.0 (0,16)
      for ax0@ax1@ax2@ax3@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,28)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,11)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,28)
          vectorize ax0@ax1@ax2@ax3@.1 (0,32)
            pad_temp.shared = ...
      for rx.1 (0,5)
        for yy_c.3 (0,4)
          for xx_c.3 (0,4)
            for rc.2 (0,2)
              for ry.2 (0,5)
                for nn_c.4 (0,4)
                  for xx_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for yy.3 (0,4)
        for xx.3 (0,8)
          conv2d_nchw = ...

==================================================
No: 161	GFLOPS: 537.95 / 5906.61	results: MeasureResult(cost:[0.0024], error_no:0, all_cost:11.27, Tstamp:1669889830.40)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,448)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,56)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,8)
        for ry.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,3)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
              vectorize ax0@ax1@ax2@ax3@.1 (0,64)
                pad_temp.shared = ...
          for rc.1 (0,2)
            for rx.1 (0,5)
              for yy_c.3 (0,2)
                for rc.2 (0,2)
                  for xx_c.4 (0,4)
                    conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 162	GFLOPS: 2146.69 / 5906.61	results: MeasureResult(cost:[0.0006], error_no:0, all_cost:3.80, Tstamp:1669889832.51)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,896)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,56)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        for rx.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,18)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
              pad_temp.shared = ...
          for ry.1 (0,5)
            for yy_c.3 (0,2)
              for xx_c.3 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 163	GFLOPS: 2111.90 / 5906.61	results: MeasureResult(cost:[0.0006], error_no:0, all_cost:5.11, Tstamp:1669889834.63)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,448)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,448)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,8)
        for rx.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,448)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,17)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,448)
              pad_temp.shared = ...
          for rc.1 (0,2)
            for rc.2 (0,2)
              for ry.2 (0,5)
                conv2d_nchw.local = ...
      conv2d_nchw = ...

==================================================
No: 164	GFLOPS: 2825.17 / 5906.61	results: MeasureResult(cost:[0.0005], error_no:0, all_cost:4.41, Tstamp:1669889836.37)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,448)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,56)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        for ry.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,48)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
              pad_temp.shared = ...
          for nn_c.3 (0,2)
            for xx_c.3 (0,2)
              for rx.2 (0,5)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          for xx.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 165	GFLOPS: 183.68 / 5906.61	results: MeasureResult(cost:[0.0070], error_no:0, all_cost:3.59, Tstamp:1669889838.06)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,448)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,14)
      conv2d_nchw.local auto_unroll: 16
      for rc.0 (0,16)
        for ry.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,14)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,288)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,14)
              pad_temp.shared = ...
          for rx.1 (0,5)
            for nn_c.3 (0,4)
              for xx_c.3 (0,8)
                for rc.2 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for xx.3 (0,8)
          conv2d_nchw = ...

==================================================
No: 166	GFLOPS: 2580.99 / 5906.61	results: MeasureResult(cost:[0.0005], error_no:0, all_cost:7.92, Tstamp:1669889840.26)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,512)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,14)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,28)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,8)
        for ry.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,28)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,256)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,28)
              pad_temp.shared = ...
          for rc.1 (0,4)
            for rx.1 (0,5)
              for nn_c.3 (0,2)
                for xx_c.3 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 167	GFLOPS: 535.75 / 5906.61	results: MeasureResult(cost:[0.0024], error_no:0, all_cost:3.95, Tstamp:1669889841.91)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,448)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,224)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,8)
      for rx.0 (0,5)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,224)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,224)
            vectorize ax0@ax1@ax2@ax3@.1 (0,32)
              pad_temp.shared = ...
        for rc.1 (0,4)
          for ry.1 (0,5)
            for yy_c.3 (0,2)
              for xx_c.3 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for yy.3 (0,4)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 168	GFLOPS: 466.44 / 5906.61	results: MeasureResult(cost:[0.0028], error_no:0, all_cost:3.16, Tstamp:1669889843.55)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,448)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,56)
      conv2d_nchw.local auto_unroll: 16
      for rc.0 (0,16)
        for ry.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,72)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
              pad_temp.shared = ...
          for xx_c.3 (0,8)
            for rc.2 (0,2)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
      for xx.3 (0,8)
        conv2d_nchw = ...

==================================================
No: 169	GFLOPS: 1446.25 / 5906.61	results: MeasureResult(cost:[0.0009], error_no:0, all_cost:3.70, Tstamp:1669889845.68)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,896)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,7)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,8)
        for ax0@ax1@ax2@ax3@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            vectorize ax0@ax1@ax2@ax3@.1 (0,4)
              pad_temp.shared = ...
        for rx.1 (0,5)
          for rc.2 (0,4)
            for ry.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 170	GFLOPS: 692.63 / 5906.61	results: MeasureResult(cost:[0.0019], error_no:0, all_cost:6.94, Tstamp:1669889847.31)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,224)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,8)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,16)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        for rx.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,16)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,288)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,16)
              pad_temp.shared = ...
          for ry.1 (0,5)
            for nn_c.3 (0,2)
              for yy_c.3 (0,7)
                for xx_c.3 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,7)
          for xx.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 171	GFLOPS: 766.59 / 5906.61	results: MeasureResult(cost:[0.0017], error_no:0, all_cost:3.58, Tstamp:1669889849.51)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,224)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,112)
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,112)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,86)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,112)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for xx_c.3 (0,4)
          for rc.2 (0,2)
            for ry.2 (0,5)
              for nn_c.4 (0,4)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for xx.3 (0,8)
        conv2d_nchw = ...

==================================================
No: 172	GFLOPS: 1074.64 / 5906.61	results: MeasureResult(cost:[0.0012], error_no:0, all_cost:5.89, Tstamp:1669889851.78)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        for ry.0 (0,5)
          for rx.0 (0,5)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
              kernel.shared = ...
            for ax0@ax1@ax2@ax3@.0.0 (0,8)
              threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
                pad_temp.shared = ...
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 173	GFLOPS: 335.39 / 5906.61	results: MeasureResult(cost:[0.0038], error_no:0, all_cost:4.00, Tstamp:1669889853.40)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,224)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,28)
    conv2d_nchw.local auto_unroll: 16
    for rc.0 (0,16)
      for ax0@ax1@ax2@ax3@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,28)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,22)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,28)
          vectorize ax0@ax1@ax2@ax3@.1 (0,16)
            pad_temp.shared = ...
      for rx.1 (0,5)
        for yy_c.3 (0,4)
          for xx_c.3 (0,4)
            for rc.2 (0,2)
              for ry.2 (0,5)
                for nn_c.4 (0,4)
                  for xx_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for yy.3 (0,4)
        for xx.3 (0,8)
          conv2d_nchw = ...

==================================================
No: 174	GFLOPS: 634.64 / 5906.61	results: MeasureResult(cost:[0.0020], error_no:0, all_cost:3.72, Tstamp:1669889855.52)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,392)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
    conv2d_nchw.local auto_unroll: 16
    for rc.0 (0,16)
      for ry.0 (0,5)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
            vectorize ax0@ax1@ax2@ax3@.1 (0,12)
              pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.2 (0,5)
            for yy_c.4 (0,32)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,32)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 175	GFLOPS: 908.91 / 5906.61	results: MeasureResult(cost:[0.0014], error_no:0, all_cost:2.99, Tstamp:1669889857.15)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,392)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    for rc.0 (0,16)
      for ry.0 (0,5)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,24)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            vectorize ax0@ax1@ax2@ax3@.1 (0,4)
              pad_temp.shared = ...
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.3 (0,16)
              for rc.2 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,16)
        conv2d_nchw = ...

==================================================
No: 176	GFLOPS: 1015.80 / 5906.61	results: MeasureResult(cost:[0.0013], error_no:0, all_cost:4.27, Tstamp:1669889859.25)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,6272)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      for ry.0 (0,5)
        for rx.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,4)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
              pad_temp.shared = ...
          for rc.1 (0,2)
            for yy_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      conv2d_nchw = ...

==================================================
No: 177	GFLOPS: 1151.14 / 5906.61	results: MeasureResult(cost:[0.0011], error_no:0, all_cost:4.85, Tstamp:1669889861.38)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,896)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,112)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,8)
      for ry.0 (0,5)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,112)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,42)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,112)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for yy_c.3 (0,2)
            for rc.2 (0,2)
              for rx.2 (0,5)
                for nn_c.4 (0,4)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 178	GFLOPS: 8.23 / 5906.61	results: MeasureResult(cost:[0.1561], error_no:0, all_cost:4.97, Tstamp:1669889863.48)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,448)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,8)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,2)
      conv2d_nchw.local auto_unroll: 16
      for rc.0 (0,8)
        for ry.0 (0,5)
          for rx.0 (0,5)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,2)
              vectorize ax0@ax1@ax2@ax3@.1 (0,4)
                kernel.shared = ...
            for ax0@ax1@ax2@ax3@.0.0 (0,224)
              threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,2)
                vectorize ax0@ax1@ax2@ax3@.1 (0,16)
                  pad_temp.shared = ...
            for yy_c.3 (0,4)
              for xx_c.3 (0,4)
                for rc.2 (0,4)
                  for xx_c.4 (0,7)
                    conv2d_nchw.local = ...
      for yy.3 (0,4)
        for xx.3 (0,28)
          conv2d_nchw = ...

==================================================
No: 179	GFLOPS: 1018.67 / 5906.61	results: MeasureResult(cost:[0.0013], error_no:0, all_cost:3.41, Tstamp:1669889865.59)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,512)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,196)
    for rc.0 (0,8)
      for ry.0 (0,5)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,196)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,42)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,196)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for xx_c.3 (0,2)
                for rc.2 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
    for yy.3 (0,4)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 180	GFLOPS: 360.24 / 5906.61	results: MeasureResult(cost:[0.0036], error_no:0, all_cost:3.40, Tstamp:1669889867.38)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,224)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,28)
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,28)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,183)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,28)
            pad_temp.shared = ...
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for xx_c.3 (0,4)
              for ry.2 (0,5)
                for rx.2 (0,5)
                  for xx_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          for xx.3 (0,8)
            conv2d_nchw = ...

==================================================
No: 181	GFLOPS: 99.42 / 5906.61	results: MeasureResult(cost:[0.0129], error_no:0, all_cost:3.29, Tstamp:1669889869.30)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,112)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
      conv2d_nchw.local auto_unroll: 16
      for rc.0 (0,32)
        for ry.0 (0,5)
          for rx.0 (0,5)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
              kernel.shared = ...
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
              vectorize ax0@ax1@ax2@ax3@.1 (0,56)
                pad_temp.shared = ...
            for yy_c.3 (0,7)
              for xx_c.3 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,7)
        for xx.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 182	GFLOPS: 113.62 / 5906.61	results: MeasureResult(cost:[0.0113], error_no:0, all_cost:3.27, Tstamp:1669889870.99)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,8)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,16)
      for rc.0 (0,2)
        for ry.0 (0,5)
          for ax0@ax1@ax2@ax3@.0.0 (0,5)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,16)
              kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,640)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,16)
              pad_temp.shared = ...
          for yy_c.3 (0,2)
            for rc.2 (0,16)
              for rx.2 (0,5)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,4)
        conv2d_nchw = ...

==================================================
No: 183	GFLOPS: 293.35 / 5906.61	results: MeasureResult(cost:[0.0044], error_no:0, all_cost:3.82, Tstamp:1669889873.12)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,448)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,14)
      conv2d_nchw.local auto_unroll: 16
      for rc.0 (0,8)
        for ry.0 (0,5)
          for ax0@ax1@ax2@ax3@.0.0 (0,2)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,14)
              kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,19)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,14)
              vectorize ax0@ax1@ax2@ax3@.1 (0,32)
                pad_temp.shared = ...
          for rc.1 (0,4)
            for rx.1 (0,5)
              for nn_c.3 (0,4)
                for yy_c.3 (0,4)
                  for xx_c.3 (0,4)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for yy.3 (0,4)
          for xx.3 (0,4)
            conv2d_nchw = ...

==================================================
No: 184	GFLOPS: 579.08 / 5906.61	results: MeasureResult(cost:[0.0022], error_no:0, all_cost:3.06, Tstamp:1669889874.75)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,896)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,28)
      for rc.0 (0,32)
        for rx.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,28)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,21)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,28)
              vectorize ax0@ax1@ax2@ax3@.1 (0,2)
                pad_temp.shared = ...
          for xx_c.3 (0,4)
            for ry.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for xx.3 (0,8)
        conv2d_nchw = ...

==================================================
No: 185	GFLOPS: 461.82 / 5906.61	results: MeasureResult(cost:[0.0028], error_no:0, all_cost:2.94, Tstamp:1669889876.40)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,392)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
    conv2d_nchw.local auto_unroll: 16
    for rc.0 (0,16)
      for ry.0 (0,5)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
            vectorize ax0@ax1@ax2@ax3@.1 (0,24)
              pad_temp.shared = ...
        for rc.1 (0,2)
          for yy_c.3 (0,8)
            for rx.2 (0,5)
              for yy_c.4 (0,4)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
    for yy.3 (0,32)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 186	GFLOPS: 1498.75 / 5906.61	results: MeasureResult(cost:[0.0009], error_no:0, all_cost:9.38, Tstamp:1669889878.53)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,3584)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,14)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,4)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,8)
        for ax0@ax1@ax2@ax3@.0.0 (0,25)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,4)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,768)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,4)
            pad_temp.shared = ...
        for rc.1 (0,4)
          for ry.1 (0,5)
            for nn_c.3 (0,2)
              for xx_c.3 (0,2)
                for rx.2 (0,5)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 187	GFLOPS: 1833.02 / 5906.61	results: MeasureResult(cost:[0.0007], error_no:0, all_cost:8.01, Tstamp:1669889880.65)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,3136)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,2)
      for ax0@ax1@ax2@ax3@.0.0 (0,7)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,4)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,32)
            pad_temp.shared = ...
      for ry.1 (0,5)
        for xx_c.3 (0,2)
          for rc.2 (0,16)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for xx.3 (0,4)
      conv2d_nchw = ...

==================================================
No: 188	GFLOPS: 926.41 / 5906.61	results: MeasureResult(cost:[0.0014], error_no:0, all_cost:3.36, Tstamp:1669889882.76)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,224)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,224)
    for rc.0 (0,32)
      for ry.0 (0,5)
        for rx.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,224)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,16)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,224)
              pad_temp.shared = ...
          for xx_c.3 (0,2)
            for nn_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 189	GFLOPS: 275.87 / 5906.61	results: MeasureResult(cost:[0.0047], error_no:0, all_cost:3.02, Tstamp:1669889884.88)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1792)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,56)
      conv2d_nchw.local auto_unroll: 16
      for rc.0 (0,2)
        for ry.0 (0,5)
          for rx.0 (0,5)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
              kernel.shared = ...
            for ax0@ax1@ax2@ax3@.0.0 (0,128)
              threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
                pad_temp.shared = ...
            for rc.1 (0,2)
              for rc.2 (0,8)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 190	GFLOPS: 183.51 / 5906.61	results: MeasureResult(cost:[0.0070], error_no:0, all_cost:6.83, Tstamp:1669889886.67)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,448)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,8)
        for ry.0 (0,5)
          for rx.0 (0,5)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
              kernel.shared = ...
            for ax0@ax1@ax2@ax3@.0.0 (0,224)
              threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
                pad_temp.shared = ...
            for rc.1 (0,4)
              for nn_c.3 (0,2)
                for yy_c.3 (0,2)
                  for yy_c.4 (0,7)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,14)
          conv2d_nchw = ...

==================================================
No: 191	GFLOPS: 1458.93 / 5906.61	results: MeasureResult(cost:[0.0009], error_no:0, all_cost:2.87, Tstamp:1669889888.32)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,784)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 16
      for rc.0 (0,8)
        for ax0@ax1@ax2@ax3@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,90)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for rc.2 (0,4)
              for ry.2 (0,5)
                for rx.2 (0,5)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for yy.3 (0,4)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 192	GFLOPS: 3069.41 / 5906.61	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:6.09, Tstamp:1669889890.45)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,224)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,112)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,112)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,103)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,112)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for yy_c.4 (0,4)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,4)
        for xx.3 (0,2)
          conv2d_nchw = ...

Time elapsed for measurement: 140.82 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 0.61 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 60	fail_ct: 4036	Time elapsed: 2.12
GA Iter: 0	Max score: 0.6692	Min score: 0.0896	#Pop: 60	#M+: 0	#M-: 0
GA Iter: 4	Max score: 1.0035	Min score: 0.7494	#Pop: 128	#M+: 1388	#M-: 0
EvolutionarySearch		#s: 128	Time elapsed: 13.08
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************==================================================
No: 193	GFLOPS: 8040.19 / 8040.19	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.97, Tstamp:1669889921.87)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 194	GFLOPS: 7998.69 / 8040.19	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.81, Tstamp:1669889923.52)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 195	GFLOPS: 8002.44 / 8040.19	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.82, Tstamp:1669889925.21)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ry.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 196	GFLOPS: 8058.62 / 8058.62	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.81, Tstamp:1669889926.90)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 197	GFLOPS: 8015.12 / 8058.62	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.84, Tstamp:1669889928.58)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,4)
            for rx.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 198	GFLOPS: 8042.37 / 8058.62	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.89, Tstamp:1669889930.31)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ry.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 199	GFLOPS: 8037.89 / 8058.62	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.83, Tstamp:1669889932.00)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,4)
            for rx.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 200	GFLOPS: 7027.83 / 8058.62	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:3.56, Tstamp:1669889933.69)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ry.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 201	GFLOPS: 8008.69 / 8058.62	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.60, Tstamp:1669889935.37)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 202	GFLOPS: 6976.88 / 8058.62	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.04, Tstamp:1669889937.56)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ry.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 203	GFLOPS: 8021.77 / 8058.62	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.76, Tstamp:1669889939.40)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 204	GFLOPS: 6977.50 / 8058.62	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:5.25, Tstamp:1669889941.58)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,33)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,3)
            for nn_c.3 (0,4)
              for ff_c.3 (0,2)
                for yy_c.3 (0,2)
                  for ry.2 (0,3)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 205	GFLOPS: 6934.20 / 8058.62	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:5.40, Tstamp:1669889943.75)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,33)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for nn_c.3 (0,4)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 206	GFLOPS: 3280.86 / 8058.62	results: MeasureResult(cost:[0.0006], error_no:0, all_cost:6.78, Tstamp:1669889945.93)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,8)
        for ax0@ax1@ax2@ax3@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,66)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rc.1 (0,4)
          for nn_c.3 (0,4)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 207	GFLOPS: 3356.91 / 8058.62	results: MeasureResult(cost:[0.0006], error_no:0, all_cost:6.89, Tstamp:1669889948.11)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,8)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,66)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rc.1 (0,4)
          for nn_c.3 (0,4)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 208	GFLOPS: 3288.36 / 8058.62	results: MeasureResult(cost:[0.0006], error_no:0, all_cost:6.97, Tstamp:1669889950.28)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,8)
        for ax0@ax1@ax2@ax3@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,66)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rc.1 (0,4)
          for rx.1 (0,3)
            for nn_c.3 (0,4)
              for ry.2 (0,3)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 209	GFLOPS: 6880.83 / 8058.62	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:5.32, Tstamp:1669889952.47)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,33)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for nn_c.3 (0,4)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 210	GFLOPS: 6940.95 / 8058.62	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:5.34, Tstamp:1669889954.68)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,33)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,3)
            for nn_c.3 (0,4)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 211	GFLOPS: 6913.78 / 8058.62	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:5.24, Tstamp:1669889956.94)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,33)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for rc.2 (0,2)
              for ry.2 (0,3)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 212	GFLOPS: 6935.81 / 8058.62	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:5.23, Tstamp:1669889959.20)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,33)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,3)
            for nn_c.3 (0,4)
              for ry.2 (0,3)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 213	GFLOPS: 3364.24 / 8058.62	results: MeasureResult(cost:[0.0006], error_no:0, all_cost:6.70, Tstamp:1669889961.47)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,8)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,66)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rc.1 (0,4)
          for rx.1 (0,3)
            for nn_c.3 (0,4)
              for ry.2 (0,3)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 214	GFLOPS: 2628.30 / 8058.62	results: MeasureResult(cost:[0.0007], error_no:0, all_cost:6.65, Tstamp:1669889963.72)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,8)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,66)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for nn_c.3 (0,4)
          for rc.2 (0,4)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 215	GFLOPS: 3356.46 / 8058.62	results: MeasureResult(cost:[0.0006], error_no:0, all_cost:6.64, Tstamp:1669889965.98)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,8)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,66)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rc.1 (0,4)
          for nn_c.3 (0,4)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 216	GFLOPS: 3228.43 / 8058.62	results: MeasureResult(cost:[0.0006], error_no:0, all_cost:6.81, Tstamp:1669889968.23)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,8)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,66)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,4)
              for rc.2 (0,4)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 217	GFLOPS: 6892.42 / 8058.62	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.58, Tstamp:1669889970.01)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,33)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for nn_c.3 (0,4)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 218	GFLOPS: 2436.05 / 8058.62	results: MeasureResult(cost:[0.0008], error_no:0, all_cost:3.77, Tstamp:1669889972.20)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,8)
        for ax0@ax1@ax2@ax3@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,66)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rc.1 (0,4)
          for nn_c.3 (0,4)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 219	GFLOPS: 2439.81 / 8058.62	results: MeasureResult(cost:[0.0008], error_no:0, all_cost:3.75, Tstamp:1669889974.35)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,8)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,66)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rc.1 (0,4)
          for nn_c.3 (0,4)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 220	GFLOPS: 1771.55 / 8058.62	results: MeasureResult(cost:[0.0011], error_no:0, all_cost:3.73, Tstamp:1669889976.53)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,8)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,66)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for rc.2 (0,4)
              for ry.2 (0,3)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 221	GFLOPS: 2382.51 / 8058.62	results: MeasureResult(cost:[0.0008], error_no:0, all_cost:3.25, Tstamp:1669889978.23)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,8)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,66)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rc.1 (0,4)
          for rx.1 (0,3)
            for nn_c.3 (0,4)
              for ry.2 (0,3)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 222	GFLOPS: 8474.14 / 8474.14	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.29, Tstamp:1669889980.15)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 223	GFLOPS: 8047.35 / 8474.14	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.86, Tstamp:1669889981.85)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 224	GFLOPS: 8030.25 / 8474.14	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.79, Tstamp:1669889983.52)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 225	GFLOPS: 8035.58 / 8474.14	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.84, Tstamp:1669889985.24)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 226	GFLOPS: 8037.54 / 8474.14	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669889987.15)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 227	GFLOPS: 8035.59 / 8474.14	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.79, Tstamp:1669889988.86)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.2 (0,3)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 228	GFLOPS: 8043.19 / 8474.14	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.83, Tstamp:1669889990.56)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 229	GFLOPS: 8529.82 / 8529.82	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.84, Tstamp:1669889992.28)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 230	GFLOPS: 7034.02 / 8529.82	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.02, Tstamp:1669889994.28)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 231	GFLOPS: 7488.88 / 8529.82	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:3.99, Tstamp:1669889995.97)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,3)
              pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 232	GFLOPS: 7503.72 / 8529.82	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.00, Tstamp:1669889997.68)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,3)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,4)
            for rx.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 233	GFLOPS: 7510.12 / 8529.82	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:3.95, Tstamp:1669889999.37)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,3)
              pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 234	GFLOPS: 7561.03 / 8529.82	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.30, Tstamp:1669890001.35)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,4)
              pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 235	GFLOPS: 7211.27 / 8529.82	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:3.92, Tstamp:1669890003.04)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,6)
              pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 236	GFLOPS: 6934.44 / 8529.82	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:5.03, Tstamp:1669890005.23)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,33)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,3)
            for rx.1 (0,3)
              for nn_c.3 (0,4)
                for ff_c.3 (0,2)
                  for yy_c.3 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 237	GFLOPS: 3325.73 / 8529.82	results: MeasureResult(cost:[0.0006], error_no:0, all_cost:6.93, Tstamp:1669890007.41)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,8)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,4)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,4)
            for rc.2 (0,4)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 238	GFLOPS: 3113.94 / 8529.82	results: MeasureResult(cost:[0.0006], error_no:0, all_cost:6.56, Tstamp:1669890009.58)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,8)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,4)
              pad_temp.shared = ...
        for nn_c.3 (0,4)
          for rc.2 (0,4)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 239	GFLOPS: 1579.99 / 8529.82	results: MeasureResult(cost:[0.0012], error_no:0, all_cost:4.92, Tstamp:1669890011.76)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,8)
        for rx.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,64)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
              pad_temp.shared = ...
          for nn_c.3 (0,4)
            for rc.2 (0,4)
              for ry.2 (0,3)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 240	GFLOPS: 3354.20 / 8529.82	results: MeasureResult(cost:[0.0006], error_no:0, all_cost:6.67, Tstamp:1669890013.94)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,8)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,66)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rc.1 (0,4)
          for ry.1 (0,3)
            for rx.1 (0,3)
              for nn_c.3 (0,4)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 241	GFLOPS: 8008.71 / 8529.82	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.64, Tstamp:1669890015.64)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 242	GFLOPS: 3527.13 / 8529.82	results: MeasureResult(cost:[0.0005], error_no:0, all_cost:7.22, Tstamp:1669890018.16)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,8)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,24)
              pad_temp.shared = ...
        for rc.1 (0,4)
          for nn_c.3 (0,4)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 243	GFLOPS: 6890.01 / 8529.82	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:5.26, Tstamp:1669890020.56)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,33)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,3)
            for nn_c.3 (0,4)
              for ff_c.3 (0,2)
                for yy_c.3 (0,2)
                  for ry.2 (0,3)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 244	GFLOPS: 7501.00 / 8529.82	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.08, Tstamp:1669890022.52)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,3)
              pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 245	GFLOPS: 8009.92 / 8529.82	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.83, Tstamp:1669890024.20)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for nn_c.3 (0,4)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 246	GFLOPS: 2662.65 / 8529.82	results: MeasureResult(cost:[0.0007], error_no:0, all_cost:4.26, Tstamp:1669890026.39)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,8)
        for ax0@ax1@ax2@ax3@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,12)
              pad_temp.shared = ...
        for rc.1 (0,4)
          for nn_c.3 (0,4)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 247	GFLOPS: 6180.71 / 8529.82	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:5.19, Tstamp:1669890028.55)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,6)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,6)
              pad_temp.shared = ...
        for rc.1 (0,2)
          for nn_c.3 (0,4)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 248	GFLOPS: 8086.63 / 8529.82	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.83, Tstamp:1669890030.24)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for nn_c.3 (0,4)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 249	GFLOPS: 4486.80 / 8529.82	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:3.87, Tstamp:1669890032.41)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,24)
            pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 250	GFLOPS: 8085.26 / 8529.82	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.17, Tstamp:1669890034.22)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,4)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 251	GFLOPS: 5691.91 / 8529.82	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:3.89, Tstamp:1669890036.39)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,12)
              pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 252	GFLOPS: 4416.63 / 8529.82	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:3.74, Tstamp:1669890038.58)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,24)
            pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 253	GFLOPS: 7992.12 / 8529.82	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.92, Tstamp:1669890040.27)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for nn_c.3 (0,4)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 254	GFLOPS: 4937.13 / 8529.82	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:2.78, Tstamp:1669890041.98)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1808)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,226)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      for rx.0 (0,3)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,2)
      for ff.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 255	GFLOPS: 1579.29 / 8529.82	results: MeasureResult(cost:[0.0012], error_no:0, all_cost:3.95, Tstamp:1669890044.19)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,8)
        for rx.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,64)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
              pad_temp.shared = ...
          for nn_c.3 (0,4)
            for rc.2 (0,4)
              for ry.2 (0,3)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 256	GFLOPS: 1578.53 / 8529.82	results: MeasureResult(cost:[0.0012], error_no:0, all_cost:3.66, Tstamp:1669890045.90)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,8)
        for rx.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,64)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
              pad_temp.shared = ...
          for rc.1 (0,4)
            for nn_c.3 (0,4)
              for ry.2 (0,3)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

Time elapsed for measurement: 139.52 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 0.64 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Iter: 5	#Pop: 31	#Target: 50	fail_ct: 10209	Time elapsed: 3.99
Sample Initial Population	#s: 53	fail_ct: 16331	Time elapsed: 6.51
GA Iter: 0	Max score: 0.4828	Min score: 0.0092	#Pop: 53	#M+: 0	#M-: 0
GA Iter: 4	Max score: 1.0040	Min score: 0.6664	#Pop: 128	#M+: 1394	#M-: 0
EvolutionarySearch		#s: 128	Time elapsed: 13.96
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************==================================================
No: 257	GFLOPS: 5947.44 / 5947.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.27, Tstamp:1669890082.25)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for yy_c.3 (0,2)
          for rc.2 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 258	GFLOPS: 5953.78 / 5953.78	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.34, Tstamp:1669890083.91)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for yy_c.3 (0,2)
              for rx.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 259	GFLOPS: 5043.69 / 5953.78	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.41, Tstamp:1669890085.53)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for ry.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 260	GFLOPS: 4313.43 / 5953.78	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.21, Tstamp:1669890087.16)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 261	GFLOPS: 5988.36 / 5988.36	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.43, Tstamp:1669890088.88)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for rc.2 (0,2)
              for ry.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 262	GFLOPS: 5937.67 / 5988.36	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.57, Tstamp:1669890090.79)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for yy_c.3 (0,2)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 263	GFLOPS: 5973.13 / 5988.36	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.44, Tstamp:1669890092.46)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for ry.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 264	GFLOPS: 5965.40 / 5988.36	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.40, Tstamp:1669890094.13)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 265	GFLOPS: 5972.69 / 5988.36	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.31, Tstamp:1669890095.98)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for yy_c.3 (0,2)
              for rx.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 266	GFLOPS: 5959.67 / 5988.36	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.61, Tstamp:1669890097.85)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for yy_c.3 (0,2)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 267	GFLOPS: 5953.98 / 5988.36	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.27, Tstamp:1669890099.54)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for ry.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 268	GFLOPS: 5975.51 / 5988.36	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.40, Tstamp:1669890101.23)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for yy_c.3 (0,2)
              for rx.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 269	GFLOPS: 5965.96 / 5988.36	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.39, Tstamp:1669890103.06)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for yy_c.3 (0,2)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 270	GFLOPS: 5909.56 / 5988.36	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.56, Tstamp:1669890104.95)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for yy_c.3 (0,2)
              for rx.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 271	GFLOPS: 5910.98 / 5988.36	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.32, Tstamp:1669890106.61)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for yy_c.3 (0,2)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 272	GFLOPS: 4273.14 / 5988.36	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.19, Tstamp:1669890108.27)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 273	GFLOPS: 5938.60 / 5988.36	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.26, Tstamp:1669890110.13)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for ry.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 274	GFLOPS: 3828.60 / 5988.36	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.80, Tstamp:1669890111.96)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            vectorize ax0@ax1@ax2@ax3@.1 (0,4)
              pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for ry.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 275	GFLOPS: 3841.26 / 5988.36	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.42, Tstamp:1669890113.62)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            vectorize ax0@ax1@ax2@ax3@.1 (0,4)
              pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for yy_c.3 (0,2)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 276	GFLOPS: 4973.28 / 5988.36	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.46, Tstamp:1669890115.25)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for yy_c.3 (0,2)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 277	GFLOPS: 5942.24 / 5988.36	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.56, Tstamp:1669890117.09)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for rc.2 (0,2)
              for ry.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 278	GFLOPS: 5840.05 / 5988.36	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.60, Tstamp:1669890118.94)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for yy_c.3 (0,2)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 279	GFLOPS: 5861.61 / 5988.36	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.31, Tstamp:1669890120.62)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for ry.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 280	GFLOPS: 3553.84 / 5988.36	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:4.42, Tstamp:1669890122.25)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rc.1 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 281	GFLOPS: 4225.61 / 5988.36	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.20, Tstamp:1669890124.05)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 282	GFLOPS: 5851.15 / 5988.36	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.57, Tstamp:1669890125.92)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for yy_c.3 (0,2)
              for rx.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 283	GFLOPS: 4981.54 / 5988.36	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.41, Tstamp:1669890127.57)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for ry.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 284	GFLOPS: 3847.00 / 5988.36	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:5.23, Tstamp:1669890129.19)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            vectorize ax0@ax1@ax2@ax3@.1 (0,4)
              pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for yy_c.3 (0,2)
              for rx.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 285	GFLOPS: 5940.50 / 5988.36	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.57, Tstamp:1669890130.97)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for ry.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 286	GFLOPS: 5935.64 / 5988.36	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.48, Tstamp:1669890132.85)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 287	GFLOPS: 5916.54 / 5988.36	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.31, Tstamp:1669890134.52)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for ry.2 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 288	GFLOPS: 5930.20 / 5988.36	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.12, Tstamp:1669890136.19)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for ry.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 289	GFLOPS: 3386.24 / 5988.36	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:5.51, Tstamp:1669890138.40)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            vectorize ax0@ax1@ax2@ax3@.1 (0,10)
              pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for ry.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 290	GFLOPS: 3799.41 / 5988.36	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:5.71, Tstamp:1669890140.51)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            vectorize ax0@ax1@ax2@ax3@.1 (0,5)
              pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for yy_c.3 (0,2)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 291	GFLOPS: 3372.70 / 5988.36	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:5.62, Tstamp:1669890142.79)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            vectorize ax0@ax1@ax2@ax3@.1 (0,10)
              pad_temp.shared = ...
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for rc.2 (0,2)
              for ry.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 292	GFLOPS: 2881.60 / 5988.36	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:5.44, Tstamp:1669890145.22)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            vectorize ax0@ax1@ax2@ax3@.1 (0,20)
              pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for ry.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 293	GFLOPS: 2986.59 / 5988.36	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:5.50, Tstamp:1669890147.63)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            vectorize ax0@ax1@ax2@ax3@.1 (0,10)
              pad_temp.shared = ...
        for rc.1 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 294	GFLOPS: 3375.35 / 5988.36	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:5.45, Tstamp:1669890149.93)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            vectorize ax0@ax1@ax2@ax3@.1 (0,10)
              pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for yy_c.3 (0,2)
              for rx.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 295	GFLOPS: 3696.31 / 5988.36	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:6.02, Tstamp:1669890152.31)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            vectorize ax0@ax1@ax2@ax3@.1 (0,5)
              pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for ry.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 296	GFLOPS: 4681.90 / 5988.36	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.49, Tstamp:1669890154.18)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for rc.2 (0,2)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 297	GFLOPS: 2689.69 / 5988.36	results: MeasureResult(cost:[0.0005], error_no:0, all_cost:3.49, Tstamp:1669890156.30)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for ry.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 298	GFLOPS: 2674.46 / 5988.36	results: MeasureResult(cost:[0.0005], error_no:0, all_cost:3.42, Tstamp:1669890158.40)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for ry.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 299	GFLOPS: 3404.17 / 5988.36	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:6.42, Tstamp:1669890160.66)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            vectorize ax0@ax1@ax2@ax3@.1 (0,10)
              pad_temp.shared = ...
        for rc.1 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 300	GFLOPS: 3393.97 / 5988.36	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:6.41, Tstamp:1669890162.91)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            vectorize ax0@ax1@ax2@ax3@.1 (0,10)
              pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for ry.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 301	GFLOPS: 2897.55 / 5988.36	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:6.02, Tstamp:1669890165.16)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            vectorize ax0@ax1@ax2@ax3@.1 (0,20)
              pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for ry.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 302	GFLOPS: 5910.91 / 5988.36	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.52, Tstamp:1669890166.96)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for ry.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 303	GFLOPS: 5933.78 / 5988.36	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.43, Tstamp:1669890168.65)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 304	GFLOPS: 4431.55 / 5988.36	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.92, Tstamp:1669890170.28)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rc.1 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 305	GFLOPS: 6039.51 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.93, Tstamp:1669890171.95)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for nn_c.3 (0,2)
                for yy_c.3 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 306	GFLOPS: 3394.07 / 6039.51	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:6.33, Tstamp:1669890174.06)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            vectorize ax0@ax1@ax2@ax3@.1 (0,10)
              pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for ry.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 307	GFLOPS: 3400.51 / 6039.51	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:6.11, Tstamp:1669890176.19)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            vectorize ax0@ax1@ax2@ax3@.1 (0,10)
              pad_temp.shared = ...
        for rc.1 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 308	GFLOPS: 6001.50 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.27, Tstamp:1669890177.84)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for rc.2 (0,2)
              for ry.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 309	GFLOPS: 5907.75 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.20, Tstamp:1669890179.50)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for ry.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 310	GFLOPS: 3353.44 / 6039.51	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:5.14, Tstamp:1669890181.60)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for ry.2 (0,5)
              for nn_c.4 (0,4)
                conv2d_nchw.local = ...
      for nn.3 (0,4)
        conv2d_nchw = ...

==================================================
No: 311	GFLOPS: 2531.50 / 6039.51	results: MeasureResult(cost:[0.0005], error_no:0, all_cost:3.08, Tstamp:1669890183.22)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 312	GFLOPS: 2641.63 / 6039.51	results: MeasureResult(cost:[0.0005], error_no:0, all_cost:3.46, Tstamp:1669890185.32)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for yy_c.3 (0,2)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 313	GFLOPS: 2643.95 / 6039.51	results: MeasureResult(cost:[0.0005], error_no:0, all_cost:3.33, Tstamp:1669890187.48)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 314	GFLOPS: 6037.09 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.30, Tstamp:1669890189.13)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for nn_c.3 (0,2)
              for yy_c.3 (0,2)
                for ry.2 (0,5)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 315	GFLOPS: 3683.90 / 6039.51	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.88, Tstamp:1669890191.36)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,8)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            vectorize ax0@ax1@ax2@ax3@.1 (0,5)
              pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for ry.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 316	GFLOPS: 3392.02 / 6039.51	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:4.34, Tstamp:1669890193.61)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            vectorize ax0@ax1@ax2@ax3@.1 (0,10)
              pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for ry.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 317	GFLOPS: 2990.80 / 6039.51	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:4.15, Tstamp:1669890195.94)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            vectorize ax0@ax1@ax2@ax3@.1 (0,4)
              pad_temp.shared = ...
        for rc.1 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 318	GFLOPS: 981.02 / 6039.51	results: MeasureResult(cost:[0.0013], error_no:0, all_cost:4.13, Tstamp:1669890198.26)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,224)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,112)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,16)
      for ry.0 (0,5)
        for rx.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,112)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,64)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,112)
              pad_temp.shared = ...
          for rc.1 (0,2)
            for nn_c.3 (0,2)
              for yy_c.3 (0,4)
                for xx_c.3 (0,2)
                  for xx_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,4)
        for xx.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 319	GFLOPS: 508.88 / 6039.51	results: MeasureResult(cost:[0.0025], error_no:0, all_cost:2.69, Tstamp:1669890200.15)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,392)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 16
      for rc.0 (0,8)
        for ry.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,144)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
              pad_temp.shared = ...
          for rc.1 (0,4)
            for rx.1 (0,5)
              for nn_c.3 (0,4)
                for yy_c.3 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 320	GFLOPS: 536.78 / 6039.51	results: MeasureResult(cost:[0.0024], error_no:0, all_cost:2.95, Tstamp:1669890201.80)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,224)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,16)
      for ax0@ax1@ax2@ax3@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,261)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for xx_c.3 (0,56)
            for rc.2 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,56)
        conv2d_nchw = ...

Time elapsed for measurement: 134.71 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 0.73 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 60	fail_ct: 4036	Time elapsed: 2.00
GA Iter: 0	Max score: 0.6853	Min score: 0.0264	#Pop: 60	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9987	Min score: 0.8868	#Pop: 128	#M+: 1402	#M-: 0
EvolutionarySearch		#s: 128	Time elapsed: 14.11
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************==================================================
No: 321	GFLOPS: 8499.17 / 8529.82	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669890232.60)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,2)
              for ry.2 (0,3)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 322	GFLOPS: 8516.24 / 8529.82	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669890234.29)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 323	GFLOPS: 8541.96 / 8541.96	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.23, Tstamp:1669890236.18)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for ry.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 324	GFLOPS: 8506.61 / 8541.96	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.00, Tstamp:1669890237.89)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.2 (0,3)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 325	GFLOPS: 8503.07 / 8541.96	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669890239.61)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 326	GFLOPS: 8568.57 / 8568.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669890241.32)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.2 (0,3)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 327	GFLOPS: 8502.89 / 8568.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.37, Tstamp:1669890243.29)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 328	GFLOPS: 8490.40 / 8568.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.99, Tstamp:1669890244.97)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 329	GFLOPS: 8569.35 / 8569.35	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669890246.68)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 330	GFLOPS: 8524.63 / 8569.35	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669890248.40)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,4)
            for rx.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 331	GFLOPS: 8499.40 / 8569.35	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.35, Tstamp:1669890250.39)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ry.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 332	GFLOPS: 6896.85 / 8569.35	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.24, Tstamp:1669890252.56)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,2)
              for ry.2 (0,3)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 333	GFLOPS: 6868.07 / 8569.35	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.31, Tstamp:1669890254.80)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,2)
              for ry.2 (0,3)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 334	GFLOPS: 8500.58 / 8569.35	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669890256.51)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ry.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 335	GFLOPS: 8496.91 / 8569.35	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.10, Tstamp:1669890258.25)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 336	GFLOPS: 6923.01 / 8569.35	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:3.71, Tstamp:1669890259.94)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.2 (0,3)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 337	GFLOPS: 6912.07 / 8569.35	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.40, Tstamp:1669890262.30)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for ry.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 338	GFLOPS: 8545.99 / 8569.35	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.09, Tstamp:1669890264.23)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 339	GFLOPS: 8462.87 / 8569.35	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669890265.93)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ry.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 340	GFLOPS: 8562.08 / 8569.35	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.99, Tstamp:1669890267.64)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,4)
            for rx.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 341	GFLOPS: 8532.12 / 8569.35	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.99, Tstamp:1669890269.34)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 342	GFLOPS: 6812.77 / 8569.35	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.48, Tstamp:1669890271.72)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for ry.2 (0,3)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 343	GFLOPS: 8576.52 / 8576.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.26, Tstamp:1669890273.65)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,4)
            for rx.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 344	GFLOPS: 8544.82 / 8576.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669890275.36)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 345	GFLOPS: 8442.32 / 8576.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669890277.06)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ry.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 346	GFLOPS: 8479.24 / 8576.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.80, Tstamp:1669890278.76)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 347	GFLOPS: 6796.21 / 8576.52	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.43, Tstamp:1669890281.10)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,4)
            for rx.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 348	GFLOPS: 6813.03 / 8576.52	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.44, Tstamp:1669890283.47)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ry.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 349	GFLOPS: 8484.73 / 8576.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.23, Tstamp:1669890285.38)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 350	GFLOPS: 8503.59 / 8576.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.98, Tstamp:1669890287.09)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,4)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 351	GFLOPS: 8508.42 / 8576.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.99, Tstamp:1669890288.80)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,4)
              for yy_c.3 (0,2)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 352	GFLOPS: 8525.88 / 8576.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.80, Tstamp:1669890290.50)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,4)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 353	GFLOPS: 7979.62 / 8576.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669890292.45)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 354	GFLOPS: 8558.02 / 8576.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.79, Tstamp:1669890294.15)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,4)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 355	GFLOPS: 8553.85 / 8576.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.82, Tstamp:1669890295.85)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for rx.2 (0,3)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 356	GFLOPS: 8059.86 / 8576.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.78, Tstamp:1669890297.52)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,4)
              for yy_c.3 (0,2)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 357	GFLOPS: 8057.30 / 8576.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669890299.42)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for yy_c.3 (0,2)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 358	GFLOPS: 8528.93 / 8576.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.91, Tstamp:1669890301.13)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,4)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 359	GFLOPS: 8537.34 / 8576.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.94, Tstamp:1669890302.82)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,4)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 360	GFLOPS: 7974.52 / 8576.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.94, Tstamp:1669890304.53)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 361	GFLOPS: 6891.23 / 8576.52	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.08, Tstamp:1669890306.45)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 362	GFLOPS: 8060.71 / 8576.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.81, Tstamp:1669890308.13)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for yy_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 363	GFLOPS: 8060.90 / 8576.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.80, Tstamp:1669890309.82)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,4)
              for yy_c.3 (0,2)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 364	GFLOPS: 8049.27 / 8576.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.74, Tstamp:1669890311.51)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 365	GFLOPS: 8025.35 / 8576.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669890313.41)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for nn_c.3 (0,4)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 366	GFLOPS: 8034.95 / 8576.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.81, Tstamp:1669890315.10)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,4)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 367	GFLOPS: 8038.75 / 8576.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.74, Tstamp:1669890316.78)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 368	GFLOPS: 8047.79 / 8576.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.79, Tstamp:1669890318.49)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for yy_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 369	GFLOPS: 7994.02 / 8576.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.94, Tstamp:1669890320.39)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for yy_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 370	GFLOPS: 8011.69 / 8576.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.73, Tstamp:1669890322.06)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,4)
              for yy_c.3 (0,2)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 371	GFLOPS: 7981.97 / 8576.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.66, Tstamp:1669890323.76)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for yy_c.3 (0,2)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 372	GFLOPS: 8024.21 / 8576.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.75, Tstamp:1669890325.48)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rx.1 (0,3)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 373	GFLOPS: 8026.17 / 8576.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.93, Tstamp:1669890327.37)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 374	GFLOPS: 8028.20 / 8576.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.68, Tstamp:1669890329.05)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 375	GFLOPS: 8010.72 / 8576.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.80, Tstamp:1669890330.74)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.2 (0,3)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 376	GFLOPS: 8011.32 / 8576.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.66, Tstamp:1669890332.45)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,4)
              for ff_c.3 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 377	GFLOPS: 8037.87 / 8576.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.05, Tstamp:1669890334.27)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,4)
            for rx.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 378	GFLOPS: 8039.11 / 8576.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.92, Tstamp:1669890335.96)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ry.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 379	GFLOPS: 8013.25 / 8576.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.90, Tstamp:1669890337.65)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 380	GFLOPS: 8082.67 / 8576.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.82, Tstamp:1669890339.42)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 381	GFLOPS: 8005.25 / 8576.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.07, Tstamp:1669890341.31)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 382	GFLOPS: 3484.32 / 8576.52	results: MeasureResult(cost:[0.0005], error_no:0, all_cost:2.76, Tstamp:1669890343.48)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,3616)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,452)
    conv2d_nchw.local auto_unroll: 16
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,452)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,452)
          pad_temp.shared = ...
      for ry.2 (0,3)
        for rx.2 (0,3)
          for ff_c.4 (0,2)
            conv2d_nchw.local = ...
    for ff.3 (0,2)
      conv2d_nchw = ...

==================================================
No: 383	GFLOPS: 744.00 / 8576.52	results: MeasureResult(cost:[0.0025], error_no:0, all_cost:2.80, Tstamp:1669890345.25)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,25538)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,2)
      for rx.0 (0,3)
        for ax0@ax1@ax2@ax3@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,32)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,4)
          for nn_c.3 (0,2)
            for rc.2 (0,4)
              for ry.2 (0,3)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      conv2d_nchw = ...

==================================================
No: 384	GFLOPS: 2786.55 / 8576.52	results: MeasureResult(cost:[0.0007], error_no:0, all_cost:2.32, Tstamp:1669890346.95)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,226)
      conv2d_nchw.local auto_unroll: 16
      for rc.0 (0,8)
        for rx.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,17)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
              pad_temp.shared = ...
          for rc.1 (0,2)
            for ff_c.3 (0,2)
              for xx_c.3 (0,2)
                for rc.2 (0,2)
                  for ry.2 (0,3)
                    conv2d_nchw.local = ...
      for ff.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

Time elapsed for measurement: 128.19 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 0.76 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Iter: 5	#Pop: 32	#Target: 50	fail_ct: 10208	Time elapsed: 4.17
Sample Initial Population	#s: 59	fail_ct: 18373	Time elapsed: 7.44
GA Iter: 0	Max score: 0.8164	Min score: -0.0208	#Pop: 59	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9991	Min score: 0.9067	#Pop: 128	#M+: 1389	#M-: 0
EvolutionarySearch		#s: 128	Time elapsed: 14.04
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************==================================================
No: 385	GFLOPS: 5968.08 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.52, Tstamp:1669890384.97)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 386	GFLOPS: 5959.56 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.86, Tstamp:1669890386.96)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for rc.2 (0,2)
                for rx.2 (0,5)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 387	GFLOPS: 5981.76 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.57, Tstamp:1669890388.63)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for ry.2 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 388	GFLOPS: 5941.88 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.57, Tstamp:1669890390.30)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for nn_c.3 (0,2)
              for yy_c.3 (0,2)
                for ry.2 (0,5)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 389	GFLOPS: 5924.93 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.56, Tstamp:1669890392.08)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 390	GFLOPS: 5948.66 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.86, Tstamp:1669890393.95)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for nn_c.3 (0,2)
                for yy_c.3 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 391	GFLOPS: 5949.11 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.54, Tstamp:1669890395.58)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.2 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 392	GFLOPS: 5938.03 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.63, Tstamp:1669890397.22)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for yy_c.3 (0,2)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 393	GFLOPS: 6014.37 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.81, Tstamp:1669890399.02)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for rc.2 (0,2)
              for ry.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 394	GFLOPS: 5987.65 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.73, Tstamp:1669890400.87)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for rc.2 (0,2)
              for ry.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 395	GFLOPS: 5973.13 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.61, Tstamp:1669890402.52)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for yy_c.3 (0,2)
          for rc.2 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 396	GFLOPS: 5981.69 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.59, Tstamp:1669890404.21)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for nn_c.3 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 397	GFLOPS: 5934.98 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.46, Tstamp:1669890405.97)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for yy_c.3 (0,2)
            for rc.2 (0,2)
              for rx.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 398	GFLOPS: 5973.54 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.80, Tstamp:1669890407.84)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for ry.2 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 399	GFLOPS: 5965.59 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.58, Tstamp:1669890409.56)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for nn_c.3 (0,2)
              for yy_c.3 (0,2)
                for ry.2 (0,5)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 400	GFLOPS: 5893.44 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.64, Tstamp:1669890411.27)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for ry.2 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 401	GFLOPS: 5941.64 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.57, Tstamp:1669890413.02)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for nn_c.3 (0,2)
              for yy_c.3 (0,2)
                for ry.2 (0,5)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 402	GFLOPS: 5898.57 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.67, Tstamp:1669890414.88)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for nn_c.3 (0,2)
              for yy_c.3 (0,2)
                for ry.2 (0,5)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 403	GFLOPS: 6019.30 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.51, Tstamp:1669890416.52)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for rc.2 (0,2)
              for ry.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 404	GFLOPS: 5944.84 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.51, Tstamp:1669890418.17)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 405	GFLOPS: 5957.14 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.53, Tstamp:1669890419.97)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.2 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 406	GFLOPS: 5940.02 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.76, Tstamp:1669890421.87)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for nn_c.3 (0,2)
                for yy_c.3 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 407	GFLOPS: 5909.82 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.53, Tstamp:1669890423.52)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rc.2 (0,2)
            for rx.2 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 408	GFLOPS: 5964.44 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.52, Tstamp:1669890425.21)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 409	GFLOPS: 5893.86 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.52, Tstamp:1669890427.07)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for nn_c.3 (0,2)
              for yy_c.3 (0,2)
                for ry.2 (0,5)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 410	GFLOPS: 5971.45 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.76, Tstamp:1669890428.95)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,5)
                for rx.2 (0,5)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 411	GFLOPS: 5867.38 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.57, Tstamp:1669890430.61)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for nn_c.3 (0,2)
                for yy_c.3 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 412	GFLOPS: 5877.54 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.53, Tstamp:1669890432.29)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for yy_c.3 (0,2)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 413	GFLOPS: 5862.91 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.59, Tstamp:1669890434.23)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.2 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 414	GFLOPS: 5894.23 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.75, Tstamp:1669890436.10)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for ry.2 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 415	GFLOPS: 5964.09 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.57, Tstamp:1669890437.76)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for rc.2 (0,2)
            for ry.2 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 416	GFLOPS: 5974.17 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.56, Tstamp:1669890439.40)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for rc.2 (0,2)
              for ry.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 417	GFLOPS: 5923.54 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.56, Tstamp:1669890441.19)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 418	GFLOPS: 5947.30 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.63, Tstamp:1669890443.04)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for yy_c.3 (0,2)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 419	GFLOPS: 5916.98 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.46, Tstamp:1669890444.67)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for nn_c.3 (0,2)
                for yy_c.3 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 420	GFLOPS: 5858.61 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.41, Tstamp:1669890446.33)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for yy_c.3 (0,2)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 421	GFLOPS: 5939.81 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.52, Tstamp:1669890448.13)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rc.2 (0,2)
            for rx.2 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 422	GFLOPS: 5871.18 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.76, Tstamp:1669890449.97)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.2 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 423	GFLOPS: 5885.87 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.43, Tstamp:1669890451.61)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for nn_c.3 (0,2)
                for yy_c.3 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 424	GFLOPS: 5968.38 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.57, Tstamp:1669890453.25)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for rc.2 (0,2)
              for ry.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 425	GFLOPS: 5964.75 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.70, Tstamp:1669890455.13)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for rc.2 (0,2)
              for ry.2 (0,5)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 426	GFLOPS: 5961.48 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.89, Tstamp:1669890457.09)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for yy_c.3 (0,2)
          for rc.2 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 427	GFLOPS: 5939.12 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.49, Tstamp:1669890458.78)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 428	GFLOPS: 5941.89 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.51, Tstamp:1669890460.44)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for yy_c.3 (0,2)
              for rx.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 429	GFLOPS: 5974.57 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.37, Tstamp:1669890462.08)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for rc.2 (0,2)
            for ry.2 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 430	GFLOPS: 6000.56 / 6039.51	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.78, Tstamp:1669890463.95)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for rc.2 (0,2)
              for ry.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 431	GFLOPS: 6043.57 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.11, Tstamp:1669890465.62)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for nn_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 432	GFLOPS: 5970.91 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.46, Tstamp:1669890467.27)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for nn_c.3 (0,2)
              for yy_c.3 (0,2)
                for rx.2 (0,5)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 433	GFLOPS: 5977.56 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.49, Tstamp:1669890469.04)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for yy_c.3 (0,2)
          for rc.2 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 434	GFLOPS: 5969.67 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.74, Tstamp:1669890470.94)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for nn_c.3 (0,2)
              for rx.2 (0,5)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 435	GFLOPS: 5941.52 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.38, Tstamp:1669890472.60)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for yy_c.3 (0,2)
            for rc.2 (0,2)
              for rx.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 436	GFLOPS: 5978.68 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.35, Tstamp:1669890474.27)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 437	GFLOPS: 5945.39 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.31, Tstamp:1669890476.17)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for nn_c.3 (0,2)
              for yy_c.3 (0,2)
                for rx.2 (0,5)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 438	GFLOPS: 5935.26 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.88, Tstamp:1669890478.04)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 439	GFLOPS: 5927.79 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.39, Tstamp:1669890479.70)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for nn_c.3 (0,2)
                for yy_c.3 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 440	GFLOPS: 5929.80 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.35, Tstamp:1669890481.37)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.2 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 441	GFLOPS: 5907.22 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.48, Tstamp:1669890483.13)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for nn_c.3 (0,2)
              for yy_c.3 (0,2)
                for ry.2 (0,5)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 442	GFLOPS: 5917.81 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.71, Tstamp:1669890484.99)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 443	GFLOPS: 4713.45 / 6043.57	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:3.26, Tstamp:1669890486.63)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rc.2 (0,2)
            for rx.2 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 444	GFLOPS: 5915.84 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.16, Tstamp:1669890488.31)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 445	GFLOPS: 5880.26 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.24, Tstamp:1669890490.11)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.2 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 446	GFLOPS: 1303.65 / 6043.57	results: MeasureResult(cost:[0.0010], error_no:0, all_cost:2.61, Tstamp:1669890491.95)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,392)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,72)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for xx_c.3 (0,4)
            for ry.2 (0,5)
              for rx.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for xx.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 447	GFLOPS: 5796.88 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.10, Tstamp:1669890493.59)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 448	GFLOPS: 5910.12 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.07, Tstamp:1669890495.28)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for ry.2 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

Time elapsed for measurement: 126.21 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 0.81 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 76	fail_ct: 4020	Time elapsed: 2.11
GA Iter: 0	Max score: 0.6919	Min score: -0.0044	#Pop: 76	#M+: 0	#M-: 0
GA Iter: 4	Max score: 1.0335	Min score: 0.9349	#Pop: 128	#M+: 1387	#M-: 0
EvolutionarySearch		#s: 128	Time elapsed: 14.00
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************==================================================
No: 449	GFLOPS: 8531.37 / 8576.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669890526.51)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 450	GFLOPS: 8502.04 / 8576.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669890528.22)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 451	GFLOPS: 8548.30 / 8576.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669890529.91)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 452	GFLOPS: 8578.72 / 8578.72	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.17, Tstamp:1669890531.74)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for rx.2 (0,3)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 453	GFLOPS: 8011.48 / 8578.72	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.83, Tstamp:1669890533.44)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for nn_c.3 (0,4)
        for yy_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 454	GFLOPS: 8511.41 / 8578.72	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669890535.14)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,4)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 455	GFLOPS: 8507.43 / 8578.72	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669890536.84)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ff_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 456	GFLOPS: 8493.19 / 8578.72	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669890538.52)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 457	GFLOPS: 8496.19 / 8578.72	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.07, Tstamp:1669890540.22)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for ff_c.3 (0,2)
                for nn_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 458	GFLOPS: 8507.58 / 8578.72	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.06, Tstamp:1669890541.96)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 459	GFLOPS: 8486.73 / 8578.72	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669890543.67)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 460	GFLOPS: 8561.01 / 8578.72	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.08, Tstamp:1669890545.49)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 461	GFLOPS: 8558.46 / 8578.72	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.08, Tstamp:1669890547.21)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 462	GFLOPS: 8502.94 / 8578.72	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.00, Tstamp:1669890549.06)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 463	GFLOPS: 8499.27 / 8578.72	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.22, Tstamp:1669890550.95)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for ff_c.3 (0,2)
                for nn_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 464	GFLOPS: 8491.69 / 8578.72	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.00, Tstamp:1669890552.66)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 465	GFLOPS: 8511.20 / 8578.72	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669890554.36)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 466	GFLOPS: 8538.86 / 8578.72	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.09, Tstamp:1669890556.08)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for ry.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 467	GFLOPS: 8531.05 / 8578.72	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.25, Tstamp:1669890558.00)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,4)
            for rx.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 468	GFLOPS: 8513.25 / 8578.72	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.98, Tstamp:1669890559.70)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,4)
              for ff_c.3 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 469	GFLOPS: 8499.93 / 8578.72	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.07, Tstamp:1669890561.40)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 470	GFLOPS: 8555.67 / 8578.72	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.06, Tstamp:1669890563.11)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 471	GFLOPS: 8432.23 / 8578.72	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.25, Tstamp:1669890565.02)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 472	GFLOPS: 8548.84 / 8578.72	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669890566.70)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 473	GFLOPS: 8494.83 / 8578.72	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.00, Tstamp:1669890568.40)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for yy_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 474	GFLOPS: 8018.84 / 8578.72	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.87, Tstamp:1669890570.10)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for nn_c.3 (0,4)
        for ff_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 475	GFLOPS: 8017.74 / 8578.72	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669890572.01)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,4)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 476	GFLOPS: 8021.14 / 8578.72	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.86, Tstamp:1669890573.70)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for nn_c.3 (0,4)
        for ff_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 477	GFLOPS: 8573.97 / 8578.72	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.99, Tstamp:1669890575.39)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 478	GFLOPS: 8514.24 / 8578.72	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.96, Tstamp:1669890577.17)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for ry.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 479	GFLOPS: 8564.27 / 8578.72	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.22, Tstamp:1669890579.09)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.2 (0,3)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 480	GFLOPS: 8554.83 / 8578.72	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.95, Tstamp:1669890580.77)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 481	GFLOPS: 8572.81 / 8578.72	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.07, Tstamp:1669890582.49)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 482	GFLOPS: 8507.41 / 8578.72	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.94, Tstamp:1669890584.24)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 483	GFLOPS: 8504.52 / 8578.72	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.10, Tstamp:1669890586.14)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,4)
              for ff_c.3 (0,2)
                for yy_c.3 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 484	GFLOPS: 8500.88 / 8578.72	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669890587.89)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,4)
              for ff_c.3 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 485	GFLOPS: 8021.39 / 8578.72	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.83, Tstamp:1669890589.57)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,2)
          for rx.2 (0,3)
            for nn_c.4 (0,2)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 486	GFLOPS: 8579.52 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.97, Tstamp:1669890591.26)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for yy_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 487	GFLOPS: 8568.28 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.27, Tstamp:1669890593.25)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for yy_c.3 (0,2)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 488	GFLOPS: 8556.98 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.99, Tstamp:1669890594.96)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,4)
              for yy_c.3 (0,2)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 489	GFLOPS: 8558.93 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.96, Tstamp:1669890596.68)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 490	GFLOPS: 8569.26 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.98, Tstamp:1669890598.39)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,4)
              for ff_c.3 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 491	GFLOPS: 8553.21 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.27, Tstamp:1669890600.39)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 492	GFLOPS: 8505.00 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.97, Tstamp:1669890602.10)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for ry.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 493	GFLOPS: 8505.05 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.88, Tstamp:1669890603.85)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,2)
              for ry.2 (0,3)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 494	GFLOPS: 8526.00 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.83, Tstamp:1669890605.56)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,2)
              for rx.2 (0,3)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 495	GFLOPS: 8026.44 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.00, Tstamp:1669890607.43)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,2)
          for ry.2 (0,3)
            for nn_c.4 (0,2)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 496	GFLOPS: 8014.87 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.99, Tstamp:1669890609.34)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for nn_c.4 (0,2)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 497	GFLOPS: 7994.50 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.79, Tstamp:1669890611.03)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,2)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 498	GFLOPS: 8540.96 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.94, Tstamp:1669890612.74)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 499	GFLOPS: 8529.47 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.91, Tstamp:1669890614.51)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 500	GFLOPS: 8520.91 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.17, Tstamp:1669890616.48)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,4)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 501	GFLOPS: 8527.29 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.80, Tstamp:1669890618.18)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for yy_c.3 (0,2)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 502	GFLOPS: 8533.77 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.93, Tstamp:1669890619.91)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,4)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 503	GFLOPS: 8539.70 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.89, Tstamp:1669890621.62)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,2)
              for rx.2 (0,3)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 504	GFLOPS: 8016.12 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.00, Tstamp:1669890623.53)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 505	GFLOPS: 8017.74 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.92, Tstamp:1669890625.23)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,4)
          for ry.2 (0,3)
            for ff_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 506	GFLOPS: 8035.27 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.94, Tstamp:1669890626.94)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for nn_c.3 (0,4)
        for ry.2 (0,3)
          for rx.2 (0,3)
            for ff_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 507	GFLOPS: 8029.39 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.79, Tstamp:1669890628.63)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for yy_c.3 (0,2)
            for nn_c.4 (0,4)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 508	GFLOPS: 8572.05 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.23, Tstamp:1669890630.53)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,4)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 509	GFLOPS: 8565.54 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.94, Tstamp:1669890632.22)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for yy_c.3 (0,2)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 510	GFLOPS: 1808.87 / 8579.52	results: MeasureResult(cost:[0.0010], error_no:0, all_cost:2.96, Tstamp:1669890634.38)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,8)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,4)
        for rx.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,66)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
              pad_temp.shared = ...
          for rc.1 (0,8)
            for ry.1 (0,3)
              for ff_c.3 (0,4)
                conv2d_nchw.local = ...
      for ff.3 (0,4)
        conv2d_nchw = ...

==================================================
No: 511	GFLOPS: 8528.49 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.98, Tstamp:1669890636.08)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 512	GFLOPS: 8032.17 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.85, Tstamp:1669890637.82)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for nn_c.3 (0,4)
        for ry.2 (0,3)
          for rx.2 (0,3)
            for ff_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

Time elapsed for measurement: 125.29 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 0.74 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Iter: 5	#Pop: 41	#Target: 50	fail_ct: 10199	Time elapsed: 4.13
Sample Initial Population	#s: 56	fail_ct: 14280	Time elapsed: 5.85
GA Iter: 0	Max score: 0.7864	Min score: 0.0358	#Pop: 56	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9942	Min score: 0.9597	#Pop: 128	#M+: 1382	#M-: 0
EvolutionarySearch		#s: 128	Time elapsed: 14.38
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************==================================================
No: 513	GFLOPS: 5972.56 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.67, Tstamp:1669890675.89)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for nn_c.3 (0,2)
              for yy_c.3 (0,2)
                for rx.2 (0,5)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 514	GFLOPS: 5791.43 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.55, Tstamp:1669890677.60)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for ry.2 (0,5)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 515	GFLOPS: 5766.64 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.48, Tstamp:1669890679.24)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,5)
                conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 516	GFLOPS: 6019.28 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.76, Tstamp:1669890680.91)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for rc.2 (0,2)
            for ry.2 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 517	GFLOPS: 5942.63 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.82, Tstamp:1669890682.82)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for nn_c.3 (0,2)
              for yy_c.3 (0,2)
                for rx.2 (0,5)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 518	GFLOPS: 5967.29 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.62, Tstamp:1669890684.49)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.2 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 519	GFLOPS: 5974.79 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.55, Tstamp:1669890686.16)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 520	GFLOPS: 5915.43 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.63, Tstamp:1669890688.01)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rc.2 (0,2)
            for rx.2 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 521	GFLOPS: 5971.91 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.72, Tstamp:1669890689.88)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for nn_c.3 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 522	GFLOPS: 5948.01 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.69, Tstamp:1669890691.54)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.2 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 523	GFLOPS: 6025.70 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669890693.22)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for ry.2 (0,5)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 524	GFLOPS: 5981.77 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.59, Tstamp:1669890695.10)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for rc.2 (0,2)
            for ry.2 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 525	GFLOPS: 5777.53 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.78, Tstamp:1669890696.95)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for nn_c.4 (0,2)
                conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 526	GFLOPS: 5966.16 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.61, Tstamp:1669890698.61)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for rc.2 (0,2)
              for ry.2 (0,5)
                for rx.2 (0,5)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 527	GFLOPS: 5963.87 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.65, Tstamp:1669890700.28)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,5)
                for rx.2 (0,5)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 528	GFLOPS: 5940.79 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.56, Tstamp:1669890702.14)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.2 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 529	GFLOPS: 5953.51 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.70, Tstamp:1669890704.00)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for nn_c.3 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 530	GFLOPS: 5874.62 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.56, Tstamp:1669890705.64)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for nn_c.3 (0,2)
              for yy_c.3 (0,2)
                for rx.2 (0,5)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 531	GFLOPS: 5965.74 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.51, Tstamp:1669890707.27)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for rc.2 (0,2)
                for ry.2 (0,5)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 532	GFLOPS: 5949.54 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.74, Tstamp:1669890709.10)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for nn_c.3 (0,2)
              for rx.2 (0,5)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 533	GFLOPS: 5870.64 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.81, Tstamp:1669890710.97)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for nn_c.3 (0,2)
              for yy_c.3 (0,2)
                for rx.2 (0,5)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 534	GFLOPS: 5980.60 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.61, Tstamp:1669890712.66)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for rc.2 (0,2)
              for ry.2 (0,5)
                for rx.2 (0,5)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 535	GFLOPS: 5941.26 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.59, Tstamp:1669890714.32)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for nn_c.3 (0,2)
              for ry.2 (0,5)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 536	GFLOPS: 5971.31 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.54, Tstamp:1669890716.07)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for rc.2 (0,2)
              for ry.2 (0,5)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 537	GFLOPS: 5957.46 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.76, Tstamp:1669890717.96)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for ry.2 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 538	GFLOPS: 5953.40 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.57, Tstamp:1669890719.62)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for rc.2 (0,2)
            for ry.2 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 539	GFLOPS: 5951.37 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.62, Tstamp:1669890721.29)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for nn_c.3 (0,2)
          for rc.2 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 540	GFLOPS: 5977.17 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.54, Tstamp:1669890723.14)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for nn_c.3 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 541	GFLOPS: 5901.58 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.72, Tstamp:1669890725.02)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for nn_c.3 (0,2)
            for rc.2 (0,2)
              for rx.2 (0,5)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 542	GFLOPS: 5987.22 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.43, Tstamp:1669890726.67)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for rc.2 (0,2)
              for ry.2 (0,5)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 543	GFLOPS: 5950.79 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.52, Tstamp:1669890728.32)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for nn_c.3 (0,2)
              for ry.2 (0,5)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 544	GFLOPS: 6015.07 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.09, Tstamp:1669890730.20)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.2 (0,5)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 545	GFLOPS: 6010.65 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.34, Tstamp:1669890732.10)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 546	GFLOPS: 5990.72 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.52, Tstamp:1669890733.76)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for rc.2 (0,2)
            for ry.2 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 547	GFLOPS: 5928.80 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.58, Tstamp:1669890735.42)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for ry.2 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 548	GFLOPS: 5916.53 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.54, Tstamp:1669890737.27)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 549	GFLOPS: 5945.13 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.84, Tstamp:1669890739.24)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rc.2 (0,2)
            for rx.2 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 550	GFLOPS: 5931.34 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.47, Tstamp:1669890740.90)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.2 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 551	GFLOPS: 5943.19 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.78, Tstamp:1669890742.58)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for rc.2 (0,2)
              for ry.2 (0,5)
                for rx.2 (0,5)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 552	GFLOPS: 5934.01 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.48, Tstamp:1669890744.33)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,5)
                for rx.2 (0,5)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 553	GFLOPS: 5898.96 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.57, Tstamp:1669890746.21)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for nn_c.3 (0,2)
              for yy_c.3 (0,2)
                for ry.2 (0,5)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 554	GFLOPS: 5917.79 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.49, Tstamp:1669890747.86)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for rc.2 (0,2)
                for rx.2 (0,5)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 555	GFLOPS: 5985.92 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.46, Tstamp:1669890749.52)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for rc.2 (0,2)
                for ry.2 (0,5)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 556	GFLOPS: 5948.15 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.44, Tstamp:1669890751.30)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for nn_c.3 (0,2)
          for rc.2 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 557	GFLOPS: 5953.29 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.73, Tstamp:1669890753.16)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for rc.2 (0,2)
              for ry.2 (0,5)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 558	GFLOPS: 5925.57 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.58, Tstamp:1669890754.83)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 559	GFLOPS: 5933.72 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.42, Tstamp:1669890756.50)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.2 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 560	GFLOPS: 5939.81 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.47, Tstamp:1669890758.27)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.2 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 561	GFLOPS: 5949.77 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.71, Tstamp:1669890760.14)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rc.2 (0,2)
            for rx.2 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 562	GFLOPS: 5948.73 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.52, Tstamp:1669890761.82)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,5)
                for rx.2 (0,5)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 563	GFLOPS: 5946.55 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.50, Tstamp:1669890763.47)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for rc.2 (0,2)
            for ry.2 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 564	GFLOPS: 5969.85 / 6043.57	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.39, Tstamp:1669890765.26)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for rc.2 (0,2)
                for ry.2 (0,5)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 565	GFLOPS: 1736.32 / 6043.57	results: MeasureResult(cost:[0.0007], error_no:0, all_cost:8.21, Tstamp:1669890767.58)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,8)
      for rx.0 (0,5)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,40)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for rc.2 (0,4)
              for ry.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 566	GFLOPS: 6681.61 / 6681.61	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.48, Tstamp:1669890769.47)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 567	GFLOPS: 5846.86 / 6681.61	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.44, Tstamp:1669890771.12)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 568	GFLOPS: 5853.88 / 6681.61	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.53, Tstamp:1669890772.78)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.2 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 569	GFLOPS: 5845.60 / 6681.61	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.73, Tstamp:1669890774.66)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 570	GFLOPS: 5875.58 / 6681.61	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.95, Tstamp:1669890776.53)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,5)
                for rx.2 (0,5)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 571	GFLOPS: 5834.78 / 6681.61	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.64, Tstamp:1669890778.19)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for rc.2 (0,2)
                for rx.2 (0,5)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 572	GFLOPS: 5871.33 / 6681.61	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.38, Tstamp:1669890779.85)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 573	GFLOPS: 5852.48 / 6681.61	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.32, Tstamp:1669890781.72)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 574	GFLOPS: 1899.99 / 6681.61	results: MeasureResult(cost:[0.0007], error_no:0, all_cost:3.62, Tstamp:1669890784.07)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,784)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
      for rc.0 (0,8)
        for rx.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,160)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
              pad_temp.shared = ...
          for nn_c.3 (0,4)
            for rc.2 (0,4)
              for ry.2 (0,5)
                for nn_c.4 (0,2)
                  for xx_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,8)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 575	GFLOPS: 912.30 / 6681.61	results: MeasureResult(cost:[0.0014], error_no:0, all_cost:5.82, Tstamp:1669890786.51)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,112)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,112)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        for ry.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,112)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,80)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,112)
              pad_temp.shared = ...
          for rx.1 (0,5)
            for nn_c.3 (0,2)
              for yy_c.3 (0,8)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for yy.3 (0,8)
          conv2d_nchw = ...

==================================================
No: 576	GFLOPS: 509.10 / 6681.61	results: MeasureResult(cost:[0.0025], error_no:0, all_cost:2.52, Tstamp:1669890788.34)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,448)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,56)
      conv2d_nchw.local auto_unroll: 16
      for rc.0 (0,32)
        for rx.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,42)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
              pad_temp.shared = ...
          for ry.1 (0,5)
            for nn_c.3 (0,4)
              for nn_c.4 (0,2)
                conv2d_nchw.local = ...
      for nn.3 (0,8)
        conv2d_nchw = ...

Time elapsed for measurement: 129.44 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 0.87 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 65	fail_ct: 4031	Time elapsed: 2.10
GA Iter: 0	Max score: 0.6263	Min score: 0.0331	#Pop: 65	#M+: 0	#M-: 0
GA Iter: 4	Max score: 1.0280	Min score: 0.9351	#Pop: 128	#M+: 1382	#M-: 0
EvolutionarySearch		#s: 128	Time elapsed: 14.10
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************==================================================
No: 577	GFLOPS: 8557.40 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.24, Tstamp:1669890819.59)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for rx.2 (0,3)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 578	GFLOPS: 8555.70 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.22, Tstamp:1669890821.31)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for rx.2 (0,3)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 579	GFLOPS: 8516.20 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.06, Tstamp:1669890823.02)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for rx.2 (0,3)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 580	GFLOPS: 8569.05 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.06, Tstamp:1669890824.75)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for ff_c.3 (0,2)
                for yy_c.3 (0,2)
                  for nn_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 581	GFLOPS: 8473.94 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.17, Tstamp:1669890826.63)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 582	GFLOPS: 8460.44 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669890828.35)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 583	GFLOPS: 8499.47 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669890830.06)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for ry.2 (0,3)
                for nn_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 584	GFLOPS: 8486.29 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.11, Tstamp:1669890831.78)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for ry.2 (0,3)
                for nn_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 585	GFLOPS: 8548.73 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.21, Tstamp:1669890833.68)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 586	GFLOPS: 8493.61 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.00, Tstamp:1669890835.40)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ff_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 587	GFLOPS: 8503.82 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.98, Tstamp:1669890837.10)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for ff_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 588	GFLOPS: 8487.67 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669890838.81)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for ff_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 589	GFLOPS: 8530.72 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.29, Tstamp:1669890840.75)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 590	GFLOPS: 8510.59 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669890842.45)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 591	GFLOPS: 8499.35 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669890844.15)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 592	GFLOPS: 8567.29 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669890845.86)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,4)
              for ff_c.3 (0,2)
                for yy_c.3 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 593	GFLOPS: 8558.38 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.14, Tstamp:1669890847.75)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 594	GFLOPS: 8559.83 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.06, Tstamp:1669890849.46)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,4)
              for ff_c.3 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 595	GFLOPS: 8497.41 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669890851.18)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,2)
              for ry.2 (0,3)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 596	GFLOPS: 8568.60 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.13, Tstamp:1669890852.92)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,2)
              for rx.2 (0,3)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 597	GFLOPS: 8485.60 / 8579.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.28, Tstamp:1669890854.84)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,2)
              for ry.2 (0,3)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 598	GFLOPS: 8588.36 / 8588.36	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.95, Tstamp:1669890856.54)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,2)
              for rx.2 (0,3)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 599	GFLOPS: 8446.63 / 8588.36	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.93, Tstamp:1669890858.25)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 600	GFLOPS: 8560.32 / 8588.36	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.07, Tstamp:1669890859.95)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 601	GFLOPS: 8571.57 / 8588.36	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.26, Tstamp:1669890861.86)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for ff_c.3 (0,2)
                for nn_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 602	GFLOPS: 8514.92 / 8588.36	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.06, Tstamp:1669890863.57)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for ry.2 (0,3)
                for nn_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 603	GFLOPS: 8527.81 / 8588.36	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.00, Tstamp:1669890865.29)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for ry.2 (0,3)
                for nn_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 604	GFLOPS: 8565.15 / 8588.36	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669890867.00)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,4)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 605	GFLOPS: 8546.98 / 8588.36	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.13, Tstamp:1669890868.90)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for yy_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 606	GFLOPS: 8517.86 / 8588.36	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669890870.60)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 607	GFLOPS: 8548.88 / 8588.36	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669890872.30)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for yy_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 608	GFLOPS: 8588.52 / 8588.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.08, Tstamp:1669890874.10)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for yy_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 609	GFLOPS: 8569.06 / 8588.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.33, Tstamp:1669890876.05)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,4)
              for yy_c.3 (0,2)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 610	GFLOPS: 8569.91 / 8588.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669890877.76)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for yy_c.3 (0,2)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 611	GFLOPS: 8572.28 / 8588.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669890879.48)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for yy_c.3 (0,2)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 612	GFLOPS: 8462.72 / 8588.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669890881.18)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for ff_c.3 (0,2)
              for nn_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 613	GFLOPS: 8430.16 / 8588.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.15, Tstamp:1669890883.09)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 614	GFLOPS: 8485.55 / 8588.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.98, Tstamp:1669890884.79)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for yy_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 615	GFLOPS: 8514.01 / 8588.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.97, Tstamp:1669890886.60)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,4)
              for yy_c.3 (0,2)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 616	GFLOPS: 8553.67 / 8588.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.84, Tstamp:1669890888.31)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 617	GFLOPS: 8488.72 / 8588.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669890890.17)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for yy_c.3 (0,2)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 618	GFLOPS: 8505.79 / 8588.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.24, Tstamp:1669890892.07)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,4)
              for ff_c.3 (0,2)
                for yy_c.3 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 619	GFLOPS: 8514.43 / 8588.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669890893.78)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for yy_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 620	GFLOPS: 8482.34 / 8588.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.87, Tstamp:1669890895.48)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for yy_c.3 (0,2)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 621	GFLOPS: 8508.04 / 8588.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669890897.21)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.2 (0,3)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 622	GFLOPS: 8514.78 / 8588.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.26, Tstamp:1669890899.11)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for yy_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 623	GFLOPS: 8542.60 / 8588.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.81, Tstamp:1669890900.82)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for nn_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 624	GFLOPS: 8577.41 / 8588.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.89, Tstamp:1669890902.51)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for ry.2 (0,3)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 625	GFLOPS: 8552.86 / 8588.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.77, Tstamp:1669890904.22)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for ry.2 (0,3)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 626	GFLOPS: 8483.33 / 8588.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.15, Tstamp:1669890906.14)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 627	GFLOPS: 8476.47 / 8588.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.86, Tstamp:1669890907.84)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 628	GFLOPS: 8462.23 / 8588.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.72, Tstamp:1669890909.56)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for ry.2 (0,3)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 629	GFLOPS: 8578.28 / 8588.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.87, Tstamp:1669890911.26)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 630	GFLOPS: 8577.75 / 8588.52	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.94, Tstamp:1669890913.14)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 631	GFLOPS: 8596.00 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.86, Tstamp:1669890914.85)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 632	GFLOPS: 8509.76 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.80, Tstamp:1669890916.55)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 633	GFLOPS: 8500.56 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.09, Tstamp:1669890918.35)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 634	GFLOPS: 8462.16 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.43, Tstamp:1669890920.30)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 635	GFLOPS: 8492.88 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.10, Tstamp:1669890922.00)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 636	GFLOPS: 8040.83 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.88, Tstamp:1669890923.71)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for rx.2 (0,3)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 637	GFLOPS: 8032.73 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.87, Tstamp:1669890925.41)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for rx.2 (0,3)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 638	GFLOPS: 695.53 / 8596.00	results: MeasureResult(cost:[0.0027], error_no:0, all_cost:2.83, Tstamp:1669890927.32)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,452)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,226)
      for rc.0 (0,8)
        for ry.0 (0,3)
          for rx.0 (0,3)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
              kernel.shared = ...
            for ax0@ax1@ax2@ax3@.0.0 (0,32)
              threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
                pad_temp.shared = ...
            for ff_c.3 (0,4)
              for rc.2 (0,4)
                for nn_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 639	GFLOPS: 1321.52 / 8596.00	results: MeasureResult(cost:[0.0014], error_no:0, all_cost:2.98, Tstamp:1669890929.52)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,12769)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 16
    for rc.0 (0,2)
      for rx.0 (0,3)
        for ax0@ax1@ax2@ax3@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,16)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for rc.2 (0,16)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for xx.3 (0,2)
      conv2d_nchw = ...

==================================================
No: 640	GFLOPS: 8501.30 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.97, Tstamp:1669890931.22)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for rx.2 (0,3)
                  for nn_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

Time elapsed for measurement: 125.68 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 0.81 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Iter: 5	#Pop: 32	#Target: 50	fail_ct: 10208	Time elapsed: 4.12
Sample Initial Population	#s: 59	fail_ct: 16325	Time elapsed: 6.67
GA Iter: 0	Max score: 0.7074	Min score: -0.0688	#Pop: 59	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9346	Min score: 0.8827	#Pop: 128	#M+: 1391	#M-: 0
EvolutionarySearch		#s: 128	Time elapsed: 14.60
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************==================================================
No: 641	GFLOPS: 6455.73 / 6681.61	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.15, Tstamp:1669890971.90)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 642	GFLOPS: 6439.19 / 6681.61	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.08, Tstamp:1669890973.57)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 643	GFLOPS: 6698.00 / 6698.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.06, Tstamp:1669890975.19)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 644	GFLOPS: 6639.91 / 6698.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.06, Tstamp:1669890976.86)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 645	GFLOPS: 6653.02 / 6698.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.04, Tstamp:1669890978.52)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 646	GFLOPS: 4600.59 / 6698.00	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:5.03, Tstamp:1669890980.16)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for xx_c.3 (0,2)
            for rc.2 (0,2)
              for ry.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 647	GFLOPS: 5963.97 / 6698.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.60, Tstamp:1669890982.06)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for yy_c.3 (0,2)
          for ry.2 (0,5)
            for nn_c.4 (0,2)
              conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 648	GFLOPS: 5630.05 / 6698.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.42, Tstamp:1669890983.72)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for nn_c.3 (0,2)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 649	GFLOPS: 5666.48 / 6698.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.37, Tstamp:1669890985.39)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for nn_c.3 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 650	GFLOPS: 5901.43 / 6698.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.61, Tstamp:1669890987.16)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.2 (0,5)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 651	GFLOPS: 5957.65 / 6698.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.82, Tstamp:1669890989.05)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for nn_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 652	GFLOPS: 5927.04 / 6698.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.61, Tstamp:1669890990.74)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for ry.2 (0,5)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 653	GFLOPS: 5639.52 / 6698.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.01, Tstamp:1669890992.40)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,8)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,36)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,4)
        for ry.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 654	GFLOPS: 5642.81 / 6698.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.17, Tstamp:1669890994.35)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,8)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,36)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,4)
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 655	GFLOPS: 6727.85 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.43, Tstamp:1669890996.32)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for ry.2 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 656	GFLOPS: 4689.61 / 6727.85	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:5.17, Tstamp:1669890997.98)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for xx_c.3 (0,2)
              for rx.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 657	GFLOPS: 6039.64 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.10, Tstamp:1669890999.65)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for ry.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 658	GFLOPS: 4649.91 / 6727.85	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:5.18, Tstamp:1669891001.41)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for xx_c.3 (0,2)
              for rx.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 659	GFLOPS: 5889.37 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.82, Tstamp:1669891003.28)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for nn_c.3 (0,2)
              for yy_c.3 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 660	GFLOPS: 5596.94 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.39, Tstamp:1669891004.96)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for nn_c.4 (0,2)
            for yy_c.4 (0,2)
              conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 661	GFLOPS: 5600.77 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.44, Tstamp:1669891006.64)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.2 (0,5)
          for nn_c.4 (0,2)
            for yy_c.4 (0,2)
              conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 662	GFLOPS: 5616.18 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.37, Tstamp:1669891008.38)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.2 (0,5)
        for rx.2 (0,5)
          for nn_c.4 (0,2)
            for yy_c.4 (0,2)
              conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 663	GFLOPS: 6035.13 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.24, Tstamp:1669891010.26)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for rc.2 (0,2)
              for nn_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 664	GFLOPS: 5949.19 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.06, Tstamp:1669891011.93)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for yy_c.3 (0,2)
          for rc.2 (0,2)
            for ry.2 (0,5)
              for nn_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 665	GFLOPS: 4676.20 / 6727.85	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:5.01, Tstamp:1669891013.56)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.1 (0,5)
            for nn_c.3 (0,2)
              for xx_c.3 (0,2)
                for rc.2 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 666	GFLOPS: 5884.24 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.38, Tstamp:1669891015.42)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.2 (0,5)
          for nn_c.4 (0,2)
            for yy_c.4 (0,2)
              conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 667	GFLOPS: 5841.26 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.49, Tstamp:1669891017.41)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 668	GFLOPS: 5830.91 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.18, Tstamp:1669891019.09)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 669	GFLOPS: 6016.10 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.21, Tstamp:1669891020.77)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for nn_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 670	GFLOPS: 5975.70 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.10, Tstamp:1669891022.45)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for nn_c.3 (0,2)
          for rc.2 (0,2)
            for ry.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 671	GFLOPS: 5978.59 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.26, Tstamp:1669891024.32)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for ry.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 672	GFLOPS: 6050.24 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.08, Tstamp:1669891025.99)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for rc.2 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 673	GFLOPS: 6054.79 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.10, Tstamp:1669891027.67)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for yy_c.3 (0,2)
        for rc.2 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for nn_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 674	GFLOPS: 6057.97 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.09, Tstamp:1669891029.45)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for ry.2 (0,5)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 675	GFLOPS: 6025.22 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.26, Tstamp:1669891031.33)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for rc.2 (0,2)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 676	GFLOPS: 6018.63 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.94, Tstamp:1669891033.01)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 677	GFLOPS: 6014.06 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.00, Tstamp:1669891034.70)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 678	GFLOPS: 6015.89 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669891036.47)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 679	GFLOPS: 5929.96 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.17, Tstamp:1669891038.28)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for nn_c.3 (0,2)
            for rc.2 (0,2)
              for rx.2 (0,5)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 680	GFLOPS: 5580.68 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.42, Tstamp:1669891039.94)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for nn_c.4 (0,2)
              conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 681	GFLOPS: 6018.74 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.08, Tstamp:1669891041.62)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for nn_c.3 (0,2)
              for yy_c.3 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 682	GFLOPS: 6034.19 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.25, Tstamp:1669891043.46)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for nn_c.3 (0,2)
              for yy_c.3 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 683	GFLOPS: 6023.46 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.32, Tstamp:1669891045.35)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 684	GFLOPS: 6021.79 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.96, Tstamp:1669891047.01)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.2 (0,5)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 685	GFLOPS: 6032.81 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.06, Tstamp:1669891048.67)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 686	GFLOPS: 6023.20 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669891050.48)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 687	GFLOPS: 5792.18 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.24, Tstamp:1669891052.35)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 688	GFLOPS: 5931.70 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.85, Tstamp:1669891054.01)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for rc.2 (0,2)
                for rx.2 (0,5)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 689	GFLOPS: 5818.85 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.38, Tstamp:1669891055.67)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for nn_c.4 (0,2)
              conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 690	GFLOPS: 6009.66 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.98, Tstamp:1669891057.46)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for nn_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 691	GFLOPS: 5965.16 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.01, Tstamp:1669891059.33)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for rc.2 (0,2)
                for ry.2 (0,5)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 692	GFLOPS: 6016.53 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.93, Tstamp:1669891061.01)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for yy_c.3 (0,2)
            for rx.2 (0,5)
              for nn_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 693	GFLOPS: 6000.82 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.87, Tstamp:1669891062.69)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for yy_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for nn_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 694	GFLOPS: 5933.22 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.86, Tstamp:1669891064.51)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for nn_c.3 (0,2)
              for rx.2 (0,5)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 695	GFLOPS: 5939.84 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.08, Tstamp:1669891066.38)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for nn_c.3 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 696	GFLOPS: 5912.51 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.82, Tstamp:1669891068.04)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for nn_c.3 (0,2)
            for rc.2 (0,2)
              for rx.2 (0,5)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 697	GFLOPS: 5913.95 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.25, Tstamp:1669891069.71)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for nn_c.3 (0,2)
              for ry.2 (0,5)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 698	GFLOPS: 5928.59 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.13, Tstamp:1669891071.51)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for nn_c.3 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 699	GFLOPS: 5939.93 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.39, Tstamp:1669891073.38)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for nn_c.3 (0,2)
          for rc.2 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 700	GFLOPS: 5946.15 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.28, Tstamp:1669891075.06)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for nn_c.3 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 701	GFLOPS: 5918.00 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.28, Tstamp:1669891076.72)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for nn_c.3 (0,2)
            for rc.2 (0,2)
              for rx.2 (0,5)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 702	GFLOPS: 1445.44 / 6727.85	results: MeasureResult(cost:[0.0009], error_no:0, all_cost:8.48, Tstamp:1669891078.94)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,196)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,216)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for nn_c.3 (0,4)
          for yy_c.3 (0,4)
            for xx_c.3 (0,2)
              for rx.2 (0,5)
                for nn_c.4 (0,4)
                  conv2d_nchw.local = ...
    for nn.3 (0,16)
      for yy.3 (0,4)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 703	GFLOPS: 6031.67 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.27, Tstamp:1669891080.57)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for ry.2 (0,5)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 704	GFLOPS: 6057.88 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.23, Tstamp:1669891082.25)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for ry.2 (0,5)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

Time elapsed for measurement: 128.82 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 0.90 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 67	fail_ct: 4029	Time elapsed: 2.25
GA Iter: 0	Max score: 0.6645	Min score: 0.0457	#Pop: 67	#M+: 0	#M-: 0
GA Iter: 4	Max score: 1.0022	Min score: 0.9362	#Pop: 128	#M+: 1396	#M-: 0
EvolutionarySearch		#s: 128	Time elapsed: 14.48
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************==================================================
No: 705	GFLOPS: 8020.69 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.63, Tstamp:1669891114.39)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for nn_c.3 (0,4)
        for ff_c.3 (0,4)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 706	GFLOPS: 8072.54 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.67, Tstamp:1669891116.08)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for nn_c.3 (0,4)
        for ff_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 707	GFLOPS: 8520.45 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.14, Tstamp:1669891117.91)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for nn_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 708	GFLOPS: 8577.28 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669891119.63)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for rx.2 (0,3)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 709	GFLOPS: 8461.38 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.20, Tstamp:1669891121.51)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for nn_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 710	GFLOPS: 8565.77 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.29, Tstamp:1669891123.43)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for rx.2 (0,3)
                  for nn_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 711	GFLOPS: 8503.42 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669891125.13)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for rx.2 (0,3)
                  for nn_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 712	GFLOPS: 8475.28 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.00, Tstamp:1669891126.84)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for ry.2 (0,3)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 713	GFLOPS: 8512.27 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669891128.57)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for ff_c.3 (0,2)
              for nn_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 714	GFLOPS: 8525.76 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.28, Tstamp:1669891130.54)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 715	GFLOPS: 8566.19 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.00, Tstamp:1669891132.23)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ff_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for nn_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 716	GFLOPS: 8493.70 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669891133.95)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for ry.2 (0,3)
                  for nn_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 717	GFLOPS: 8458.89 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.97, Tstamp:1669891135.66)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for nn_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 718	GFLOPS: 8500.57 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.34, Tstamp:1669891137.67)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for ff_c.3 (0,2)
                for yy_c.3 (0,2)
                  for nn_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 719	GFLOPS: 8523.84 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.98, Tstamp:1669891139.40)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 720	GFLOPS: 8510.09 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.97, Tstamp:1669891141.07)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ff_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for nn_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 721	GFLOPS: 8479.76 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669891142.76)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for nn_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 722	GFLOPS: 8583.98 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.33, Tstamp:1669891144.76)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for ff_c.3 (0,2)
                for nn_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 723	GFLOPS: 8510.30 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669891146.48)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 724	GFLOPS: 8485.06 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.98, Tstamp:1669891148.18)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 725	GFLOPS: 8572.86 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669891149.90)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for rx.2 (0,3)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 726	GFLOPS: 8580.98 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.28, Tstamp:1669891151.87)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for rx.2 (0,3)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 727	GFLOPS: 8543.44 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669891153.59)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 728	GFLOPS: 8503.81 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.00, Tstamp:1669891155.28)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 729	GFLOPS: 8464.54 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669891157.01)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for ff_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 730	GFLOPS: 8452.10 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.18, Tstamp:1669891158.88)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ff_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 731	GFLOPS: 8433.06 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.06, Tstamp:1669891160.60)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ff_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 732	GFLOPS: 8526.05 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.18, Tstamp:1669891162.43)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for yy_c.3 (0,2)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 733	GFLOPS: 8511.45 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669891164.13)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for rx.2 (0,3)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 734	GFLOPS: 8453.22 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.23, Tstamp:1669891166.10)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 735	GFLOPS: 8519.65 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.24, Tstamp:1669891168.00)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 736	GFLOPS: 8539.61 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.95, Tstamp:1669891169.71)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for ry.2 (0,3)
                  for nn_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 737	GFLOPS: 8563.79 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669891171.42)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for rx.2 (0,3)
                  for nn_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 738	GFLOPS: 8552.54 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669891173.15)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 739	GFLOPS: 8473.55 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.22, Tstamp:1669891175.13)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 740	GFLOPS: 8477.84 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.95, Tstamp:1669891176.84)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 741	GFLOPS: 8475.73 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.99, Tstamp:1669891178.55)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 742	GFLOPS: 8482.63 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669891180.25)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 743	GFLOPS: 8470.76 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.20, Tstamp:1669891182.21)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for nn_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 744	GFLOPS: 8554.37 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.93, Tstamp:1669891183.91)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ff_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for nn_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 745	GFLOPS: 8530.24 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669891185.62)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for nn_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 746	GFLOPS: 8476.73 / 8596.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.83, Tstamp:1669891187.33)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for rx.2 (0,3)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 747	GFLOPS: 8597.40 / 8597.40	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.00, Tstamp:1669891189.32)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for nn_c.3 (0,4)
        for ff_c.3 (0,4)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 748	GFLOPS: 8602.87 / 8602.87	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.82, Tstamp:1669891191.03)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for nn_c.3 (0,4)
        for ff_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 749	GFLOPS: 8399.29 / 8602.87	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.95, Tstamp:1669891192.74)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ry.2 (0,3)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 750	GFLOPS: 6861.53 / 8602.87	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.14, Tstamp:1669891194.93)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for rx.2 (0,3)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 751	GFLOPS: 7968.44 / 8602.87	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.83, Tstamp:1669891196.65)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for rx.2 (0,3)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 752	GFLOPS: 8072.32 / 8602.87	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.96, Tstamp:1669891198.56)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for ry.2 (0,3)
                  for nn_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 753	GFLOPS: 8024.54 / 8602.87	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.78, Tstamp:1669891200.26)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for rx.2 (0,3)
                  for nn_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 754	GFLOPS: 7058.42 / 8602.87	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:3.54, Tstamp:1669891201.96)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for yy_c.3 (0,2)
            for nn_c.4 (0,4)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 755	GFLOPS: 6866.75 / 8602.87	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:3.71, Tstamp:1669891203.66)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,4)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 756	GFLOPS: 8047.04 / 8602.87	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.00, Tstamp:1669891205.57)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for ff_c.3 (0,2)
                for yy_c.3 (0,2)
                  for nn_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 757	GFLOPS: 7990.00 / 8602.87	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.83, Tstamp:1669891207.27)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for rx.2 (0,3)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 758	GFLOPS: 8003.91 / 8602.87	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.71, Tstamp:1669891208.96)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,4)
          for ff_c.3 (0,4)
            for rx.2 (0,3)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 759	GFLOPS: 8037.56 / 8602.87	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.85, Tstamp:1669891210.71)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for ff_c.3 (0,2)
              for nn_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 760	GFLOPS: 7994.64 / 8602.87	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669891212.62)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for yy_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 761	GFLOPS: 8011.29 / 8602.87	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.10, Tstamp:1669891214.31)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rx.1 (0,3)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 762	GFLOPS: 7998.50 / 8602.87	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.05, Tstamp:1669891216.00)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for yy_c.3 (0,2)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 763	GFLOPS: 8012.61 / 8602.87	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.98, Tstamp:1669891217.75)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,4)
              for yy_c.3 (0,2)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 764	GFLOPS: 8052.48 / 8602.87	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.18, Tstamp:1669891219.71)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for yy_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 765	GFLOPS: 8029.28 / 8602.87	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.90, Tstamp:1669891221.41)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for ry.2 (0,3)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 766	GFLOPS: 5172.88 / 8602.87	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:3.41, Tstamp:1669891223.12)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1808)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,4)
        for ry.0 (0,3)
          for rx.0 (0,3)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
              kernel.shared = ...
            for ax0@ax1@ax2@ax3@.0.0 (0,32)
              threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
                pad_temp.shared = ...
            for rc.1 (0,8)
              for ff_c.3 (0,2)
                for yy_c.3 (0,2)
                  conv2d_nchw.local = ...
      for ff.3 (0,2)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 767	GFLOPS: 8032.39 / 8602.87	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.82, Tstamp:1669891224.83)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for ry.2 (0,3)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 768	GFLOPS: 8012.58 / 8602.87	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.14, Tstamp:1669891226.81)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

Time elapsed for measurement: 126.82 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 0.93 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Iter: 5	#Pop: 33	#Target: 50	fail_ct: 10207	Time elapsed: 4.09
Sample Initial Population	#s: 54	fail_ct: 16330	Time elapsed: 6.77
GA Iter: 0	Max score: 0.5465	Min score: 0.0152	#Pop: 54	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9732	Min score: 0.8779	#Pop: 128	#M+: 1395	#M-: 0
EvolutionarySearch		#s: 128	Time elapsed: 14.49
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************==================================================
No: 769	GFLOPS: 6706.63 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.08, Tstamp:1669891264.49)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rc.2 (0,2)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 770	GFLOPS: 6664.88 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.99, Tstamp:1669891266.16)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 771	GFLOPS: 6647.20 / 6727.85	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.21, Tstamp:1669891267.82)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 772	GFLOPS: 7025.38 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669891269.69)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 773	GFLOPS: 6664.20 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.08, Tstamp:1669891271.36)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rc.2 (0,2)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 774	GFLOPS: 6681.18 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.16, Tstamp:1669891273.03)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.2 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 775	GFLOPS: 6679.45 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.19, Tstamp:1669891274.83)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 776	GFLOPS: 6673.88 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.31, Tstamp:1669891276.69)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 777	GFLOPS: 6683.98 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.07, Tstamp:1669891278.36)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for yy_c.3 (0,2)
              for rx.2 (0,5)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 778	GFLOPS: 6527.66 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.05, Tstamp:1669891280.02)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for rc.2 (0,2)
            for ry.2 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 779	GFLOPS: 7017.04 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669891281.82)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 780	GFLOPS: 5187.05 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.82, Tstamp:1669891283.68)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,256)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,256)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,256)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      conv2d_nchw = ...

==================================================
No: 781	GFLOPS: 6768.39 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669891285.34)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 782	GFLOPS: 6757.46 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.10, Tstamp:1669891287.00)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rc.2 (0,2)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 783	GFLOPS: 4355.43 / 7025.38	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:7.03, Tstamp:1669891289.32)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,4)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,144)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,8)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 784	GFLOPS: 6461.79 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.20, Tstamp:1669891291.11)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 785	GFLOPS: 6477.69 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.20, Tstamp:1669891293.00)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for yy_c.3 (0,2)
              for rx.2 (0,5)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 786	GFLOPS: 6716.32 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.82, Tstamp:1669891294.65)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 787	GFLOPS: 6732.52 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.92, Tstamp:1669891296.32)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for ry.2 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 788	GFLOPS: 5957.77 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.74, Tstamp:1669891298.18)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,30)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 789	GFLOPS: 3242.71 / 7025.38	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:4.24, Tstamp:1669891300.50)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,256)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,256)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,256)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for ry.2 (0,5)
            for nn_c.4 (0,2)
              conv2d_nchw.local = ...
    for nn.3 (0,2)
      conv2d_nchw = ...

==================================================
No: 790	GFLOPS: 5782.78 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.12, Tstamp:1669891302.37)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for yy_c.3 (0,2)
          for rc.2 (0,2)
            for ry.2 (0,5)
              for nn_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 791	GFLOPS: 5501.91 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.47, Tstamp:1669891304.06)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,256)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,256)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,256)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for yy_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      conv2d_nchw = ...

==================================================
No: 792	GFLOPS: 5922.24 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.65, Tstamp:1669891305.73)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for nn_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 793	GFLOPS: 6009.07 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.12, Tstamp:1669891307.51)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for rc.2 (0,2)
          for ry.2 (0,5)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 794	GFLOPS: 6044.25 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.36, Tstamp:1669891309.38)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,5)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 795	GFLOPS: 6015.87 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.09, Tstamp:1669891311.06)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for nn_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 796	GFLOPS: 5304.56 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.47, Tstamp:1669891312.74)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,256)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,256)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,256)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for rc.2 (0,2)
          for ry.2 (0,5)
            for yy_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      conv2d_nchw = ...

==================================================
No: 797	GFLOPS: 6043.41 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.09, Tstamp:1669891314.58)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,5)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 798	GFLOPS: 6009.68 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.36, Tstamp:1669891316.45)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for nn_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 799	GFLOPS: 6023.07 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.09, Tstamp:1669891318.12)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for yy_c.3 (0,2)
          for rc.2 (0,2)
            for ry.2 (0,5)
              for nn_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 800	GFLOPS: 5940.50 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.06, Tstamp:1669891319.79)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for rc.2 (0,2)
          for ry.2 (0,5)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 801	GFLOPS: 6037.44 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.13, Tstamp:1669891321.53)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for rc.2 (0,2)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 802	GFLOPS: 3216.75 / 7025.38	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:4.18, Tstamp:1669891323.86)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,256)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,256)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,256)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for ry.2 (0,5)
            for nn_c.4 (0,2)
              conv2d_nchw.local = ...
    for nn.3 (0,2)
      conv2d_nchw = ...

==================================================
No: 803	GFLOPS: 5935.84 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.20, Tstamp:1669891325.70)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for rc.2 (0,2)
          for ry.2 (0,5)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 804	GFLOPS: 6027.56 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.08, Tstamp:1669891327.37)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.2 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 805	GFLOPS: 5958.69 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.20, Tstamp:1669891329.03)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for yy_c.3 (0,2)
          for rc.2 (0,2)
            for ry.2 (0,5)
              for nn_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 806	GFLOPS: 5965.30 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.34, Tstamp:1669891330.97)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for nn_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 807	GFLOPS: 5984.66 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.29, Tstamp:1669891332.83)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rc.2 (0,2)
          for rx.2 (0,5)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 808	GFLOPS: 6001.79 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.07, Tstamp:1669891334.49)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.2 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 809	GFLOPS: 6023.32 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669891336.17)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for ry.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 810	GFLOPS: 6001.68 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.15, Tstamp:1669891338.10)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for nn_c.3 (0,2)
          for rc.2 (0,2)
            for ry.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 811	GFLOPS: 5493.41 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.70, Tstamp:1669891339.99)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,256)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,256)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,256)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      conv2d_nchw = ...

==================================================
No: 812	GFLOPS: 5972.14 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.08, Tstamp:1669891341.67)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for nn_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 813	GFLOPS: 5517.63 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.44, Tstamp:1669891343.32)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,256)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,256)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,256)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      conv2d_nchw = ...

==================================================
No: 814	GFLOPS: 5987.25 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669891345.10)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 815	GFLOPS: 6019.52 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.27, Tstamp:1669891346.97)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for rc.2 (0,2)
              for ry.2 (0,5)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 816	GFLOPS: 6009.30 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.06, Tstamp:1669891348.64)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for nn_c.3 (0,2)
          for rc.2 (0,2)
            for ry.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 817	GFLOPS: 5992.36 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.09, Tstamp:1669891350.31)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for nn_c.3 (0,2)
              for yy_c.3 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 818	GFLOPS: 5991.96 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669891352.09)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.2 (0,5)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 819	GFLOPS: 5995.10 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.33, Tstamp:1669891354.07)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.2 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 820	GFLOPS: 6009.77 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.07, Tstamp:1669891355.75)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 821	GFLOPS: 6010.46 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.94, Tstamp:1669891357.43)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 822	GFLOPS: 5995.97 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669891359.16)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for nn_c.3 (0,2)
              for yy_c.3 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 823	GFLOPS: 5994.47 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.24, Tstamp:1669891361.09)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 824	GFLOPS: 6007.66 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.97, Tstamp:1669891362.76)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 825	GFLOPS: 5482.09 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.92, Tstamp:1669891364.42)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,256)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,256)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,256)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      conv2d_nchw = ...

==================================================
No: 826	GFLOPS: 5283.71 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.84, Tstamp:1669891366.23)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,256)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,256)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,256)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for rc.2 (0,2)
          for ry.2 (0,5)
            for yy_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      conv2d_nchw = ...

==================================================
No: 827	GFLOPS: 5885.68 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.18, Tstamp:1669891368.10)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for nn_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 828	GFLOPS: 6553.05 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.91, Tstamp:1669891369.75)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.1 (0,5)
            for rc.2 (0,2)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 829	GFLOPS: 6006.44 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.19, Tstamp:1669891371.43)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 830	GFLOPS: 347.29 / 7025.38	results: MeasureResult(cost:[0.0037], error_no:0, all_cost:2.87, Tstamp:1669891373.21)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,98)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,180)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for xx_c.3 (0,4)
            for ry.2 (0,5)
              for nn_c.4 (0,2)
                for xx_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for xx.3 (0,16)
          conv2d_nchw = ...

==================================================
No: 831	GFLOPS: 5944.28 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.13, Tstamp:1669891375.09)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for ry.2 (0,5)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 832	GFLOPS: 6001.44 / 7025.38	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.06, Tstamp:1669891376.77)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for nn_c.3 (0,2)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

Time elapsed for measurement: 127.64 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 1.03 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 87	fail_ct: 4009	Time elapsed: 2.10
GA Iter: 0	Max score: 0.7804	Min score: 0.0062	#Pop: 87	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9959	Min score: 0.9346	#Pop: 128	#M+: 1399	#M-: 0
EvolutionarySearch		#s: 128	Time elapsed: 14.50
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************==================================================
No: 833	GFLOPS: 8473.10 / 8602.87	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.00, Tstamp:1669891409.40)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for nn_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 834	GFLOPS: 8575.32 / 8602.87	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.24, Tstamp:1669891411.34)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for ff_c.3 (0,2)
                for yy_c.3 (0,2)
                  for nn_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 835	GFLOPS: 8475.09 / 8602.87	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669891413.03)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for nn_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 836	GFLOPS: 8528.07 / 8602.87	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.06, Tstamp:1669891414.76)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for nn_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 837	GFLOPS: 8572.40 / 8602.87	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669891416.47)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,4)
              for ff_c.3 (0,2)
                for yy_c.3 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 838	GFLOPS: 8536.21 / 8602.87	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.34, Tstamp:1669891418.44)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for ry.2 (0,3)
                  for nn_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 839	GFLOPS: 8517.00 / 8602.87	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669891420.14)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for ff_c.3 (0,2)
              for nn_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 840	GFLOPS: 8468.66 / 8602.87	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.07, Tstamp:1669891421.83)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for nn_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 841	GFLOPS: 8482.09 / 8602.87	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669891423.53)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for ff_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 842	GFLOPS: 8467.15 / 8602.87	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.22, Tstamp:1669891425.42)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for ff_c.3 (0,2)
              for nn_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 843	GFLOPS: 8440.07 / 8602.87	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669891427.12)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for rx.2 (0,3)
                  for nn_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 844	GFLOPS: 8500.51 / 8602.87	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669891428.84)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for ry.2 (0,3)
                  for nn_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 845	GFLOPS: 8506.08 / 8602.87	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669891430.54)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for rx.2 (0,3)
                  for nn_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 846	GFLOPS: 8595.66 / 8602.87	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.24, Tstamp:1669891432.44)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for nn_c.3 (0,4)
        for ff_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 847	GFLOPS: 8535.04 / 8602.87	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669891434.15)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for nn_c.3 (0,4)
        for ff_c.3 (0,4)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 848	GFLOPS: 8548.90 / 8602.87	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.79, Tstamp:1669891435.86)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for nn_c.3 (0,4)
        for yy_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 849	GFLOPS: 8490.88 / 8602.87	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.09, Tstamp:1669891437.62)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 850	GFLOPS: 8596.70 / 8602.87	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.30, Tstamp:1669891439.52)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for nn_c.3 (0,2)
        for ff_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 851	GFLOPS: 8513.50 / 8602.87	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669891441.24)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,4)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 852	GFLOPS: 8612.96 / 8612.96	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.06, Tstamp:1669891442.96)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,4)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 853	GFLOPS: 8587.06 / 8612.96	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.12, Tstamp:1669891444.74)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for nn_c.3 (0,2)
        for ff_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 854	GFLOPS: 8515.10 / 8612.96	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.28, Tstamp:1669891446.69)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 855	GFLOPS: 8562.67 / 8612.96	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669891448.39)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for ff_c.3 (0,4)
          for ry.2 (0,3)
            for nn_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 856	GFLOPS: 8599.23 / 8612.96	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669891450.10)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,4)
          for ff_c.3 (0,4)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 857	GFLOPS: 8487.90 / 8612.96	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.00, Tstamp:1669891451.81)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,4)
          for ry.2 (0,3)
            for ff_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 858	GFLOPS: 8464.06 / 8612.96	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.24, Tstamp:1669891453.78)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,2)
          for ry.2 (0,3)
            for nn_c.4 (0,2)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 859	GFLOPS: 8563.95 / 8612.96	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669891455.49)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for ff_c.3 (0,2)
          for ry.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 860	GFLOPS: 8566.27 / 8612.96	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.96, Tstamp:1669891457.20)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,4)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 861	GFLOPS: 8586.22 / 8612.96	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.96, Tstamp:1669891458.89)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for nn_c.4 (0,2)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 862	GFLOPS: 8521.26 / 8612.96	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.15, Tstamp:1669891460.78)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for ff_c.3 (0,2)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 863	GFLOPS: 8489.28 / 8612.96	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669891462.48)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 864	GFLOPS: 8596.09 / 8612.96	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.98, Tstamp:1669891464.21)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 865	GFLOPS: 8616.58 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669891465.92)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 866	GFLOPS: 8586.20 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.29, Tstamp:1669891467.86)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.4 (0,4)
            for ff_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 867	GFLOPS: 8545.56 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.00, Tstamp:1669891469.59)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,4)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 868	GFLOPS: 8542.94 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669891471.32)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for ry.2 (0,3)
          for nn_c.4 (0,4)
            for ff_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 869	GFLOPS: 8481.40 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.88, Tstamp:1669891473.00)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,2)
          for ff_c.3 (0,4)
            for ry.2 (0,3)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 870	GFLOPS: 8474.58 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.15, Tstamp:1669891474.91)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 871	GFLOPS: 8522.56 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.91, Tstamp:1669891476.59)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for nn_c.3 (0,2)
        for ff_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 872	GFLOPS: 8525.42 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.99, Tstamp:1669891478.29)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 873	GFLOPS: 8554.97 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.89, Tstamp:1669891480.08)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,4)
          for rx.2 (0,3)
            for ff_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 874	GFLOPS: 8593.59 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.08, Tstamp:1669891481.95)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.4 (0,4)
            for ff_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 875	GFLOPS: 8587.76 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.93, Tstamp:1669891483.67)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.2 (0,3)
          for nn_c.4 (0,4)
            for ff_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 876	GFLOPS: 8556.05 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.90, Tstamp:1669891485.37)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for ff_c.3 (0,4)
            for nn_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 877	GFLOPS: 8493.32 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.95, Tstamp:1669891487.15)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,4)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 878	GFLOPS: 8368.30 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.25, Tstamp:1669891489.05)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 879	GFLOPS: 8500.29 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.87, Tstamp:1669891490.75)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,4)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 880	GFLOPS: 8585.05 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.94, Tstamp:1669891492.47)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,4)
          for ff_c.3 (0,2)
            for rx.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 881	GFLOPS: 8566.29 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.85, Tstamp:1669891494.16)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for nn_c.3 (0,2)
        for yy_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 882	GFLOPS: 8484.33 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669891496.08)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,4)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 883	GFLOPS: 8589.45 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.98, Tstamp:1669891497.79)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for yy_c.3 (0,2)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 884	GFLOPS: 8589.02 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.87, Tstamp:1669891499.49)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for yy_c.3 (0,2)
            for nn_c.4 (0,4)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 885	GFLOPS: 8571.17 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669891501.30)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for yy_c.3 (0,2)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 886	GFLOPS: 8481.30 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669891503.25)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,4)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 887	GFLOPS: 8591.18 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.65, Tstamp:1669891504.96)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for yy_c.3 (0,2)
            for nn_c.4 (0,4)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 888	GFLOPS: 8544.66 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.93, Tstamp:1669891506.64)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for yy_c.3 (0,2)
          for ry.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 889	GFLOPS: 8529.76 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.02, Tstamp:1669891508.34)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for nn_c.3 (0,4)
        for yy_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 890	GFLOPS: 8534.45 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.21, Tstamp:1669891510.22)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for nn_c.3 (0,4)
        for yy_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 891	GFLOPS: 8510.07 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.02, Tstamp:1669891511.94)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ff_c.3 (0,4)
        for ry.2 (0,3)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 892	GFLOPS: 8553.40 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.05, Tstamp:1669891513.66)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for nn_c.3 (0,2)
        for ff_c.3 (0,4)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 893	GFLOPS: 8504.99 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.10, Tstamp:1669891515.44)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,2)
          for ff_c.3 (0,4)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 894	GFLOPS: 3565.51 / 8616.58	results: MeasureResult(cost:[0.0005], error_no:0, all_cost:3.09, Tstamp:1669891517.79)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1808)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,904)
    conv2d_nchw.local auto_unroll: 16
    for rc.0 (0,4)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,904)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,904)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for rc.2 (0,8)
          for rx.2 (0,3)
            for yy_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      conv2d_nchw = ...

==================================================
No: 895	GFLOPS: 686.66 / 8616.58	results: MeasureResult(cost:[0.0027], error_no:0, all_cost:4.17, Tstamp:1669891519.66)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,226)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,8)
        for ry.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,33)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
              pad_temp.shared = ...
          for rx.1 (0,3)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for rc.2 (0,4)
                  for nn_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 896	GFLOPS: 7289.26 / 8616.58	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:2.87, Tstamp:1669891521.82)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,226)
    conv2d_nchw.local auto_unroll: 16
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for ff_c.3 (0,2)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,2)
      for ff.3 (0,2)
        for yy.3 (0,2)
          for xx.3 (0,2)
            conv2d_nchw = ...

Time elapsed for measurement: 127.34 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 1.06 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                                Conv5x5_origin |        0.217 |        5906.61 |     64 |
|    1 |                                                   Conv5x5_opt |        0.236 |        7992.17 |     64 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.453 ms	Trials: 128	Used time : 325 s	Next ID: 0	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                                Conv5x5_origin |        0.217 |        5906.61 |    128 |
|    1 |                                                   Conv5x5_opt |        0.236 |        7992.17 |     64 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.453 ms	Trials: 192	Used time : 488 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                                Conv5x5_origin |        0.217 |        5906.61 |    128 |
|    1 |                                                   Conv5x5_opt |        0.221 |        8529.82 |    128 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.438 ms	Trials: 256	Used time : 644 s	Next ID: 0	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                                Conv5x5_origin |        0.213 |        6039.51 |    192 |
|    1 |                                                   Conv5x5_opt |        0.221 |        8529.82 |    128 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.433 ms	Trials: 320	Used time : 800 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                                Conv5x5_origin |        0.213 |        6039.51 |    192 |
|    1 |                                                   Conv5x5_opt |        0.220 |        8576.52 |    192 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.432 ms	Trials: 384	Used time : 945 s	Next ID: 0	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                                Conv5x5_origin |        0.213 |        6043.57 |    256 |
|    1 |                                                   Conv5x5_opt |        0.220 |        8576.52 |    192 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.432 ms	Trials: 448	Used time : 1094 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                                Conv5x5_origin |        0.213 |        6043.57 |    256 |
|    1 |                                                   Conv5x5_opt |        0.219 |        8579.52 |    256 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.432 ms	Trials: 512	Used time : 1236 s	Next ID: 0	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                                Conv5x5_origin |        0.192 |        6681.61 |    320 |
|    1 |                                                   Conv5x5_opt |        0.219 |        8579.52 |    256 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.412 ms	Trials: 576	Used time : 1386 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                                Conv5x5_origin |        0.192 |        6681.61 |    320 |
|    1 |                                                   Conv5x5_opt |        0.219 |        8596.00 |    320 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.411 ms	Trials: 640	Used time : 1529 s	Next ID: 0	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                                Conv5x5_origin |        0.191 |        6727.85 |    384 |
|    1 |                                                   Conv5x5_opt |        0.219 |        8596.00 |    320 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.410 ms	Trials: 704	Used time : 1680 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                                Conv5x5_origin |        0.191 |        6727.85 |    384 |
|    1 |                                                   Conv5x5_opt |        0.219 |        8602.87 |    384 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.410 ms	Trials: 768	Used time : 1825 s	Next ID: 0	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                                Conv5x5_origin |        0.183 |        7025.38 |    448 |
|    1 |                                                   Conv5x5_opt |        0.219 |        8602.87 |    384 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.402 ms	Trials: 832	Used time : 1975 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                                Conv5x5_origin |        0.183 |        7025.38 |    448 |
|    1 |                                                   Conv5x5_opt |        0.219 |        8616.58 |    448 |----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Iter: 5	#Pop: 36	#Target: 50	fail_ct: 10204	Time elapsed: 3.95
Sample Initial Population	#s: 50	fail_ct: 16334	Time elapsed: 6.38
GA Iter: 0	Max score: 0.7170	Min score: -0.0656	#Pop: 50	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9851	Min score: 0.8642	#Pop: 128	#M+: 1390	#M-: 0
EvolutionarySearch		#s: 128	Time elapsed: 14.39
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************==================================================
No: 897	GFLOPS: 7028.12 / 7028.12	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.87, Tstamp:1669891558.65)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 898	GFLOPS: 7011.77 / 7028.12	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669891560.45)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 899	GFLOPS: 7056.45 / 7056.45	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.84, Tstamp:1669891562.11)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 900	GFLOPS: 7041.47 / 7056.45	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.90, Tstamp:1669891563.77)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 901	GFLOPS: 6972.83 / 7056.45	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.27, Tstamp:1669891565.69)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rc.2 (0,2)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 902	GFLOPS: 7000.30 / 7056.45	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.19, Tstamp:1669891567.51)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 903	GFLOPS: 7044.77 / 7056.45	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669891569.18)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 904	GFLOPS: 6992.42 / 7056.45	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669891570.84)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for rc.2 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 905	GFLOPS: 6916.98 / 7056.45	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.26, Tstamp:1669891572.77)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for ry.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 906	GFLOPS: 7061.45 / 7061.45	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.17, Tstamp:1669891574.58)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 907	GFLOPS: 6990.05 / 7061.45	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669891576.25)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rc.2 (0,2)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 908	GFLOPS: 7062.44 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.07, Tstamp:1669891577.93)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 909	GFLOPS: 7057.30 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669891579.85)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 910	GFLOPS: 7043.03 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.29, Tstamp:1669891581.72)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 911	GFLOPS: 6946.45 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669891583.39)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for ry.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 912	GFLOPS: 7027.14 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.88, Tstamp:1669891585.07)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for yy_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 913	GFLOPS: 7037.75 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.91, Tstamp:1669891586.86)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for yy_c.3 (0,2)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 914	GFLOPS: 6765.07 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.22, Tstamp:1669891588.68)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 915	GFLOPS: 6776.14 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669891590.32)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 916	GFLOPS: 6767.73 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.08, Tstamp:1669891592.01)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.2 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 917	GFLOPS: 6942.10 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.99, Tstamp:1669891593.80)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for rc.2 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 918	GFLOPS: 6777.50 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.19, Tstamp:1669891595.64)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for rc.2 (0,2)
          for ry.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 919	GFLOPS: 6496.69 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.84, Tstamp:1669891597.28)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for ry.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 920	GFLOPS: 7055.44 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.00, Tstamp:1669891598.96)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for yy_c.3 (0,2)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 921	GFLOPS: 6910.03 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.54, Tstamp:1669891600.87)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.2 (0,5)
          for yy_c.4 (0,2)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 922	GFLOPS: 6707.70 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.22, Tstamp:1669891602.71)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for rc.2 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 923	GFLOPS: 6727.80 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.84, Tstamp:1669891604.34)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 924	GFLOPS: 6636.73 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.92, Tstamp:1669891605.99)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 925	GFLOPS: 6653.45 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.09, Tstamp:1669891607.90)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 926	GFLOPS: 6640.27 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.05, Tstamp:1669891609.74)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.2 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 927	GFLOPS: 6659.37 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.89, Tstamp:1669891611.41)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for yy_c.3 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 928	GFLOPS: 6723.85 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.87, Tstamp:1669891613.06)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 929	GFLOPS: 6553.64 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669891614.89)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for rc.2 (0,2)
          for ry.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 930	GFLOPS: 6778.95 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.23, Tstamp:1669891616.75)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for yy_c.3 (0,2)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 931	GFLOPS: 6697.26 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.84, Tstamp:1669891618.40)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rc.2 (0,2)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 932	GFLOPS: 6721.70 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.00, Tstamp:1669891620.06)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for rc.2 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 933	GFLOPS: 6692.59 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.83, Tstamp:1669891621.70)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 934	GFLOPS: 6738.44 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.80, Tstamp:1669891623.36)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 935	GFLOPS: 6768.20 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.98, Tstamp:1669891625.03)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 936	GFLOPS: 6768.63 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.83, Tstamp:1669891626.70)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for ry.2 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 937	GFLOPS: 6658.81 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.82, Tstamp:1669891628.45)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for rc.2 (0,2)
            for ry.2 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 938	GFLOPS: 6685.20 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.81, Tstamp:1669891630.11)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.2 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 939	GFLOPS: 6689.75 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.74, Tstamp:1669891631.80)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rc.2 (0,2)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 940	GFLOPS: 6686.41 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.73, Tstamp:1669891633.47)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 941	GFLOPS: 6565.73 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:5.05, Tstamp:1669891635.44)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.1 (0,5)
            for rc.2 (0,2)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 942	GFLOPS: 6928.09 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.36, Tstamp:1669891637.11)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for yy_c.3 (0,2)
          for rx.2 (0,5)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 943	GFLOPS: 6657.82 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.37, Tstamp:1669891638.79)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.2 (0,5)
          for yy_c.4 (0,2)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 944	GFLOPS: 6673.30 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.50, Tstamp:1669891640.58)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.2 (0,5)
        for rx.2 (0,5)
          for yy_c.4 (0,2)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 945	GFLOPS: 6706.88 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.90, Tstamp:1669891642.41)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 946	GFLOPS: 6665.37 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.70, Tstamp:1669891644.06)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for yy_c.3 (0,2)
              for rx.2 (0,5)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 947	GFLOPS: 6772.81 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.83, Tstamp:1669891645.72)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for ry.2 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 948	GFLOPS: 5602.58 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.98, Tstamp:1669891647.38)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,8)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,36)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,4)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 949	GFLOPS: 6447.02 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.63, Tstamp:1669891649.07)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for yy_c.3 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 950	GFLOPS: 6467.87 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.85, Tstamp:1669891650.85)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.2 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 951	GFLOPS: 6457.60 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.74, Tstamp:1669891652.51)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.1 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 952	GFLOPS: 6478.01 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.64, Tstamp:1669891654.13)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for rc.2 (0,2)
            for ry.2 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 953	GFLOPS: 6636.87 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.96, Tstamp:1669891655.78)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.1 (0,5)
            for rc.2 (0,2)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 954	GFLOPS: 6725.42 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.85, Tstamp:1669891657.44)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for ry.2 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 955	GFLOPS: 6745.76 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.94, Tstamp:1669891659.12)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for ry.2 (0,5)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 956	GFLOPS: 6695.96 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669891661.03)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for yy_c.3 (0,2)
            for rc.2 (0,2)
              for rx.2 (0,5)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 957	GFLOPS: 6697.94 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.76, Tstamp:1669891662.69)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for ry.2 (0,5)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 958	GFLOPS: 448.12 / 7062.44	results: MeasureResult(cost:[0.0029], error_no:0, all_cost:2.65, Tstamp:1669891664.32)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,112)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 16
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,135)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for nn_c.3 (0,2)
            for xx_c.3 (0,2)
              for rx.2 (0,5)
                for yy_c.4 (0,7)
                  for xx_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for yy.3 (0,7)
          for xx.3 (0,4)
            conv2d_nchw = ...

==================================================
No: 959	GFLOPS: 6562.39 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.75, Tstamp:1669891666.03)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,14)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for nn_c.3 (0,2)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 960	GFLOPS: 6729.80 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.85, Tstamp:1669891667.84)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rc.2 (0,2)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

Time elapsed for measurement: 124.05 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 1.15 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 57	fail_ct: 4039	Time elapsed: 2.10
GA Iter: 0	Max score: 0.7285	Min score: 0.0371	#Pop: 57	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9993	Min score: 0.9349	#Pop: 128	#M+: 1398	#M-: 0
EvolutionarySearch		#s: 128	Time elapsed: 14.42
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************==================================================
No: 961	GFLOPS: 8586.60 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.97, Tstamp:1669891699.46)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for nn_c.3 (0,4)
        for ff_c.3 (0,4)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 962	GFLOPS: 8597.76 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.45, Tstamp:1669891701.42)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for nn_c.3 (0,4)
        for ff_c.3 (0,4)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 963	GFLOPS: 8567.79 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669891703.13)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 964	GFLOPS: 8599.33 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.99, Tstamp:1669891704.84)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 965	GFLOPS: 8575.57 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669891706.56)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ff_c.3 (0,2)
        for yy_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 966	GFLOPS: 8554.30 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.22, Tstamp:1669891708.46)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for ff_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 967	GFLOPS: 8562.67 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.10, Tstamp:1669891710.19)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for ff_c.3 (0,4)
          for ry.2 (0,3)
            for nn_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 968	GFLOPS: 8507.89 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.82, Tstamp:1669891711.89)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for nn_c.3 (0,2)
        for ff_c.3 (0,4)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 969	GFLOPS: 8580.05 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.06, Tstamp:1669891713.61)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for nn_c.4 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 970	GFLOPS: 8504.79 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.22, Tstamp:1669891715.48)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 971	GFLOPS: 8557.42 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.12, Tstamp:1669891717.20)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 972	GFLOPS: 8589.25 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.06, Tstamp:1669891718.90)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for yy_c.3 (0,2)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 973	GFLOPS: 8584.23 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.06, Tstamp:1669891720.65)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for yy_c.3 (0,2)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 974	GFLOPS: 8491.78 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.26, Tstamp:1669891722.54)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for ff_c.3 (0,4)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 975	GFLOPS: 8581.93 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669891724.26)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for ff_c.3 (0,2)
          for yy_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 976	GFLOPS: 8605.96 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669891725.98)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 977	GFLOPS: 8523.38 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669891727.68)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 978	GFLOPS: 8528.85 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.21, Tstamp:1669891729.55)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,4)
          for ff_c.3 (0,4)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 979	GFLOPS: 8607.45 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669891731.27)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.2 (0,3)
        for rx.2 (0,3)
          for nn_c.4 (0,4)
            for ff_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 980	GFLOPS: 8597.86 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.07, Tstamp:1669891733.00)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,4)
              for yy_c.3 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 981	GFLOPS: 8604.53 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.06, Tstamp:1669891734.70)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,2)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 982	GFLOPS: 8593.76 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.16, Tstamp:1669891736.57)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 983	GFLOPS: 8598.69 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669891738.30)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 984	GFLOPS: 8604.30 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.90, Tstamp:1669891739.98)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,2)
          for rx.2 (0,3)
            for nn_c.4 (0,2)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 985	GFLOPS: 8600.43 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669891741.67)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for nn_c.3 (0,4)
        for ry.2 (0,3)
          for rx.2 (0,3)
            for ff_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 986	GFLOPS: 8568.82 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.23, Tstamp:1669891743.57)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for yy_c.3 (0,2)
        for ry.2 (0,3)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 987	GFLOPS: 8570.06 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669891745.25)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for nn_c.3 (0,2)
        for yy_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 988	GFLOPS: 8547.95 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.96, Tstamp:1669891746.96)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for yy_c.3 (0,2)
          for ry.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 989	GFLOPS: 8577.66 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.92, Tstamp:1669891748.72)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for nn_c.4 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 990	GFLOPS: 8548.15 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.28, Tstamp:1669891750.67)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 991	GFLOPS: 8543.80 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.98, Tstamp:1669891752.34)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for nn_c.3 (0,2)
        for ff_c.3 (0,4)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 992	GFLOPS: 8490.15 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669891754.06)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 993	GFLOPS: 8579.07 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.00, Tstamp:1669891755.77)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,4)
          for yy_c.3 (0,2)
            for rx.2 (0,3)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 994	GFLOPS: 8504.87 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.15, Tstamp:1669891757.64)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,2)
          for ff_c.3 (0,4)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 995	GFLOPS: 8556.20 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.96, Tstamp:1669891759.34)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for nn_c.3 (0,4)
        for ff_c.3 (0,4)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 996	GFLOPS: 8592.25 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.94, Tstamp:1669891761.05)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,4)
          for ff_c.3 (0,4)
            for rx.2 (0,3)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 997	GFLOPS: 8521.61 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669891762.89)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,4)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 998	GFLOPS: 8486.96 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.13, Tstamp:1669891764.79)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,4)
          for ff_c.3 (0,4)
            for ry.2 (0,3)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 999	GFLOPS: 8509.16 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.97, Tstamp:1669891766.51)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for ff_c.3 (0,2)
                for yy_c.3 (0,2)
                  for nn_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1000	GFLOPS: 8606.49 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.98, Tstamp:1669891768.21)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1001	GFLOPS: 8523.16 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.91, Tstamp:1669891769.90)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for ff_c.3 (0,2)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1002	GFLOPS: 8575.96 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.15, Tstamp:1669891771.77)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1003	GFLOPS: 8595.62 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.99, Tstamp:1669891773.50)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.2 (0,3)
        for rx.2 (0,3)
          for nn_c.4 (0,4)
            for ff_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1004	GFLOPS: 8591.94 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.98, Tstamp:1669891775.21)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for nn_c.3 (0,4)
        for ry.2 (0,3)
          for rx.2 (0,3)
            for ff_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1005	GFLOPS: 8592.48 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.96, Tstamp:1669891776.94)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for nn_c.3 (0,2)
        for ry.2 (0,3)
          for rx.2 (0,3)
            for nn_c.4 (0,2)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1006	GFLOPS: 8532.61 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669891778.83)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for ff_c.3 (0,2)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1007	GFLOPS: 8596.59 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.93, Tstamp:1669891780.55)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for nn_c.4 (0,2)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1008	GFLOPS: 8550.40 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.97, Tstamp:1669891782.26)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for nn_c.3 (0,4)
        for ff_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1009	GFLOPS: 8599.51 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669891784.06)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,4)
          for ff_c.3 (0,2)
            for rx.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1010	GFLOPS: 8493.70 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.12, Tstamp:1669891785.98)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ff_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for nn_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1011	GFLOPS: 8568.87 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.91, Tstamp:1669891787.68)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for ff_c.3 (0,4)
            for nn_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1012	GFLOPS: 8585.51 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.88, Tstamp:1669891789.41)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,4)
          for ff_c.3 (0,4)
            for rx.2 (0,3)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1013	GFLOPS: 8478.05 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.90, Tstamp:1669891791.12)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,4)
          for ff_c.3 (0,4)
            for ry.2 (0,3)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1014	GFLOPS: 8575.60 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.08, Tstamp:1669891793.06)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for nn_c.3 (0,4)
        for ff_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1015	GFLOPS: 8471.08 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.92, Tstamp:1669891794.77)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for nn_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1016	GFLOPS: 8556.73 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.84, Tstamp:1669891796.49)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,4)
              for yy_c.3 (0,2)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1017	GFLOPS: 8445.79 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.00, Tstamp:1669891798.20)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for rx.2 (0,3)
                  for nn_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1018	GFLOPS: 8576.01 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.31, Tstamp:1669891800.15)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ff_c.3 (0,4)
        for yy_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1019	GFLOPS: 8503.45 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.89, Tstamp:1669891801.82)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,2)
          for ff_c.3 (0,4)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1020	GFLOPS: 8498.51 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.10, Tstamp:1669891803.66)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,2)
          for ff_c.3 (0,4)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1021	GFLOPS: 8496.15 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.03, Tstamp:1669891805.36)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,2)
          for ff_c.3 (0,4)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1022	GFLOPS: 8384.45 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.07, Tstamp:1669891807.23)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for yy_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1023	GFLOPS: 8361.15 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.12, Tstamp:1669891809.11)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for yy_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1024	GFLOPS: 8358.53 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.88, Tstamp:1669891810.79)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for yy_c.3 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

Time elapsed for measurement: 125.20 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 1.13 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Iter: 5	#Pop: 39	#Target: 50	fail_ct: 10201	Time elapsed: 4.16
Sample Initial Population	#s: 52	fail_ct: 14284	Time elapsed: 5.81
GA Iter: 0	Max score: 0.7694	Min score: -0.2338	#Pop: 52	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9971	Min score: 0.9079	#Pop: 128	#M+: 1396	#M-: 0
EvolutionarySearch		#s: 128	Time elapsed: 14.31
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************==================================================
No: 1025	GFLOPS: 7034.29 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.91, Tstamp:1669891846.03)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1026	GFLOPS: 7029.66 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.81, Tstamp:1669891847.67)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1027	GFLOPS: 7020.76 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.85, Tstamp:1669891849.32)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1028	GFLOPS: 7030.84 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.89, Tstamp:1669891850.96)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for yy_c.3 (0,2)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1029	GFLOPS: 7011.82 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.95, Tstamp:1669891852.60)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1030	GFLOPS: 7007.15 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.29, Tstamp:1669891854.45)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for yy_c.3 (0,2)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1031	GFLOPS: 6993.45 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.06, Tstamp:1669891856.12)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1032	GFLOPS: 7004.66 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669891857.78)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1033	GFLOPS: 6995.17 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669891859.57)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1034	GFLOPS: 7003.05 / 7062.44	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.27, Tstamp:1669891861.42)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for yy_c.3 (0,2)
          for rc.2 (0,2)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1035	GFLOPS: 7112.07 / 7112.07	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.07, Tstamp:1669891863.11)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for yy_c.3 (0,2)
        for rc.2 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1036	GFLOPS: 7040.86 / 7112.07	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.20, Tstamp:1669891864.81)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1037	GFLOPS: 7020.81 / 7112.07	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669891866.59)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.2 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1038	GFLOPS: 7029.22 / 7112.07	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.41, Tstamp:1669891868.43)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for yy_c.3 (0,2)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1039	GFLOPS: 6796.82 / 7112.07	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669891870.10)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for rc.2 (0,2)
          for ry.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1040	GFLOPS: 6760.15 / 7112.07	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669891871.76)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for rc.2 (0,2)
          for ry.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1041	GFLOPS: 6941.37 / 7112.07	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.09, Tstamp:1669891873.55)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for ry.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1042	GFLOPS: 6888.44 / 7112.07	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669891875.36)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for ry.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1043	GFLOPS: 6361.04 / 7112.07	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.27, Tstamp:1669891877.00)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,8)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for yy_c.4 (0,2)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1044	GFLOPS: 6903.00 / 7112.07	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.85, Tstamp:1669891878.65)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for ry.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1045	GFLOPS: 6990.92 / 7112.07	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.07, Tstamp:1669891880.31)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for rc.2 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1046	GFLOPS: 6923.06 / 7112.07	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.17, Tstamp:1669891881.97)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for rc.2 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1047	GFLOPS: 6944.86 / 7112.07	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.37, Tstamp:1669891883.63)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.2 (0,5)
        for rx.2 (0,5)
          for yy_c.4 (0,2)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1048	GFLOPS: 6913.65 / 7112.07	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.43, Tstamp:1669891885.31)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1049	GFLOPS: 6914.96 / 7112.07	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.44, Tstamp:1669891887.06)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for yy_c.4 (0,2)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1050	GFLOPS: 6864.60 / 7112.07	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.39, Tstamp:1669891888.73)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for yy_c.3 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1051	GFLOPS: 4602.66 / 7112.07	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:3.37, Tstamp:1669891890.37)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.2 (0,5)
          for nn_c.4 (0,2)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for nn.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1052	GFLOPS: 6757.21 / 7112.07	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669891892.03)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1053	GFLOPS: 6777.83 / 7112.07	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.14, Tstamp:1669891893.85)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1054	GFLOPS: 6755.91 / 7112.07	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669891895.52)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1055	GFLOPS: 6775.58 / 7112.07	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669891897.21)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rc.2 (0,2)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1056	GFLOPS: 6795.96 / 7112.07	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669891899.10)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for yy_c.3 (0,2)
          for rc.2 (0,2)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1057	GFLOPS: 6782.93 / 7112.07	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.16, Tstamp:1669891900.94)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for yy_c.3 (0,2)
        for rc.2 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1058	GFLOPS: 6796.88 / 7112.07	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.96, Tstamp:1669891902.60)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rc.2 (0,2)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1059	GFLOPS: 6775.77 / 7112.07	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.99, Tstamp:1669891904.25)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1060	GFLOPS: 6788.47 / 7112.07	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.92, Tstamp:1669891906.16)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for yy_c.3 (0,2)
        for rc.2 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1061	GFLOPS: 6762.81 / 7112.07	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669891908.01)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1062	GFLOPS: 6915.71 / 7112.07	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.39, Tstamp:1669891909.67)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for yy_c.3 (0,2)
          for rx.2 (0,5)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1063	GFLOPS: 6520.83 / 7112.07	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.78, Tstamp:1669891911.34)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1064	GFLOPS: 4712.09 / 7112.07	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.05, Tstamp:1669891913.23)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for nn_c.3 (0,2)
        for rc.2 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1065	GFLOPS: 6975.60 / 7112.07	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.00, Tstamp:1669891915.08)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1066	GFLOPS: 6985.16 / 7112.07	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.81, Tstamp:1669891916.75)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1067	GFLOPS: 6973.34 / 7112.07	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.90, Tstamp:1669891918.43)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1068	GFLOPS: 6977.75 / 7112.07	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.76, Tstamp:1669891920.19)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for yy_c.3 (0,2)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1069	GFLOPS: 7119.92 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.12, Tstamp:1669891922.12)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for yy_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1070	GFLOPS: 6533.98 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.95, Tstamp:1669891923.79)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for rc.2 (0,2)
          for ry.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1071	GFLOPS: 6701.82 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.84, Tstamp:1669891925.46)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for ry.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1072	GFLOPS: 6977.92 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.83, Tstamp:1669891927.37)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1073	GFLOPS: 6969.35 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.97, Tstamp:1669891929.21)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1074	GFLOPS: 6972.78 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.67, Tstamp:1669891930.86)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1075	GFLOPS: 7097.77 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.77, Tstamp:1669891932.53)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for yy_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1076	GFLOPS: 6974.87 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.75, Tstamp:1669891934.48)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for yy_c.3 (0,2)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1077	GFLOPS: 6533.21 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.88, Tstamp:1669891936.32)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1078	GFLOPS: 6721.60 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.69, Tstamp:1669891937.97)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for ry.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1079	GFLOPS: 6277.78 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.80, Tstamp:1669891939.63)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for rc.2 (0,2)
          for ry.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1080	GFLOPS: 6704.51 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.89, Tstamp:1669891941.48)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1081	GFLOPS: 6728.04 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.34, Tstamp:1669891943.32)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for yy_c.3 (0,2)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1082	GFLOPS: 6498.90 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.00, Tstamp:1669891944.96)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for rc.2 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1083	GFLOPS: 5559.11 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.58, Tstamp:1669891946.59)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,4)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for yy_c.4 (0,2)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1084	GFLOPS: 6578.63 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.96, Tstamp:1669891948.21)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for ry.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1085	GFLOPS: 5137.34 / 7119.92	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:2.92, Tstamp:1669891949.84)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,3)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          vectorize ax0@ax1@ax2@ax3@.1 (0,5)
            pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1086	GFLOPS: 2496.19 / 7119.92	results: MeasureResult(cost:[0.0005], error_no:0, all_cost:2.90, Tstamp:1669891951.47)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,112)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,896)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,896)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,13)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,896)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for yy_c.3 (0,2)
            for rx.2 (0,5)
              conv2d_nchw.local = ...
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1087	GFLOPS: 1979.04 / 7119.92	results: MeasureResult(cost:[0.0006], error_no:0, all_cost:2.95, Tstamp:1669891953.60)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,224)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,8)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,448)
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,448)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,19)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,448)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for rx.2 (0,5)
              conv2d_nchw.local = ...
      conv2d_nchw = ...

==================================================
No: 1088	GFLOPS: 5128.94 / 7119.92	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:2.90, Tstamp:1669891955.29)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,3)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          vectorize ax0@ax1@ax2@ax3@.1 (0,5)
            pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

Time elapsed for measurement: 123.14 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 1.06 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 57	fail_ct: 4039	Time elapsed: 2.25
GA Iter: 0	Max score: 0.9241	Min score: -0.0119	#Pop: 57	#M+: 0	#M-: 0
GA Iter: 4	Max score: 1.0159	Min score: 0.9381	#Pop: 128	#M+: 1396	#M-: 0
EvolutionarySearch		#s: 128	Time elapsed: 14.41
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************==================================================
No: 1089	GFLOPS: 2828.69 / 8616.58	results: MeasureResult(cost:[0.0007], error_no:0, all_cost:3.56, Tstamp:1669891988.64)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 16
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,2)
          for ff_c.3 (0,4)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1090	GFLOPS: 6910.23 / 8616.58	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.31, Tstamp:1669891990.80)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,2)
          for ff_c.3 (0,4)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1091	GFLOPS: 8552.18 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.87, Tstamp:1669891992.51)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for nn_c.3 (0,2)
        for ff_c.3 (0,4)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1092	GFLOPS: 6829.02 / 8616.58	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:3.74, Tstamp:1669891994.20)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,4)
              for yy_c.3 (0,2)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1093	GFLOPS: 8578.54 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669891995.90)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for nn_c.3 (0,2)
        for ff_c.3 (0,4)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1094	GFLOPS: 8543.84 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.36, Tstamp:1669891997.82)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,4)
              for yy_c.3 (0,2)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1095	GFLOPS: 8539.52 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.10, Tstamp:1669891999.52)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for ff_c.3 (0,4)
            for yy_c.3 (0,2)
              for nn_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1096	GFLOPS: 8489.70 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669892001.22)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,2)
          for ff_c.3 (0,4)
            for ry.2 (0,3)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1097	GFLOPS: 8593.13 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669892002.92)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for nn_c.3 (0,4)
        for ff_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1098	GFLOPS: 8507.41 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.20, Tstamp:1669892004.87)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,4)
          for ff_c.3 (0,4)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1099	GFLOPS: 8551.20 / 8616.58	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669892006.59)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for ff_c.3 (0,4)
            for yy_c.3 (0,2)
              for nn_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1100	GFLOPS: 8625.46 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.99, Tstamp:1669892008.30)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for ff_c.3 (0,4)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1101	GFLOPS: 8499.31 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.95, Tstamp:1669892010.00)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for ff_c.3 (0,4)
          for yy_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1102	GFLOPS: 8482.31 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.12, Tstamp:1669892011.83)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for ff_c.3 (0,4)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1103	GFLOPS: 8566.05 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.92, Tstamp:1669892013.53)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for ff_c.3 (0,2)
          for yy_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1104	GFLOPS: 8561.60 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669892015.23)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1105	GFLOPS: 8571.93 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.94, Tstamp:1669892016.99)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for ff_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1106	GFLOPS: 8581.73 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.21, Tstamp:1669892018.86)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ff_c.3 (0,2)
        for yy_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1107	GFLOPS: 8624.37 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669892020.56)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1108	GFLOPS: 8622.41 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.91, Tstamp:1669892022.26)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for ff_c.3 (0,4)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1109	GFLOPS: 8550.69 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.87, Tstamp:1669892023.97)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for ry.2 (0,3)
          for nn_c.4 (0,4)
            for ff_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1110	GFLOPS: 8527.28 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.19, Tstamp:1669892025.85)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for ff_c.3 (0,2)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1111	GFLOPS: 8539.97 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.97, Tstamp:1669892027.56)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ff_c.3 (0,2)
        for ry.2 (0,3)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1112	GFLOPS: 8486.23 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.97, Tstamp:1669892029.28)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,4)
          for ry.2 (0,3)
            for ff_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1113	GFLOPS: 8473.47 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.06, Tstamp:1669892030.99)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,2)
          for ry.2 (0,3)
            for nn_c.4 (0,2)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1114	GFLOPS: 8591.60 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.17, Tstamp:1669892032.89)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for nn_c.3 (0,2)
        for ry.2 (0,3)
          for rx.2 (0,3)
            for nn_c.4 (0,2)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1115	GFLOPS: 8597.98 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669892034.59)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,2)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1116	GFLOPS: 8589.76 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.96, Tstamp:1669892036.29)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,4)
              for yy_c.3 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1117	GFLOPS: 8592.68 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.89, Tstamp:1669892038.01)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.2 (0,3)
          for nn_c.4 (0,4)
            for ff_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1118	GFLOPS: 8545.51 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669892039.85)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ff_c.3 (0,2)
        for ry.2 (0,3)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1119	GFLOPS: 8581.00 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.87, Tstamp:1669892041.54)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,4)
          for rx.2 (0,3)
            for ff_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1120	GFLOPS: 8599.99 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.91, Tstamp:1669892043.22)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1121	GFLOPS: 8522.17 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.96, Tstamp:1669892044.98)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for nn_c.3 (0,2)
        for ff_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1122	GFLOPS: 8578.87 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.12, Tstamp:1669892046.84)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ff_c.3 (0,4)
        for yy_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1123	GFLOPS: 8576.51 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.85, Tstamp:1669892048.53)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for yy_c.3 (0,2)
        for ry.2 (0,3)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1124	GFLOPS: 8573.74 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.76, Tstamp:1669892050.23)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,4)
          for yy_c.3 (0,2)
            for rx.2 (0,3)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1125	GFLOPS: 8528.56 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.94, Tstamp:1669892051.96)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ff_c.3 (0,4)
        for ry.2 (0,3)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1126	GFLOPS: 8531.78 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.94, Tstamp:1669892053.82)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for nn_c.3 (0,4)
        for ff_c.3 (0,4)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1127	GFLOPS: 8485.89 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.91, Tstamp:1669892055.53)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for rx.2 (0,3)
                  for nn_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1128	GFLOPS: 8368.50 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.79, Tstamp:1669892057.23)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1129	GFLOPS: 8340.67 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.91, Tstamp:1669892058.95)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for rx.2 (0,3)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1130	GFLOPS: 4450.39 / 8625.46	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:5.25, Tstamp:1669892061.31)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,8)
      for ry.0 (0,3)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for rc.2 (0,4)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,4)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1131	GFLOPS: 2798.71 / 8625.46	results: MeasureResult(cost:[0.0007], error_no:0, all_cost:3.78, Tstamp:1669892063.68)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 16
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for nn_c.3 (0,4)
        for ff_c.3 (0,4)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1132	GFLOPS: 4456.31 / 8625.46	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:5.37, Tstamp:1669892066.03)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,8)
      for ry.0 (0,3)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for rc.2 (0,4)
                  for nn_c.4 (0,2)
                    for ff_c.4 (0,2)
                      conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1133	GFLOPS: 4436.00 / 8625.46	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:5.33, Tstamp:1669892068.35)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,8)
      for ry.0 (0,3)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rc.1 (0,4)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    for yy_c.4 (0,2)
                      conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1134	GFLOPS: 2796.46 / 8625.46	results: MeasureResult(cost:[0.0007], error_no:0, all_cost:3.69, Tstamp:1669892070.68)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 16
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for ff_c.3 (0,4)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1135	GFLOPS: 2791.47 / 8625.46	results: MeasureResult(cost:[0.0007], error_no:0, all_cost:3.26, Tstamp:1669892072.50)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 16
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for nn_c.3 (0,2)
        for ff_c.3 (0,4)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1136	GFLOPS: 4444.45 / 8625.46	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:5.22, Tstamp:1669892074.67)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,8)
      for ry.0 (0,3)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rc.1 (0,4)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for ff_c.3 (0,4)
                for nn_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1137	GFLOPS: 7007.76 / 8625.46	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:3.54, Tstamp:1669892076.37)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,4)
          for ff_c.3 (0,4)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1138	GFLOPS: 4944.82 / 8625.46	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:4.47, Tstamp:1669892078.57)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      for ry.0 (0,3)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rc.1 (0,2)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,4)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1139	GFLOPS: 4424.26 / 8625.46	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:5.07, Tstamp:1669892080.76)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,8)
      for ry.0 (0,3)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ff_c.3 (0,4)
          for rc.2 (0,4)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1140	GFLOPS: 5817.68 / 8625.46	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:3.96, Tstamp:1669892082.46)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      for ry.0 (0,3)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ff_c.3 (0,4)
            for rc.2 (0,2)
              for rx.2 (0,3)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1141	GFLOPS: 4416.99 / 8625.46	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:5.28, Tstamp:1669892084.77)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,8)
      for ry.0 (0,3)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ff_c.3 (0,4)
          for rc.2 (0,4)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1142	GFLOPS: 8565.31 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.61, Tstamp:1669892086.67)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for nn_c.3 (0,4)
        for ry.2 (0,3)
          for rx.2 (0,3)
            for ff_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1143	GFLOPS: 6759.81 / 8625.46	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:3.50, Tstamp:1669892088.36)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,4)
          for ry.2 (0,3)
            for ff_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1144	GFLOPS: 2774.36 / 8625.46	results: MeasureResult(cost:[0.0007], error_no:0, all_cost:3.74, Tstamp:1669892090.68)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 16
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for rx.2 (0,3)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1145	GFLOPS: 2776.52 / 8625.46	results: MeasureResult(cost:[0.0007], error_no:0, all_cost:3.37, Tstamp:1669892092.92)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 16
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for rx.2 (0,3)
                  for nn_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1146	GFLOPS: 2792.51 / 8625.46	results: MeasureResult(cost:[0.0007], error_no:0, all_cost:3.13, Tstamp:1669892095.18)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 16
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for nn_c.3 (0,2)
        for ff_c.3 (0,4)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1147	GFLOPS: 6875.64 / 8625.46	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:3.48, Tstamp:1669892097.42)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for ff_c.3 (0,2)
          for yy_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1148	GFLOPS: 6822.51 / 8625.46	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:2.93, Tstamp:1669892099.20)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1149	GFLOPS: 8576.45 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.94, Tstamp:1669892100.90)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for nn_c.3 (0,2)
        for ff_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1150	GFLOPS: 8564.00 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.87, Tstamp:1669892102.61)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ff_c.3 (0,2)
        for yy_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1151	GFLOPS: 4383.66 / 8625.46	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:4.31, Tstamp:1669892104.31)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      for ry.0 (0,3)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ff_c.3 (0,4)
            for rc.2 (0,2)
              for rx.2 (0,3)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1152	GFLOPS: 5924.79 / 8625.46	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:3.17, Tstamp:1669892106.06)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      for ry.0 (0,3)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ff_c.3 (0,4)
            for rc.2 (0,2)
              for rx.2 (0,3)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

Time elapsed for measurement: 132.92 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 1.00 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Iter: 5	#Pop: 26	#Target: 50	fail_ct: 10214	Time elapsed: 4.15
Sample Initial Population	#s: 52	fail_ct: 18380	Time elapsed: 7.57
GA Iter: 0	Max score: 0.3791	Min score: -0.0271	#Pop: 52	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9910	Min score: 0.9023	#Pop: 128	#M+: 1392	#M-: 0
EvolutionarySearch		#s: 128	Time elapsed: 14.64
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************==================================================
No: 1153	GFLOPS: 7042.18 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.94, Tstamp:1669892144.07)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for yy_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1154	GFLOPS: 5891.62 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.94, Tstamp:1669892145.73)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,5)
          for yy_c.3 (0,2)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1155	GFLOPS: 6996.49 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.16, Tstamp:1669892147.59)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1156	GFLOPS: 7037.85 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.82, Tstamp:1669892149.24)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1157	GFLOPS: 5881.88 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.99, Tstamp:1669892150.89)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1158	GFLOPS: 5894.76 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.99, Tstamp:1669892152.68)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1159	GFLOPS: 5878.45 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.10, Tstamp:1669892154.51)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1160	GFLOPS: 5863.22 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.93, Tstamp:1669892156.16)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for yy_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1161	GFLOPS: 5831.42 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.97, Tstamp:1669892157.84)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1162	GFLOPS: 5822.53 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.93, Tstamp:1669892159.64)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1163	GFLOPS: 7076.84 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.46, Tstamp:1669892161.61)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for yy_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1164	GFLOPS: 7010.13 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669892163.28)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for yy_c.3 (0,2)
          for rc.2 (0,2)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1165	GFLOPS: 7118.27 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.97, Tstamp:1669892164.93)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for yy_c.3 (0,2)
        for rc.2 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1166	GFLOPS: 7112.15 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.99, Tstamp:1669892166.84)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for yy_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1167	GFLOPS: 7035.00 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.21, Tstamp:1669892168.72)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for yy_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1168	GFLOPS: 7024.86 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.09, Tstamp:1669892170.40)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for yy_c.3 (0,2)
        for rc.2 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1169	GFLOPS: 7031.72 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669892172.07)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1170	GFLOPS: 7058.63 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669892173.83)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.2 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1171	GFLOPS: 6904.21 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.99, Tstamp:1669892175.62)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1172	GFLOPS: 7008.10 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669892177.30)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for yy_c.3 (0,2)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1173	GFLOPS: 7003.84 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.00, Tstamp:1669892178.97)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for yy_c.3 (0,2)
          for rc.2 (0,2)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1174	GFLOPS: 7058.17 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.24, Tstamp:1669892180.84)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1175	GFLOPS: 7021.94 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.32, Tstamp:1669892182.82)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1176	GFLOPS: 6988.29 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.96, Tstamp:1669892184.49)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rc.2 (0,2)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1177	GFLOPS: 6994.03 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669892186.18)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.2 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1178	GFLOPS: 6733.81 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669892187.98)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for rc.2 (0,2)
          for ry.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1179	GFLOPS: 6938.59 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.21, Tstamp:1669892189.85)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1180	GFLOPS: 6740.10 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.98, Tstamp:1669892191.51)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for yy_c.3 (0,2)
          for rc.2 (0,2)
            for ry.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1181	GFLOPS: 6983.83 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669892193.19)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.2 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1182	GFLOPS: 6976.93 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.96, Tstamp:1669892195.13)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rc.2 (0,2)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1183	GFLOPS: 6955.09 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.14, Tstamp:1669892197.00)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1184	GFLOPS: 6939.11 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669892198.68)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for rc.2 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1185	GFLOPS: 6753.08 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.88, Tstamp:1669892200.35)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for ry.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1186	GFLOPS: 6749.17 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.89, Tstamp:1669892202.11)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1187	GFLOPS: 6953.68 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.19, Tstamp:1669892203.98)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1188	GFLOPS: 6757.54 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.86, Tstamp:1669892205.64)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1189	GFLOPS: 6787.22 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.88, Tstamp:1669892207.32)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for ry.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1190	GFLOPS: 6950.69 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.96, Tstamp:1669892209.09)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for ry.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1191	GFLOPS: 7011.11 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.18, Tstamp:1669892210.95)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1192	GFLOPS: 5411.93 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.92, Tstamp:1669892212.61)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1193	GFLOPS: 5440.49 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.81, Tstamp:1669892214.24)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,5)
          for ry.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1194	GFLOPS: 6778.61 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.13, Tstamp:1669892216.08)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.2 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1195	GFLOPS: 6755.82 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.13, Tstamp:1669892217.96)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for yy_c.3 (0,2)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1196	GFLOPS: 6791.54 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.92, Tstamp:1669892219.63)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for yy_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1197	GFLOPS: 6773.90 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.84, Tstamp:1669892221.30)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for yy_c.3 (0,2)
          for rc.2 (0,2)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1198	GFLOPS: 6781.08 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.77, Tstamp:1669892223.06)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for nn_c.3 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1199	GFLOPS: 6811.40 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.70, Tstamp:1669892225.03)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for yy_c.3 (0,2)
          for ry.2 (0,5)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1200	GFLOPS: 6838.40 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.41, Tstamp:1669892226.69)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for ry.2 (0,5)
          for yy_c.4 (0,2)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1201	GFLOPS: 6747.07 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.95, Tstamp:1669892228.38)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1202	GFLOPS: 6914.67 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.42, Tstamp:1669892230.15)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1203	GFLOPS: 6909.64 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.59, Tstamp:1669892232.00)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.2 (0,5)
          for yy_c.4 (0,2)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1204	GFLOPS: 6830.58 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.37, Tstamp:1669892233.67)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for ry.2 (0,5)
          for yy_c.4 (0,2)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1205	GFLOPS: 4715.15 / 7119.92	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:3.78, Tstamp:1669892235.31)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for nn_c.3 (0,2)
        for rc.2 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1206	GFLOPS: 6357.66 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.42, Tstamp:1669892237.17)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for yy_c.3 (0,2)
          for rx.2 (0,5)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1207	GFLOPS: 6671.50 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.81, Tstamp:1669892239.05)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for yy_c.3 (0,2)
            for rc.2 (0,2)
              for rx.2 (0,5)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1208	GFLOPS: 6723.81 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.83, Tstamp:1669892240.70)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for yy_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1209	GFLOPS: 6986.38 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.15, Tstamp:1669892242.37)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for xx_c.3 (0,2)
          for rc.2 (0,2)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1210	GFLOPS: 6344.45 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.68, Tstamp:1669892244.16)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for yy_c.3 (0,2)
          for ry.2 (0,5)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1211	GFLOPS: 6354.50 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.92, Tstamp:1669892246.04)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for yy_c.3 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1212	GFLOPS: 6365.98 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.66, Tstamp:1669892247.71)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for ry.2 (0,5)
          for yy_c.4 (0,2)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1213	GFLOPS: 6751.69 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.89, Tstamp:1669892249.37)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for rc.2 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1214	GFLOPS: 236.78 / 7119.92	results: MeasureResult(cost:[0.0054], error_no:0, all_cost:2.37, Tstamp:1669892251.14)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,392)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
      conv2d_nchw.local auto_unroll: 16
      for rc.0 (0,8)
        for ry.0 (0,5)
          for rx.0 (0,5)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
              kernel.shared = ...
            for ax0@ax1@ax2@ax3@.0.0 (0,256)
              threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
                pad_temp.shared = ...
            for rc.1 (0,4)
              for yy_c.3 (0,2)
                for yy_c.4 (0,8)
                  for xx_c.4 (0,2)
                    conv2d_nchw.local = ...
      for yy.3 (0,16)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1215	GFLOPS: 6704.99 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669892253.02)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1216	GFLOPS: 6681.59 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.77, Tstamp:1669892254.68)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,5)
            for yy_c.3 (0,2)
              for rx.2 (0,5)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

Time elapsed for measurement: 125.30 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 1.28 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 58	fail_ct: 4038	Time elapsed: 2.12
GA Iter: 0	Max score: 0.7208	Min score: 0.0354	#Pop: 58	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9926	Min score: 0.9322	#Pop: 128	#M+: 1390	#M-: 0
EvolutionarySearch		#s: 128	Time elapsed: 15.13
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************==================================================
No: 1217	GFLOPS: 8550.10 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.97, Tstamp:1669892287.21)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for ff_c.3 (0,2)
          for ry.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1218	GFLOPS: 8608.05 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.88, Tstamp:1669892288.92)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,2)
          for rx.2 (0,3)
            for nn_c.4 (0,2)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1219	GFLOPS: 8493.75 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.28, Tstamp:1669892290.61)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1220	GFLOPS: 8601.36 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.18, Tstamp:1669892292.48)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,4)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1221	GFLOPS: 8524.74 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669892294.19)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1222	GFLOPS: 8509.98 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669892296.05)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,2)
          for ff_c.3 (0,4)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1223	GFLOPS: 8619.08 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.17, Tstamp:1669892297.95)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,4)
          for ff_c.3 (0,4)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1224	GFLOPS: 8369.66 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.10, Tstamp:1669892299.66)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1225	GFLOPS: 8358.11 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.98, Tstamp:1669892301.36)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for ff_c.3 (0,4)
                for yy_c.3 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1226	GFLOPS: 8341.87 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.06, Tstamp:1669892303.14)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for ff_c.3 (0,4)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1227	GFLOPS: 7598.16 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.31, Tstamp:1669892305.06)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,5)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,4)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for nn_c.4 (0,2)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1228	GFLOPS: 7612.54 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.11, Tstamp:1669892306.76)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,5)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,4)
            pad_temp.shared = ...
      for nn_c.3 (0,2)
        for ry.2 (0,3)
          for rx.2 (0,3)
            for nn_c.4 (0,2)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1229	GFLOPS: 7535.32 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.15, Tstamp:1669892308.47)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,5)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,4)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,2)
          for ry.2 (0,3)
            for nn_c.4 (0,2)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1230	GFLOPS: 7516.17 / 8625.46	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.07, Tstamp:1669892310.16)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,6)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,3)
            pad_temp.shared = ...
      for nn_c.3 (0,4)
        for ff_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1231	GFLOPS: 7606.45 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.35, Tstamp:1669892312.09)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,5)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,4)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for yy_c.3 (0,2)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1232	GFLOPS: 7606.35 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.12, Tstamp:1669892313.78)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,5)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,4)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1233	GFLOPS: 7526.10 / 8625.46	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.06, Tstamp:1669892315.48)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,5)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,4)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1234	GFLOPS: 7541.63 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.62, Tstamp:1669892317.77)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,6)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,3)
            pad_temp.shared = ...
      for nn_c.3 (0,4)
        for ff_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1235	GFLOPS: 7476.70 / 8625.46	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.04, Tstamp:1669892319.63)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,6)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,3)
            pad_temp.shared = ...
      for ff_c.3 (0,2)
        for yy_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1236	GFLOPS: 7584.13 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.69, Tstamp:1669892322.00)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,6)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,3)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1237	GFLOPS: 7534.46 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.72, Tstamp:1669892324.41)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,6)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,3)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,4)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1238	GFLOPS: 7497.27 / 8625.46	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.23, Tstamp:1669892326.31)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,6)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,3)
            pad_temp.shared = ...
      for nn_c.3 (0,4)
        for ry.2 (0,3)
          for rx.2 (0,3)
            for ff_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1239	GFLOPS: 7505.37 / 8625.46	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:3.91, Tstamp:1669892328.01)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,5)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,4)
            pad_temp.shared = ...
      for nn_c.3 (0,4)
        for ff_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1240	GFLOPS: 7248.09 / 8625.46	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.06, Tstamp:1669892329.72)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,3)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            pad_temp.shared = ...
      for nn_c.3 (0,4)
        for ff_c.3 (0,4)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1241	GFLOPS: 7233.41 / 8625.46	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.09, Tstamp:1669892331.41)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,3)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            pad_temp.shared = ...
      for nn_c.3 (0,4)
        for ff_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1242	GFLOPS: 7213.70 / 8625.46	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.27, Tstamp:1669892333.32)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,3)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,4)
              for yy_c.3 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1243	GFLOPS: 7238.50 / 8625.46	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.06, Tstamp:1669892335.02)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,3)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            pad_temp.shared = ...
      for ff_c.3 (0,2)
        for yy_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1244	GFLOPS: 7214.55 / 8625.46	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.07, Tstamp:1669892336.72)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,3)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,2)
          for rx.2 (0,3)
            for nn_c.4 (0,2)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1245	GFLOPS: 7213.16 / 8625.46	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.07, Tstamp:1669892338.48)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,3)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            pad_temp.shared = ...
      for nn_c.3 (0,2)
        for ry.2 (0,3)
          for rx.2 (0,3)
            for nn_c.4 (0,2)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1246	GFLOPS: 7250.61 / 8625.46	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.28, Tstamp:1669892340.39)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,3)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            pad_temp.shared = ...
      for ff_c.3 (0,2)
        for ry.2 (0,3)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1247	GFLOPS: 7244.73 / 8625.46	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.65, Tstamp:1669892342.58)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,3)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1248	GFLOPS: 7221.47 / 8625.46	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.05, Tstamp:1669892344.28)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,3)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            pad_temp.shared = ...
      for nn_c.3 (0,4)
        for ff_c.3 (0,4)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1249	GFLOPS: 7239.27 / 8625.46	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.05, Tstamp:1669892345.98)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,3)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            pad_temp.shared = ...
      for ff_c.3 (0,2)
        for yy_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1250	GFLOPS: 7222.93 / 8625.46	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.02, Tstamp:1669892347.79)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,3)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for yy_c.3 (0,2)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1251	GFLOPS: 7240.74 / 8625.46	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.21, Tstamp:1669892349.69)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,3)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1252	GFLOPS: 7253.11 / 8625.46	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.06, Tstamp:1669892351.39)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,3)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1253	GFLOPS: 7229.57 / 8625.46	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:3.99, Tstamp:1669892353.10)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,3)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            pad_temp.shared = ...
      for ff_c.3 (0,4)
        for ry.2 (0,3)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1254	GFLOPS: 7254.30 / 8625.46	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:3.95, Tstamp:1669892354.80)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,3)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1255	GFLOPS: 7280.05 / 8625.46	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.67, Tstamp:1669892357.17)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,3)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1256	GFLOPS: 7241.77 / 8625.46	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.22, Tstamp:1669892359.10)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,3)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            pad_temp.shared = ...
      for nn_c.3 (0,4)
        for yy_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1257	GFLOPS: 6792.09 / 8625.46	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:3.72, Tstamp:1669892360.81)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,4)
          for yy_c.3 (0,2)
            for rx.2 (0,3)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1258	GFLOPS: 8014.57 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.83, Tstamp:1669892362.51)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for nn_c.3 (0,2)
        for ff_c.3 (0,4)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1259	GFLOPS: 8006.72 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.86, Tstamp:1669892364.22)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for rx.2 (0,3)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1260	GFLOPS: 8002.45 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669892366.13)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for rx.2 (0,3)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1261	GFLOPS: 6920.92 / 8625.46	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:3.66, Tstamp:1669892367.84)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,4)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1262	GFLOPS: 6773.77 / 8625.46	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:3.77, Tstamp:1669892369.54)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1263	GFLOPS: 6940.42 / 8625.46	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:3.81, Tstamp:1669892371.23)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1264	GFLOPS: 8060.90 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.09, Tstamp:1669892373.17)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ff_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1265	GFLOPS: 8044.43 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.76, Tstamp:1669892374.85)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for rx.2 (0,3)
                  for nn_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1266	GFLOPS: 8018.38 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.84, Tstamp:1669892376.54)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ff_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1267	GFLOPS: 8013.58 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.73, Tstamp:1669892378.27)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for nn_c.3 (0,4)
        for ff_c.3 (0,4)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1268	GFLOPS: 8016.29 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.93, Tstamp:1669892380.16)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for nn_c.3 (0,4)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1269	GFLOPS: 7982.14 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.77, Tstamp:1669892381.87)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1270	GFLOPS: 8042.10 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.76, Tstamp:1669892383.57)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for rx.1 (0,3)
        for ff_c.3 (0,4)
          for ry.2 (0,3)
            for nn_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1271	GFLOPS: 8014.17 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.78, Tstamp:1669892385.31)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rx.1 (0,3)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1272	GFLOPS: 8018.61 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.94, Tstamp:1669892387.19)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for rx.2 (0,3)
                  for nn_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1273	GFLOPS: 7988.56 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.93, Tstamp:1669892388.89)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for ry.2 (0,3)
                for nn_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1274	GFLOPS: 8004.03 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.93, Tstamp:1669892390.60)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for nn_c.3 (0,4)
        for yy_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1275	GFLOPS: 8016.38 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.94, Tstamp:1669892392.40)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for rx.2 (0,3)
                  for nn_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1276	GFLOPS: 8025.96 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.13, Tstamp:1669892394.36)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for ff_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1277	GFLOPS: 8030.66 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.86, Tstamp:1669892396.06)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,2)
              for rx.2 (0,3)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1278	GFLOPS: 2541.92 / 8625.46	results: MeasureResult(cost:[0.0007], error_no:0, all_cost:2.81, Tstamp:1669892397.80)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1808)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,226)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        for rx.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,16)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
              pad_temp.shared = ...
          for ry.1 (0,3)
            for yy_c.4 (0,2)
              conv2d_nchw.local = ...
      for yy.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1279	GFLOPS: 8071.29 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.90, Tstamp:1669892399.52)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for ff_c.3 (0,4)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1280	GFLOPS: 8028.58 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.02, Tstamp:1669892401.35)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

Time elapsed for measurement: 128.01 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 1.14 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Iter: 5	#Pop: 36	#Target: 50	fail_ct: 10204	Time elapsed: 4.43
Sample Initial Population	#s: 54	fail_ct: 14282	Time elapsed: 6.28
GA Iter: 0	Max score: 0.6645	Min score: -0.0674	#Pop: 54	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9785	Min score: 0.9098	#Pop: 128	#M+: 1393	#M-: 0
EvolutionarySearch		#s: 128	Time elapsed: 15.51
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************==================================================
No: 1281	GFLOPS: 7028.88 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669892439.16)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for yy_c.3 (0,2)
        for rc.2 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1282	GFLOPS: 7042.67 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669892440.83)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for yy_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1283	GFLOPS: 4880.35 / 7119.92	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:3.92, Tstamp:1669892442.60)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for nn_c.3 (0,2)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for nn.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1284	GFLOPS: 7014.98 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.33, Tstamp:1669892444.45)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1285	GFLOPS: 6996.62 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669892446.12)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for yy_c.3 (0,2)
          for rc.2 (0,2)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1286	GFLOPS: 6984.79 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.96, Tstamp:1669892447.79)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for xx_c.3 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1287	GFLOPS: 6741.99 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.09, Tstamp:1669892449.58)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for yy_c.3 (0,2)
          for rc.2 (0,2)
            for ry.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1288	GFLOPS: 6933.73 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.22, Tstamp:1669892451.42)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for rc.2 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1289	GFLOPS: 6996.40 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669892453.08)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for rc.2 (0,2)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1290	GFLOPS: 6969.86 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669892454.75)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for rc.2 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1291	GFLOPS: 6779.24 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.10, Tstamp:1669892456.57)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for yy_c.3 (0,2)
          for rc.2 (0,2)
            for ry.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1292	GFLOPS: 6954.24 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.28, Tstamp:1669892458.46)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1293	GFLOPS: 6972.46 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.08, Tstamp:1669892460.14)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for rc.2 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1294	GFLOPS: 6781.06 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.77, Tstamp:1669892461.80)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,15)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1295	GFLOPS: 6767.37 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.79, Tstamp:1669892463.56)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,15)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1296	GFLOPS: 6887.77 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.61, Tstamp:1669892465.44)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for yy_c.3 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1297	GFLOPS: 6775.34 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.83, Tstamp:1669892467.08)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.2 (0,5)
        for rx.2 (0,5)
          for nn_c.4 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1298	GFLOPS: 6916.02 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.39, Tstamp:1669892468.75)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for yy_c.4 (0,2)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1299	GFLOPS: 6916.57 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.44, Tstamp:1669892470.55)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.2 (0,5)
        for rx.2 (0,5)
          for yy_c.4 (0,2)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1300	GFLOPS: 6164.75 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.08, Tstamp:1669892472.39)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,20)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1301	GFLOPS: 4575.47 / 7119.92	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:3.91, Tstamp:1669892474.03)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.2 (0,5)
            for nn_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for nn.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1302	GFLOPS: 6783.82 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.94, Tstamp:1669892475.71)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for yy_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1303	GFLOPS: 6988.90 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.96, Tstamp:1669892477.51)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for xx_c.3 (0,2)
          for rc.2 (0,2)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1304	GFLOPS: 6149.59 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.96, Tstamp:1669892479.36)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for ry.2 (0,5)
          for nn_c.4 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1305	GFLOPS: 6750.20 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669892481.03)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1306	GFLOPS: 6744.94 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669892482.71)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for xx_c.3 (0,2)
          for rc.2 (0,2)
            for ry.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1307	GFLOPS: 6968.76 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.86, Tstamp:1669892484.51)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1308	GFLOPS: 6720.42 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.13, Tstamp:1669892486.36)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for yy_c.3 (0,2)
        for rc.2 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1309	GFLOPS: 7052.65 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.95, Tstamp:1669892488.02)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for xx_c.3 (0,2)
        for rc.2 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1310	GFLOPS: 6698.06 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.88, Tstamp:1669892489.69)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1311	GFLOPS: 6716.55 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.88, Tstamp:1669892491.46)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1312	GFLOPS: 7051.77 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669892493.33)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1313	GFLOPS: 7042.57 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.84, Tstamp:1669892495.00)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.2 (0,5)
        for rx.2 (0,5)
          for nn_c.4 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1314	GFLOPS: 7023.06 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.87, Tstamp:1669892496.69)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for nn_c.4 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1315	GFLOPS: 5743.66 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669892498.53)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.2 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for xx_c.4 (0,4)
              conv2d_nchw.local = ...
    for xx.3 (0,4)
      conv2d_nchw = ...

==================================================
No: 1316	GFLOPS: 6738.42 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.07, Tstamp:1669892500.40)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1317	GFLOPS: 6727.02 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.88, Tstamp:1669892502.06)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for yy_c.3 (0,2)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1318	GFLOPS: 6695.06 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.86, Tstamp:1669892503.72)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rc.2 (0,2)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1319	GFLOPS: 6941.22 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.94, Tstamp:1669892505.60)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for xx_c.3 (0,2)
            for ry.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1320	GFLOPS: 6961.95 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.07, Tstamp:1669892507.47)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for xx_c.3 (0,2)
          for rc.2 (0,2)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1321	GFLOPS: 6987.52 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.92, Tstamp:1669892509.16)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for xx_c.3 (0,2)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1322	GFLOPS: 6974.03 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.97, Tstamp:1669892510.87)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for xx_c.3 (0,2)
          for rc.2 (0,2)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1323	GFLOPS: 7022.39 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.95, Tstamp:1669892512.63)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for xx_c.3 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1324	GFLOPS: 6818.16 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.63, Tstamp:1669892514.52)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for yy_c.3 (0,2)
          for ry.2 (0,5)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1325	GFLOPS: 6614.72 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.43, Tstamp:1669892516.19)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for yy_c.3 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1326	GFLOPS: 6741.84 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.97, Tstamp:1669892517.85)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for rc.2 (0,2)
              for ry.2 (0,5)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1327	GFLOPS: 6750.46 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.76, Tstamp:1669892519.75)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for rc.2 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1328	GFLOPS: 6612.34 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.56, Tstamp:1669892521.60)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for yy_c.3 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1329	GFLOPS: 6666.63 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.36, Tstamp:1669892523.24)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for yy_c.3 (0,2)
          for rx.2 (0,5)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1330	GFLOPS: 6677.94 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.39, Tstamp:1669892524.91)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.2 (0,5)
        for rx.2 (0,5)
          for yy_c.4 (0,2)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1331	GFLOPS: 6661.96 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.40, Tstamp:1669892526.83)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.2 (0,5)
          for yy_c.4 (0,2)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1332	GFLOPS: 6671.99 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.51, Tstamp:1669892528.63)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for yy_c.4 (0,2)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1333	GFLOPS: 6589.82 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.73, Tstamp:1669892530.29)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1334	GFLOPS: 6534.71 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.59, Tstamp:1669892531.95)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for yy_c.3 (0,2)
          for rc.2 (0,2)
            for ry.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1335	GFLOPS: 6672.86 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.35, Tstamp:1669892533.76)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1336	GFLOPS: 6670.89 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.49, Tstamp:1669892535.58)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for yy_c.4 (0,2)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1337	GFLOPS: 6739.13 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.07, Tstamp:1669892537.25)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for rc.2 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1338	GFLOPS: 6782.73 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.03, Tstamp:1669892538.93)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1339	GFLOPS: 6739.39 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.98, Tstamp:1669892540.73)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for rc.2 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1340	GFLOPS: 6575.42 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.11, Tstamp:1669892542.55)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for ry.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1341	GFLOPS: 6515.52 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.85, Tstamp:1669892544.20)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for rc.2 (0,2)
          for ry.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1342	GFLOPS: 506.61 / 7119.92	results: MeasureResult(cost:[0.0025], error_no:0, all_cost:2.37, Tstamp:1669892545.84)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,6272)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
      for rc.0 (0,4)
        for ax0@ax1@ax2@ax3@.0.0 (0,7)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,144)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
            pad_temp.shared = ...
        for rc.1 (0,4)
          for rx.1 (0,5)
            for rc.2 (0,2)
              for ry.2 (0,5)
                conv2d_nchw.local = ...
      conv2d_nchw = ...

==================================================
No: 1343	GFLOPS: 909.81 / 7119.92	results: MeasureResult(cost:[0.0014], error_no:0, all_cost:2.47, Tstamp:1669892547.63)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,448)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,112)
      conv2d_nchw.local auto_unroll: 16
      for rc.0 (0,32)
        for ry.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,112)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,19)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,112)
              pad_temp.shared = ...
          for rx.1 (0,5)
            for nn_c.3 (0,2)
              for xx_c.3 (0,4)
                conv2d_nchw.local = ...
      for nn.3 (0,2)
        for xx.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 1344	GFLOPS: 6118.29 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.98, Tstamp:1669892549.46)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for yy_c.3 (0,2)
            for rc.2 (0,2)
              for rx.2 (0,5)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

Time elapsed for measurement: 125.10 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 1.23 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 74	fail_ct: 4022	Time elapsed: 2.15
GA Iter: 0	Max score: 0.7421	Min score: 0.0529	#Pop: 74	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9930	Min score: 0.9302	#Pop: 128	#M+: 1397	#M-: 0
EvolutionarySearch		#s: 128	Time elapsed: 15.05
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************==================================================
No: 1345	GFLOPS: 8581.90 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.98, Tstamp:1669892582.30)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for nn_c.3 (0,2)
        for ff_c.3 (0,4)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1346	GFLOPS: 8498.90 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669892584.00)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,3)
        for ff_c.3 (0,4)
          for yy_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1347	GFLOPS: 8373.61 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.31, Tstamp:1669892586.00)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1348	GFLOPS: 8327.64 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.29, Tstamp:1669892587.88)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,4)
              for yy_c.3 (0,2)
                for rx.2 (0,3)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1349	GFLOPS: 8348.59 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669892589.58)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,4)
              for yy_c.3 (0,2)
                for rx.2 (0,3)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1350	GFLOPS: 8343.30 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669892591.29)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1351	GFLOPS: 8376.49 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.17, Tstamp:1669892593.12)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1352	GFLOPS: 8378.57 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.20, Tstamp:1669892595.00)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for rx.2 (0,3)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1353	GFLOPS: 8385.15 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669892596.71)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ry.2 (0,3)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1354	GFLOPS: 6839.02 / 8625.46	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.26, Tstamp:1669892598.90)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for rx.1 (0,3)
        for ff_c.3 (0,4)
          for ry.2 (0,3)
            for nn_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1355	GFLOPS: 8018.19 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.63, Tstamp:1669892600.62)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for nn_c.3 (0,4)
        for yy_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1356	GFLOPS: 8015.57 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.84, Tstamp:1669892602.33)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for rx.2 (0,3)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1357	GFLOPS: 8018.38 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.06, Tstamp:1669892604.24)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for nn_c.3 (0,4)
        for ff_c.3 (0,4)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1358	GFLOPS: 8022.04 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.85, Tstamp:1669892605.95)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for rx.2 (0,3)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1359	GFLOPS: 8058.00 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.94, Tstamp:1669892607.65)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for nn_c.3 (0,2)
        for ff_c.3 (0,4)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1360	GFLOPS: 8058.75 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.88, Tstamp:1669892609.38)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for nn_c.3 (0,2)
        for ff_c.3 (0,4)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1361	GFLOPS: 8012.10 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.08, Tstamp:1669892611.27)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,4)
              for yy_c.3 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1362	GFLOPS: 8016.82 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.84, Tstamp:1669892612.98)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1363	GFLOPS: 8020.46 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.83, Tstamp:1669892614.67)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.4 (0,4)
            for ff_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1364	GFLOPS: 8030.20 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.78, Tstamp:1669892616.39)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.2 (0,3)
          for nn_c.4 (0,4)
            for ff_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1365	GFLOPS: 8015.68 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669892618.29)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for rx.1 (0,3)
        for ff_c.3 (0,2)
          for ry.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1366	GFLOPS: 8032.53 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.85, Tstamp:1669892619.98)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ff_c.3 (0,2)
        for ry.2 (0,3)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1367	GFLOPS: 8015.86 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.84, Tstamp:1669892621.69)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,2)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1368	GFLOPS: 8052.70 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.92, Tstamp:1669892623.40)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for ff_c.3 (0,2)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1369	GFLOPS: 8009.13 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669892625.28)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1370	GFLOPS: 8054.88 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.81, Tstamp:1669892626.98)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1371	GFLOPS: 8066.67 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.87, Tstamp:1669892628.68)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for ff_c.3 (0,2)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1372	GFLOPS: 8053.29 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.87, Tstamp:1669892630.47)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for rx.1 (0,3)
        for ry.2 (0,3)
          for nn_c.4 (0,4)
            for ff_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1373	GFLOPS: 8011.80 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669892632.35)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1374	GFLOPS: 8007.60 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.83, Tstamp:1669892634.05)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.4 (0,4)
            for ff_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1375	GFLOPS: 8008.62 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.78, Tstamp:1669892635.74)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.2 (0,3)
          for nn_c.4 (0,4)
            for ff_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1376	GFLOPS: 8028.44 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.83, Tstamp:1669892637.56)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.2 (0,3)
        for rx.2 (0,3)
          for nn_c.4 (0,4)
            for ff_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1377	GFLOPS: 8056.39 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669892639.44)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for ff_c.3 (0,2)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1378	GFLOPS: 8022.25 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.83, Tstamp:1669892641.14)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,4)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1379	GFLOPS: 8032.77 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.82, Tstamp:1669892642.85)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ff_c.3 (0,2)
        for ry.2 (0,3)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1380	GFLOPS: 8015.98 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.78, Tstamp:1669892644.56)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1381	GFLOPS: 8039.91 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.96, Tstamp:1669892646.44)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for rx.1 (0,3)
        for ff_c.3 (0,2)
          for ry.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1382	GFLOPS: 7999.99 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.82, Tstamp:1669892648.14)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for yy_c.3 (0,2)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1383	GFLOPS: 8033.70 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.82, Tstamp:1669892649.85)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for ff_c.3 (0,4)
            for yy_c.3 (0,2)
              for nn_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1384	GFLOPS: 8063.30 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.75, Tstamp:1669892651.60)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for ff_c.3 (0,4)
            for nn_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1385	GFLOPS: 8024.49 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.96, Tstamp:1669892653.48)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for yy_c.3 (0,2)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1386	GFLOPS: 8029.31 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.79, Tstamp:1669892655.20)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for yy_c.3 (0,2)
            for nn_c.4 (0,4)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1387	GFLOPS: 8036.50 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.80, Tstamp:1669892656.90)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for ff_c.3 (0,4)
            for yy_c.3 (0,2)
              for nn_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1388	GFLOPS: 8052.53 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.81, Tstamp:1669892658.62)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,4)
              for yy_c.3 (0,2)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1389	GFLOPS: 8065.63 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.91, Tstamp:1669892660.48)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for ff_c.3 (0,4)
          for yy_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1390	GFLOPS: 8052.14 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.83, Tstamp:1669892662.20)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ff_c.3 (0,2)
        for yy_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1391	GFLOPS: 8011.05 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.78, Tstamp:1669892663.92)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for ff_c.3 (0,2)
          for yy_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1392	GFLOPS: 8040.64 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.73, Tstamp:1669892665.62)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for ff_c.3 (0,2)
          for yy_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1393	GFLOPS: 8063.85 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.96, Tstamp:1669892667.49)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for rx.1 (0,3)
        for ff_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1394	GFLOPS: 8053.19 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.72, Tstamp:1669892669.19)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for rx.1 (0,3)
        for ff_c.3 (0,4)
          for ry.2 (0,3)
            for nn_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1395	GFLOPS: 8077.76 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.78, Tstamp:1669892670.89)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ff_c.3 (0,4)
        for ry.2 (0,3)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1396	GFLOPS: 8075.63 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.76, Tstamp:1669892672.69)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ff_c.3 (0,4)
        for ry.2 (0,3)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1397	GFLOPS: 8053.67 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.88, Tstamp:1669892674.58)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ff_c.3 (0,4)
        for yy_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1398	GFLOPS: 7997.10 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.67, Tstamp:1669892676.26)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,4)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1399	GFLOPS: 8053.74 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.74, Tstamp:1669892677.95)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for nn_c.3 (0,2)
        for yy_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1400	GFLOPS: 8023.04 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.76, Tstamp:1669892679.72)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for rx.1 (0,3)
        for yy_c.3 (0,2)
          for ry.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1401	GFLOPS: 8001.98 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.22, Tstamp:1669892681.60)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,4)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1402	GFLOPS: 7995.39 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.82, Tstamp:1669892683.32)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,4)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1403	GFLOPS: 8061.04 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.97, Tstamp:1669892685.03)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,4)
          for ff_c.3 (0,4)
            for ry.2 (0,3)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1404	GFLOPS: 8063.43 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.87, Tstamp:1669892686.81)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1405	GFLOPS: 8037.93 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.03, Tstamp:1669892688.68)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for nn_c.3 (0,2)
        for ff_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1406	GFLOPS: 1906.03 / 8625.46	results: MeasureResult(cost:[0.0010], error_no:0, all_cost:3.64, Tstamp:1669892690.88)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1808)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,226)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,8)
        for rx.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,17)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
              pad_temp.shared = ...
          for ry.1 (0,3)
            for rc.2 (0,4)
              for nn_c.4 (0,2)
                conv2d_nchw.local = ...
      for nn.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1407	GFLOPS: 8057.71 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.82, Tstamp:1669892692.59)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for rx.2 (0,3)
                  for nn_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1408	GFLOPS: 8040.63 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.80, Tstamp:1669892694.29)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for rx.1 (0,3)
        for ff_c.3 (0,4)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

Time elapsed for measurement: 126.40 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 1.32 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Iter: 5	#Pop: 38	#Target: 50	fail_ct: 10202	Time elapsed: 4.15
Sample Initial Population	#s: 56	fail_ct: 14280	Time elapsed: 5.86
GA Iter: 0	Max score: 0.5659	Min score: -0.2014	#Pop: 56	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9836	Min score: 0.9154	#Pop: 128	#M+: 1387	#M-: 0
EvolutionarySearch		#s: 128	Time elapsed: 14.86
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************==================================================
No: 1409	GFLOPS: 7053.71 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.84, Tstamp:1669892733.40)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for nn_c.4 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1410	GFLOPS: 7055.00 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.91, Tstamp:1669892735.10)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1411	GFLOPS: 7050.21 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.84, Tstamp:1669892736.74)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for nn_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1412	GFLOPS: 7049.61 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.91, Tstamp:1669892738.41)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for nn_c.3 (0,2)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1413	GFLOPS: 7035.10 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669892740.23)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.2 (0,5)
          for nn_c.4 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1414	GFLOPS: 7031.33 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.91, Tstamp:1669892741.92)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for xx_c.3 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1415	GFLOPS: 6995.36 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.06, Tstamp:1669892743.58)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for xx_c.3 (0,2)
        for rc.2 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1416	GFLOPS: 6962.67 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.06, Tstamp:1669892745.25)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for xx_c.3 (0,2)
            for rc.2 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1417	GFLOPS: 6759.33 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.21, Tstamp:1669892747.07)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for xx_c.3 (0,2)
          for rc.2 (0,2)
            for ry.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1418	GFLOPS: 7074.94 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669892748.74)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for xx_c.3 (0,2)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1419	GFLOPS: 7009.93 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669892750.40)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for xx_c.3 (0,2)
        for rc.2 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1420	GFLOPS: 6995.83 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.12, Tstamp:1669892752.09)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for xx_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1421	GFLOPS: 7024.37 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669892753.76)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for xx_c.3 (0,2)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1422	GFLOPS: 7049.73 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.10, Tstamp:1669892755.44)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for xx_c.3 (0,2)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1423	GFLOPS: 6961.48 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.07, Tstamp:1669892757.18)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for xx_c.3 (0,2)
            for ry.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1424	GFLOPS: 6738.29 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.00, Tstamp:1669892758.86)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for xx_c.3 (0,2)
          for rc.2 (0,2)
            for ry.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1425	GFLOPS: 6930.64 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.88, Tstamp:1669892760.53)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1426	GFLOPS: 6891.99 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.81, Tstamp:1669892762.19)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for nn_c.3 (0,2)
          for ry.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1427	GFLOPS: 6785.79 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.21, Tstamp:1669892764.01)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for yy_c.3 (0,2)
          for rc.2 (0,2)
            for ry.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1428	GFLOPS: 6990.37 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.16, Tstamp:1669892765.70)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for rc.2 (0,2)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1429	GFLOPS: 7014.76 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.99, Tstamp:1669892767.37)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1430	GFLOPS: 6888.78 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.96, Tstamp:1669892769.05)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1431	GFLOPS: 7070.40 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.20, Tstamp:1669892770.91)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1432	GFLOPS: 7048.67 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669892772.60)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1433	GFLOPS: 7032.86 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.98, Tstamp:1669892774.24)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for xx_c.3 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1434	GFLOPS: 6776.22 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.68, Tstamp:1669892775.92)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,15)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1435	GFLOPS: 4597.40 / 7119.92	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.05, Tstamp:1669892777.71)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for ry.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for nn.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1436	GFLOPS: 7061.86 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669892779.37)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for xx_c.3 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1437	GFLOPS: 6896.02 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.53, Tstamp:1669892781.07)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1438	GFLOPS: 6738.34 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.29, Tstamp:1669892782.93)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for rc.2 (0,2)
              for ry.2 (0,5)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1439	GFLOPS: 6959.99 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.19, Tstamp:1669892784.78)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              for ry.2 (0,5)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1440	GFLOPS: 5650.42 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.28, Tstamp:1669892786.44)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.2 (0,5)
        for rx.2 (0,5)
          for yy_c.4 (0,2)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1441	GFLOPS: 5662.77 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.28, Tstamp:1669892788.12)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for yy_c.3 (0,2)
          for rx.2 (0,5)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1442	GFLOPS: 6755.08 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.88, Tstamp:1669892789.94)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for xx_c.3 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1443	GFLOPS: 6759.47 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.92, Tstamp:1669892791.76)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for nn_c.3 (0,2)
          for ry.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1444	GFLOPS: 6796.09 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.74, Tstamp:1669892793.41)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,15)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1445	GFLOPS: 7083.95 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.82, Tstamp:1669892795.08)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for nn_c.3 (0,2)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1446	GFLOPS: 7055.89 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.80, Tstamp:1669892796.91)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for xx_c.3 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1447	GFLOPS: 6985.16 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.98, Tstamp:1669892798.74)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for nn_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1448	GFLOPS: 7086.44 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.87, Tstamp:1669892800.41)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for nn_c.4 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1449	GFLOPS: 7089.06 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.84, Tstamp:1669892802.09)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1450	GFLOPS: 7064.71 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.75, Tstamp:1669892803.88)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1451	GFLOPS: 7114.48 / 7119.92	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.97, Tstamp:1669892805.72)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for nn_c.3 (0,2)
        for yy_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1452	GFLOPS: 7138.10 / 7138.10	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.80, Tstamp:1669892807.41)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for xx_c.3 (0,2)
          for ry.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,4)
        conv2d_nchw = ...

==================================================
No: 1453	GFLOPS: 6975.27 / 7138.10	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.85, Tstamp:1669892809.10)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1454	GFLOPS: 6797.49 / 7138.10	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.74, Tstamp:1669892810.97)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,15)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1455	GFLOPS: 6735.91 / 7138.10	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.94, Tstamp:1669892812.78)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,15)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for yy_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1456	GFLOPS: 6101.71 / 7138.10	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.78, Tstamp:1669892814.43)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for yy_c.3 (0,4)
          for ry.2 (0,5)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,4)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1457	GFLOPS: 6701.00 / 7138.10	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.72, Tstamp:1669892816.08)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rc.2 (0,2)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1458	GFLOPS: 6763.01 / 7138.10	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.73, Tstamp:1669892817.72)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for rc.2 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1459	GFLOPS: 6699.13 / 7138.10	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.84, Tstamp:1669892819.39)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for yy_c.3 (0,2)
        for rc.2 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1460	GFLOPS: 6691.44 / 7138.10	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.73, Tstamp:1669892821.05)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for yy_c.3 (0,2)
          for rc.2 (0,2)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1461	GFLOPS: 5569.01 / 7138.10	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.17, Tstamp:1669892822.71)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for yy_c.4 (0,2)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1462	GFLOPS: 6793.42 / 7138.10	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669892824.50)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for rc.2 (0,2)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1463	GFLOPS: 6157.91 / 7138.10	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.61, Tstamp:1669892826.15)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,15)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for ry.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1464	GFLOPS: 7082.92 / 7138.10	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.74, Tstamp:1669892827.83)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1465	GFLOPS: 7071.03 / 7138.10	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.06, Tstamp:1669892829.51)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.2 (0,5)
        for rx.2 (0,5)
          for nn_c.4 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1466	GFLOPS: 7134.63 / 7138.10	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.21, Tstamp:1669892831.38)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for yy_c.3 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for nn_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1467	GFLOPS: 7066.11 / 7138.10	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.01, Tstamp:1669892833.05)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for nn_c.4 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1468	GFLOPS: 6987.46 / 7138.10	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.96, Tstamp:1669892834.73)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for yy_c.3 (0,2)
          for rx.2 (0,5)
            for nn_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1469	GFLOPS: 6714.47 / 7138.10	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.13, Tstamp:1669892836.44)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1470	GFLOPS: 4405.93 / 7138.10	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:6.68, Tstamp:1669892838.72)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1792)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,56)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,4)
        for ax0@ax1@ax2@ax3@.0.0 (0,4)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,103)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
            pad_temp.shared = ...
        for rc.1 (0,4)
          for rx.1 (0,5)
            for rc.2 (0,2)
              for ry.2 (0,5)
                for yy_c.4 (0,2)
                  for xx_c.4 (0,2)
                    conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1471	GFLOPS: 6779.01 / 7138.10	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.10, Tstamp:1669892840.57)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for yy_c.3 (0,2)
        for xx_c.3 (0,2)
          for rc.2 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1472	GFLOPS: 6715.74 / 7138.10	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.92, Tstamp:1669892842.23)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

Time elapsed for measurement: 125.66 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 1.32 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 69	fail_ct: 4027	Time elapsed: 2.13
GA Iter: 0	Max score: 0.6293	Min score: -0.0348	#Pop: 69	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9770	Min score: 0.8920	#Pop: 128	#M+: 1396	#M-: 0
EvolutionarySearch		#s: 128	Time elapsed: 14.90
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************==================================================
No: 1473	GFLOPS: 8440.02 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669892875.45)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ff_c.3 (0,4)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1474	GFLOPS: 8343.05 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669892877.14)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ff_c.3 (0,4)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1475	GFLOPS: 8367.85 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.09, Tstamp:1669892878.86)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1476	GFLOPS: 8362.46 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.13, Tstamp:1669892880.67)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,4)
              for rx.2 (0,3)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1477	GFLOPS: 8354.97 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.19, Tstamp:1669892882.40)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for ff_c.3 (0,4)
            for ry.2 (0,3)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1478	GFLOPS: 8386.79 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.19, Tstamp:1669892884.27)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for ff_c.3 (0,4)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1479	GFLOPS: 8367.23 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.25, Tstamp:1669892886.13)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1480	GFLOPS: 8503.08 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.89, Tstamp:1669892887.85)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1481	GFLOPS: 8014.18 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.85, Tstamp:1669892889.55)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for nn_c.3 (0,2)
        for ff_c.3 (0,4)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1482	GFLOPS: 7978.63 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.82, Tstamp:1669892891.25)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1483	GFLOPS: 8011.58 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669892893.11)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,4)
          for ff_c.3 (0,4)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1484	GFLOPS: 7997.09 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.83, Tstamp:1669892894.81)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,4)
          for ff_c.3 (0,4)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1485	GFLOPS: 8037.70 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.86, Tstamp:1669892896.52)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1486	GFLOPS: 8004.65 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.82, Tstamp:1669892898.25)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for nn_c.3 (0,4)
        for ff_c.3 (0,4)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1487	GFLOPS: 7975.46 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669892900.13)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1488	GFLOPS: 8025.89 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.83, Tstamp:1669892901.82)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rx.1 (0,3)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for nn_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1489	GFLOPS: 8046.16 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.85, Tstamp:1669892903.52)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,2)
          for ff_c.3 (0,4)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1490	GFLOPS: 8050.30 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.82, Tstamp:1669892905.32)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,2)
          for ff_c.3 (0,4)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1491	GFLOPS: 8054.05 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669892907.21)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for ff_c.3 (0,4)
          for yy_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1492	GFLOPS: 8025.36 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.83, Tstamp:1669892908.92)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,4)
              for yy_c.3 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1493	GFLOPS: 8052.48 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.88, Tstamp:1669892910.62)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for ff_c.3 (0,2)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1494	GFLOPS: 8063.93 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.88, Tstamp:1669892912.43)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for nn_c.3 (0,2)
        for ry.2 (0,3)
          for rx.2 (0,3)
            for nn_c.4 (0,2)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1495	GFLOPS: 8050.13 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669892914.32)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1496	GFLOPS: 8016.91 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.93, Tstamp:1669892916.02)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,2)
          for rx.2 (0,3)
            for nn_c.4 (0,2)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1497	GFLOPS: 8007.24 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.82, Tstamp:1669892917.74)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1498	GFLOPS: 8021.12 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.83, Tstamp:1669892919.46)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.2 (0,3)
        for rx.2 (0,3)
          for nn_c.4 (0,4)
            for ff_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1499	GFLOPS: 8060.67 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.96, Tstamp:1669892921.35)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for ff_c.3 (0,4)
            for nn_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1500	GFLOPS: 8024.00 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.83, Tstamp:1669892923.04)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for rx.1 (0,3)
        for ry.2 (0,3)
          for nn_c.4 (0,4)
            for ff_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1501	GFLOPS: 8017.60 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.83, Tstamp:1669892924.75)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,4)
          for ff_c.3 (0,4)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1502	GFLOPS: 8025.86 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.92, Tstamp:1669892926.53)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for rx.1 (0,3)
        for yy_c.3 (0,2)
          for ry.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1503	GFLOPS: 8041.79 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669892928.41)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for yy_c.3 (0,2)
        for ry.2 (0,3)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1504	GFLOPS: 8063.78 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.83, Tstamp:1669892930.11)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for yy_c.3 (0,2)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1505	GFLOPS: 8008.65 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.89, Tstamp:1669892931.80)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1506	GFLOPS: 8024.00 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.84, Tstamp:1669892933.60)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for ff_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1507	GFLOPS: 8060.45 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.99, Tstamp:1669892935.48)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,4)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1508	GFLOPS: 8017.78 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.81, Tstamp:1669892937.18)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,4)
          for ff_c.3 (0,2)
            for rx.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1509	GFLOPS: 8054.28 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.85, Tstamp:1669892938.90)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ff_c.3 (0,2)
        for yy_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1510	GFLOPS: 8002.83 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.83, Tstamp:1669892940.66)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,4)
          for rx.2 (0,3)
            for ff_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1511	GFLOPS: 7964.50 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.96, Tstamp:1669892942.56)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for nn_c.3 (0,4)
        for ff_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1512	GFLOPS: 8062.35 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.73, Tstamp:1669892944.25)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for rx.1 (0,3)
        for ff_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1513	GFLOPS: 7963.81 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.81, Tstamp:1669892945.94)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for nn_c.3 (0,4)
        for ff_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1514	GFLOPS: 8019.01 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.76, Tstamp:1669892947.75)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,4)
              for yy_c.3 (0,2)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1515	GFLOPS: 8052.38 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.89, Tstamp:1669892949.62)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,2)
          for ff_c.3 (0,4)
            for ry.2 (0,3)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1516	GFLOPS: 8054.44 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.79, Tstamp:1669892951.31)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,2)
          for ff_c.3 (0,4)
            for ry.2 (0,3)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1517	GFLOPS: 8055.62 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.72, Tstamp:1669892953.01)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ff_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1518	GFLOPS: 8028.53 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.79, Tstamp:1669892954.87)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1519	GFLOPS: 8015.73 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.93, Tstamp:1669892956.78)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1520	GFLOPS: 8061.30 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.70, Tstamp:1669892958.50)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1521	GFLOPS: 8011.66 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.72, Tstamp:1669892960.24)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1522	GFLOPS: 8009.49 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.77, Tstamp:1669892961.94)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1523	GFLOPS: 8035.81 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.95, Tstamp:1669892963.84)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,2)
          for ff_c.3 (0,4)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1524	GFLOPS: 8032.84 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.74, Tstamp:1669892965.53)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,2)
              for rx.2 (0,3)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1525	GFLOPS: 8064.69 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.78, Tstamp:1669892967.24)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,2)
          for ff_c.3 (0,4)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1526	GFLOPS: 8061.43 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.76, Tstamp:1669892968.93)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,2)
          for ff_c.3 (0,4)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1527	GFLOPS: 8027.14 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669892970.84)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rx.1 (0,3)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1528	GFLOPS: 8034.98 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.72, Tstamp:1669892972.53)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,2)
              for rx.2 (0,3)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1529	GFLOPS: 8049.42 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.87, Tstamp:1669892974.25)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rx.1 (0,3)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for nn_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1530	GFLOPS: 8032.18 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.86, Tstamp:1669892975.95)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for ff_c.3 (0,2)
                for yy_c.3 (0,2)
                  for nn_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1531	GFLOPS: 8062.74 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.11, Tstamp:1669892977.87)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1532	GFLOPS: 8038.94 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.86, Tstamp:1669892979.56)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for nn_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1533	GFLOPS: 8042.86 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.79, Tstamp:1669892981.26)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for ff_c.3 (0,4)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1534	GFLOPS: 1000.04 / 8625.46	results: MeasureResult(cost:[0.0019], error_no:0, all_cost:4.00, Tstamp:1669892983.47)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        for ry.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,33)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
              pad_temp.shared = ...
          for nn_c.3 (0,4)
            for rx.2 (0,3)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for nn.3 (0,4)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1535	GFLOPS: 629.02 / 8625.46	results: MeasureResult(cost:[0.0030], error_no:0, all_cost:3.80, Tstamp:1669892985.27)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,452)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        for rx.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,96)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
              pad_temp.shared = ...
          for rc.1 (0,2)
            for ry.1 (0,3)
              for nn_c.4 (0,8)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,8)
        for ff.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1536	GFLOPS: 8020.50 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.04, Tstamp:1669892987.18)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for ff_c.3 (0,2)
                for yy_c.3 (0,2)
                  for nn_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

Time elapsed for measurement: 126.48 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 1.53 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Iter: 5	#Pop: 25	#Target: 50	fail_ct: 10215	Time elapsed: 4.27
Sample Iter: 10	#Pop: 55	#Target: 50	fail_ct: 20425	Time elapsed: 8.28
Sample Initial Population	#s: 55	fail_ct: 20425	Time elapsed: 8.30
GA Iter: 0	Max score: 0.8489	Min score: -0.0343	#Pop: 55	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9880	Min score: 0.9163	#Pop: 128	#M+: 1390	#M-: 0
EvolutionarySearch		#s: 128	Time elapsed: 14.75
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************==================================================
No: 1537	GFLOPS: 7095.76 / 7138.10	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.89, Tstamp:1669893026.86)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.2 (0,5)
        for rx.2 (0,5)
          for nn_c.4 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1538	GFLOPS: 7035.69 / 7138.10	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.86, Tstamp:1669893028.54)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for yy_c.3 (0,2)
          for rx.2 (0,5)
            for nn_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1539	GFLOPS: 7034.37 / 7138.10	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.91, Tstamp:1669893030.19)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for nn_c.3 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1540	GFLOPS: 7038.86 / 7138.10	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.90, Tstamp:1669893031.86)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1541	GFLOPS: 7055.82 / 7138.10	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.89, Tstamp:1669893033.53)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for xx_c.3 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1542	GFLOPS: 7008.51 / 7138.10	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.92, Tstamp:1669893035.22)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.2 (0,5)
        for rx.2 (0,5)
          for nn_c.4 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1543	GFLOPS: 7065.44 / 7138.10	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669893036.98)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.2 (0,5)
          for nn_c.4 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1544	GFLOPS: 7055.08 / 7138.10	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.92, Tstamp:1669893038.65)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for nn_c.3 (0,2)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1545	GFLOPS: 7143.15 / 7143.15	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.90, Tstamp:1669893040.32)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1546	GFLOPS: 7048.67 / 7143.15	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.90, Tstamp:1669893041.99)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1547	GFLOPS: 7105.40 / 7143.15	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.16, Tstamp:1669893043.80)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for nn_c.3 (0,2)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1548	GFLOPS: 7030.14 / 7143.15	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.92, Tstamp:1669893045.48)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for nn_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1549	GFLOPS: 7091.64 / 7143.15	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.87, Tstamp:1669893047.15)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for nn_c.3 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1550	GFLOPS: 7063.74 / 7143.15	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.89, Tstamp:1669893048.81)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for yy_c.3 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for nn_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1551	GFLOPS: 7024.17 / 7143.15	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669893050.61)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for yy_c.3 (0,2)
          for rx.2 (0,5)
            for nn_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1552	GFLOPS: 7055.51 / 7143.15	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.91, Tstamp:1669893052.29)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for nn_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1553	GFLOPS: 7146.00 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.96, Tstamp:1669893053.95)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for nn_c.3 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1554	GFLOPS: 7045.19 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.89, Tstamp:1669893055.63)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1555	GFLOPS: 7019.23 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669893057.44)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1556	GFLOPS: 7043.00 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.87, Tstamp:1669893059.11)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for nn_c.3 (0,2)
        for yy_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1557	GFLOPS: 7097.57 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669893060.78)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for xx_c.3 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1558	GFLOPS: 7057.57 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669893062.46)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for xx_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1559	GFLOPS: 6989.86 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.26, Tstamp:1669893064.27)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for xx_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1560	GFLOPS: 7061.76 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.10, Tstamp:1669893065.95)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for xx_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1561	GFLOPS: 7091.70 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.99, Tstamp:1669893067.61)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for xx_c.3 (0,2)
        for rc.2 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1562	GFLOPS: 7070.26 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669893069.28)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for xx_c.3 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1563	GFLOPS: 5874.32 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.17, Tstamp:1669893071.14)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1564	GFLOPS: 5875.67 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.89, Tstamp:1669893072.79)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for nn_c.3 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1565	GFLOPS: 5885.96 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.92, Tstamp:1669893074.45)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.2 (0,5)
        for rx.2 (0,5)
          for nn_c.4 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1566	GFLOPS: 5881.60 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.00, Tstamp:1669893076.15)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.2 (0,5)
          for nn_c.4 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1567	GFLOPS: 7063.01 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.11, Tstamp:1669893078.02)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for xx_c.3 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1568	GFLOPS: 6994.09 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669893079.69)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for xx_c.3 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1569	GFLOPS: 6942.14 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.99, Tstamp:1669893081.35)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              for ry.2 (0,5)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1570	GFLOPS: 7003.66 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.84, Tstamp:1669893083.13)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1571	GFLOPS: 5873.74 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.12, Tstamp:1669893084.98)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for nn_c.3 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1572	GFLOPS: 5856.88 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.76, Tstamp:1669893086.57)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,5)
        for nn_c.3 (0,2)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1573	GFLOPS: 5858.44 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.94, Tstamp:1669893088.22)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1574	GFLOPS: 5873.28 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.81, Tstamp:1669893089.87)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,5)
        for nn_c.3 (0,2)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1575	GFLOPS: 7035.59 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.88, Tstamp:1669893091.53)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1576	GFLOPS: 6498.59 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.71, Tstamp:1669893093.18)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for yy_c.3 (0,2)
          for ry.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,4)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1577	GFLOPS: 7072.08 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669893094.85)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for yy_c.3 (0,2)
        for xx_c.3 (0,2)
          for rc.2 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1578	GFLOPS: 5893.12 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.98, Tstamp:1669893096.61)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for ry.1 (0,5)
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1579	GFLOPS: 6743.83 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.70, Tstamp:1669893098.27)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for nn_c.3 (0,2)
          for ry.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1580	GFLOPS: 6735.73 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.78, Tstamp:1669893099.93)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1581	GFLOPS: 6889.04 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.94, Tstamp:1669893101.60)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for yy_c.3 (0,2)
          for ry.2 (0,5)
            for nn_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1582	GFLOPS: 6471.83 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.12, Tstamp:1669893103.61)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for yy_c.3 (0,2)
          for ry.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,4)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1583	GFLOPS: 6981.47 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.96, Tstamp:1669893105.30)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for xx_c.3 (0,2)
            for rc.2 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1584	GFLOPS: 6936.58 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.83, Tstamp:1669893106.95)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for xx_c.3 (0,2)
            for ry.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1585	GFLOPS: 6941.83 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.93, Tstamp:1669893108.69)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for xx_c.3 (0,2)
            for ry.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1586	GFLOPS: 6797.32 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.29, Tstamp:1669893110.65)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for xx_c.3 (0,2)
          for rc.2 (0,2)
            for ry.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1587	GFLOPS: 6983.01 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.77, Tstamp:1669893112.32)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for xx_c.3 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1588	GFLOPS: 7086.12 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.80, Tstamp:1669893114.00)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1589	GFLOPS: 6482.20 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.87, Tstamp:1669893115.76)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,36)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
          pad_temp.shared = ...
      for nn_c.3 (0,4)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1590	GFLOPS: 7076.18 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669893117.63)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1591	GFLOPS: 5061.03 / 7146.00	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:3.73, Tstamp:1669893119.26)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for rx.1 (0,5)
        for yy_c.3 (0,2)
          for ry.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,4)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1592	GFLOPS: 1684.21 / 7146.00	results: MeasureResult(cost:[0.0008], error_no:0, all_cost:3.85, Tstamp:1669893121.35)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,16)
      for ax0@ax1@ax2@ax3@.0.0 (0,2)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,72)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for rc.2 (0,2)
              for rx.2 (0,5)
                for nn_c.4 (0,2)
                  for xx_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 1593	GFLOPS: 6904.20 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.96, Tstamp:1669893123.01)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for ry.2 (0,5)
          for nn_c.4 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1594	GFLOPS: 5411.60 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.90, Tstamp:1669893124.69)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            pad_temp.shared = ...
      for rx.1 (0,5)
        for nn_c.3 (0,2)
          for ry.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1595	GFLOPS: 6887.28 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.61, Tstamp:1669893126.40)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for yy_c.3 (0,2)
        for xx_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1596	GFLOPS: 6923.79 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.60, Tstamp:1669893128.06)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for rx.2 (0,5)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1597	GFLOPS: 5733.54 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.83, Tstamp:1669893129.71)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.2 (0,5)
          for nn_c.4 (0,2)
            for xx_c.4 (0,4)
              conv2d_nchw.local = ...
    for nn.3 (0,2)
      for xx.3 (0,4)
        conv2d_nchw = ...

==================================================
No: 1598	GFLOPS: 1322.98 / 7146.00	results: MeasureResult(cost:[0.0010], error_no:0, all_cost:4.35, Tstamp:1669893131.55)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,784)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        for ry.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,128)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
              pad_temp.shared = ...
          for rc.1 (0,2)
            for yy_c.3 (0,2)
              for rx.2 (0,5)
                for yy_c.4 (0,4)
                  for xx_c.4 (0,2)
                    conv2d_nchw.local = ...
      for yy.3 (0,8)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1599	GFLOPS: 6799.46 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.07, Tstamp:1669893133.21)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1600	GFLOPS: 6890.76 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.61, Tstamp:1669893134.91)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

Time elapsed for measurement: 123.20 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 1.66 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 82	fail_ct: 4014	Time elapsed: 2.49
GA Iter: 0	Max score: 0.6469	Min score: -0.0077	#Pop: 82	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9742	Min score: 0.9103	#Pop: 128	#M+: 1416	#M-: 0
EvolutionarySearch		#s: 128	Time elapsed: 15.79
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************==================================================
No: 1601	GFLOPS: 8411.33 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669893170.61)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for rx.2 (0,3)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1602	GFLOPS: 8336.27 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.21, Tstamp:1669893172.46)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ff_c.3 (0,4)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1603	GFLOPS: 8391.08 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.09, Tstamp:1669893174.18)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for ry.2 (0,3)
            for nn_c.4 (0,2)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1604	GFLOPS: 8373.47 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.15, Tstamp:1669893175.94)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1605	GFLOPS: 8396.05 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.20, Tstamp:1669893177.64)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for ry.2 (0,3)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1606	GFLOPS: 8363.98 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.94, Tstamp:1669893179.31)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for ff_c.3 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1607	GFLOPS: 8379.53 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669893181.03)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for yy_c.3 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1608	GFLOPS: 8407.38 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.17, Tstamp:1669893182.82)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for ry.2 (0,3)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1609	GFLOPS: 8362.22 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.13, Tstamp:1669893184.53)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1610	GFLOPS: 8012.21 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.62, Tstamp:1669893186.21)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for nn_c.3 (0,4)
        for ff_c.3 (0,4)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1611	GFLOPS: 7975.24 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.87, Tstamp:1669893187.91)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for ry.2 (0,3)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1612	GFLOPS: 7960.06 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669893189.74)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for ry.2 (0,3)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1613	GFLOPS: 8000.48 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.68, Tstamp:1669893191.44)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for rx.2 (0,3)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1614	GFLOPS: 8011.53 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669893193.34)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,4)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1615	GFLOPS: 6381.90 / 8625.46	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:4.33, Tstamp:1669893195.76)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rx.1 (0,3)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1616	GFLOPS: 8057.43 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669893197.65)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1617	GFLOPS: 8060.49 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.95, Tstamp:1669893199.37)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for nn_c.3 (0,2)
        for ry.2 (0,3)
          for rx.2 (0,3)
            for nn_c.4 (0,2)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1618	GFLOPS: 8512.75 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669893201.08)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ff_c.3 (0,4)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1619	GFLOPS: 8044.35 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.86, Tstamp:1669893202.81)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,2)
          for ff_c.3 (0,4)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1620	GFLOPS: 8035.83 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.12, Tstamp:1669893204.70)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1621	GFLOPS: 8542.65 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669893206.39)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for ff_c.3 (0,4)
              for yy_c.3 (0,2)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1622	GFLOPS: 8588.21 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.00, Tstamp:1669893208.10)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for ff_c.3 (0,4)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1623	GFLOPS: 8066.51 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.85, Tstamp:1669893209.80)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,4)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1624	GFLOPS: 8050.26 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.07, Tstamp:1669893211.72)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,4)
          for ry.2 (0,3)
            for ff_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1625	GFLOPS: 7999.83 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.82, Tstamp:1669893213.41)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,4)
          for rx.2 (0,3)
            for ff_c.4 (0,4)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1626	GFLOPS: 8011.42 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.81, Tstamp:1669893215.12)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,4)
          for ff_c.3 (0,4)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1627	GFLOPS: 8022.53 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.89, Tstamp:1669893216.81)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for rx.2 (0,3)
                  for nn_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1628	GFLOPS: 8023.69 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.06, Tstamp:1669893218.73)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for nn_c.4 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1629	GFLOPS: 8032.04 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.84, Tstamp:1669893220.43)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for yy_c.3 (0,2)
        for ry.2 (0,3)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1630	GFLOPS: 8043.58 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.77, Tstamp:1669893222.12)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for nn_c.4 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1631	GFLOPS: 8576.58 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.87, Tstamp:1669893223.83)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1632	GFLOPS: 8509.18 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.18, Tstamp:1669893225.72)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for ry.2 (0,3)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1633	GFLOPS: 8001.49 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.70, Tstamp:1669893227.42)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,4)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1634	GFLOPS: 8613.42 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.78, Tstamp:1669893229.12)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for ff_c.3 (0,4)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1635	GFLOPS: 8549.80 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.92, Tstamp:1669893230.85)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for ff_c.3 (0,4)
            for ry.2 (0,3)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1636	GFLOPS: 8030.64 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.06, Tstamp:1669893232.83)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for nn_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1637	GFLOPS: 8019.09 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.71, Tstamp:1669893234.55)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ff_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for nn_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1638	GFLOPS: 8051.70 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.83, Tstamp:1669893236.25)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for yy_c.3 (0,2)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1639	GFLOPS: 7998.90 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.82, Tstamp:1669893237.95)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1640	GFLOPS: 8549.72 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.18, Tstamp:1669893239.93)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for ff_c.3 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1641	GFLOPS: 8580.64 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.96, Tstamp:1669893241.64)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for yy_c.3 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1642	GFLOPS: 8054.94 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.80, Tstamp:1669893243.36)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for nn_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1643	GFLOPS: 8043.33 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.79, Tstamp:1669893245.06)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for nn_c.3 (0,2)
        for ff_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1644	GFLOPS: 8059.21 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.96, Tstamp:1669893246.93)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1645	GFLOPS: 8030.57 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.76, Tstamp:1669893248.62)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for nn_c.3 (0,2)
        for ff_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1646	GFLOPS: 8015.40 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.81, Tstamp:1669893250.46)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,4)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1647	GFLOPS: 8038.90 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.71, Tstamp:1669893252.15)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rx.1 (0,3)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1648	GFLOPS: 8037.25 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.69, Tstamp:1669893253.84)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for yy_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1649	GFLOPS: 8046.88 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.76, Tstamp:1669893255.54)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rx.1 (0,3)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1650	GFLOPS: 8046.67 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.87, Tstamp:1669893257.34)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rx.1 (0,3)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1651	GFLOPS: 7985.05 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.73, Tstamp:1669893259.04)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for yy_c.3 (0,2)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1652	GFLOPS: 8041.00 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.73, Tstamp:1669893260.70)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1653	GFLOPS: 8001.92 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.67, Tstamp:1669893262.40)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for ff_c.3 (0,2)
              for nn_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1654	GFLOPS: 8000.68 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.77, Tstamp:1669893264.23)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rx.1 (0,3)
          for ry.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1655	GFLOPS: 8006.77 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.63, Tstamp:1669893265.93)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1656	GFLOPS: 8014.60 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.75, Tstamp:1669893267.79)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.2 (0,3)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1657	GFLOPS: 8005.10 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.17, Tstamp:1669893269.70)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1658	GFLOPS: 8015.25 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.98, Tstamp:1669893271.40)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.2 (0,3)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1659	GFLOPS: 7976.16 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.99, Tstamp:1669893273.10)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1660	GFLOPS: 8001.24 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.94, Tstamp:1669893274.81)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1661	GFLOPS: 8008.79 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.16, Tstamp:1669893276.71)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,4)
              for ff_c.3 (0,2)
                for yy_c.3 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1662	GFLOPS: 598.35 / 8625.46	results: MeasureResult(cost:[0.0031], error_no:0, all_cost:4.61, Tstamp:1669893278.44)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,25538)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
      conv2d_nchw.local auto_unroll: 1024
      for rx.0 (0,3)
        for ax0@ax1@ax2@ax3@.0.0 (0,12)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,96)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
            pad_temp.shared = ...
        for nn_c.3 (0,2)
          for rc.2 (0,32)
            for ry.2 (0,3)
              conv2d_nchw.local = ...
      for nn.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1663	GFLOPS: 8009.94 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.83, Tstamp:1669893280.14)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1664	GFLOPS: 8031.09 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.83, Tstamp:1669893281.84)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for ff_c.3 (0,2)
              for nn_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

Time elapsed for measurement: 126.73 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 1.55 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Iter: 5	#Pop: 43	#Target: 50	fail_ct: 10197	Time elapsed: 4.27
Sample Initial Population	#s: 55	fail_ct: 14281	Time elapsed: 5.93
GA Iter: 0	Max score: 0.4466	Min score: -0.0243	#Pop: 55	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9866	Min score: 0.9177	#Pop: 128	#M+: 1387	#M-: 0
EvolutionarySearch		#s: 128	Time elapsed: 15.46
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************==================================================
No: 1665	GFLOPS: 7056.02 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.87, Tstamp:1669893318.96)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for nn_c.3 (0,2)
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1666	GFLOPS: 7039.85 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669893320.64)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for nn_c.3 (0,2)
        for yy_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1667	GFLOPS: 7034.94 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.92, Tstamp:1669893322.35)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for yy_c.3 (0,2)
          for rx.2 (0,5)
            for nn_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1668	GFLOPS: 7108.92 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.08, Tstamp:1669893324.21)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for nn_c.3 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1669	GFLOPS: 7061.74 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.87, Tstamp:1669893325.87)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.2 (0,5)
          for nn_c.4 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1670	GFLOPS: 7112.14 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.91, Tstamp:1669893327.55)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for nn_c.3 (0,2)
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1671	GFLOPS: 7033.54 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.87, Tstamp:1669893329.32)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1672	GFLOPS: 7101.00 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.24, Tstamp:1669893331.21)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for yy_c.3 (0,2)
        for xx_c.3 (0,2)
          for rc.2 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1673	GFLOPS: 7095.01 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669893332.88)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1674	GFLOPS: 7111.92 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.90, Tstamp:1669893334.56)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for nn_c.3 (0,2)
        for yy_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1675	GFLOPS: 7105.29 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.91, Tstamp:1669893336.31)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.2 (0,5)
          for nn_c.4 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1676	GFLOPS: 6994.09 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.27, Tstamp:1669893338.19)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for rc.2 (0,2)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1677	GFLOPS: 7099.67 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669893339.86)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1678	GFLOPS: 7036.04 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669893341.54)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for yy_c.3 (0,2)
        for xx_c.3 (0,2)
          for rc.2 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1679	GFLOPS: 6992.10 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.13, Tstamp:1669893343.31)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for xx_c.3 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1680	GFLOPS: 7097.68 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.07, Tstamp:1669893345.17)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1681	GFLOPS: 7028.18 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669893346.84)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for yy_c.3 (0,2)
        for xx_c.3 (0,2)
          for rc.2 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1682	GFLOPS: 6734.59 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669893348.52)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for rc.2 (0,2)
              for ry.2 (0,5)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1683	GFLOPS: 6132.07 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.94, Tstamp:1669893350.26)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for yy_c.3 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,4)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1684	GFLOPS: 6987.09 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.22, Tstamp:1669893352.11)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for xx_c.3 (0,2)
            for rc.2 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1685	GFLOPS: 6958.53 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669893353.79)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              for ry.2 (0,5)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1686	GFLOPS: 7120.31 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.89, Tstamp:1669893355.47)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1687	GFLOPS: 6912.01 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.88, Tstamp:1669893357.24)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for ry.2 (0,5)
          for nn_c.4 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1688	GFLOPS: 6939.27 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.24, Tstamp:1669893359.12)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              for ry.2 (0,5)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1689	GFLOPS: 7026.30 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.90, Tstamp:1669893360.80)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for xx_c.3 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1690	GFLOPS: 6798.55 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.84, Tstamp:1669893362.46)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for ry.2 (0,5)
          for nn_c.4 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1691	GFLOPS: 6125.68 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.85, Tstamp:1669893364.19)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for yy_c.3 (0,4)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,4)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1692	GFLOPS: 6881.11 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.64, Tstamp:1669893366.07)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for yy_c.3 (0,2)
        for xx_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1693	GFLOPS: 6619.65 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.43, Tstamp:1669893367.74)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for yy_c.3 (0,2)
        for xx_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1694	GFLOPS: 6788.06 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.88, Tstamp:1669893369.40)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for ry.2 (0,5)
          for nn_c.4 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1695	GFLOPS: 6661.86 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.45, Tstamp:1669893371.15)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for rx.2 (0,5)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1696	GFLOPS: 6890.10 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669893372.99)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for nn_c.3 (0,2)
          for ry.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1697	GFLOPS: 6790.23 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.99, Tstamp:1669893374.66)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1698	GFLOPS: 6169.19 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.85, Tstamp:1669893376.32)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,4)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1699	GFLOPS: 6165.89 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.81, Tstamp:1669893378.11)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for yy_c.3 (0,2)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,4)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1700	GFLOPS: 6677.30 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.60, Tstamp:1669893379.98)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for rx.2 (0,5)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1701	GFLOPS: 5617.73 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.50, Tstamp:1669893381.64)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,36)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
          pad_temp.shared = ...
      for nn_c.3 (0,4)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1702	GFLOPS: 6892.97 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.65, Tstamp:1669893383.29)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,36)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for nn_c.3 (0,4)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1703	GFLOPS: 6785.88 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.71, Tstamp:1669893385.08)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,15)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1704	GFLOPS: 6909.11 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.96, Tstamp:1669893386.94)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for yy_c.3 (0,2)
          for ry.2 (0,5)
            for nn_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1705	GFLOPS: 6166.28 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.79, Tstamp:1669893388.58)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for yy_c.3 (0,2)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,4)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1706	GFLOPS: 6146.54 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.72, Tstamp:1669893390.22)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,4)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1707	GFLOPS: 6756.86 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.92, Tstamp:1669893392.04)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for xx_c.3 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1708	GFLOPS: 7091.05 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.72, Tstamp:1669893393.90)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,36)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for rx.2 (0,5)
              for nn_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 1709	GFLOPS: 6753.77 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.72, Tstamp:1669893395.56)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for yy_c.3 (0,2)
          for ry.2 (0,5)
            for nn_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1710	GFLOPS: 6719.73 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.75, Tstamp:1669893397.23)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.2 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1711	GFLOPS: 6674.54 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.46, Tstamp:1669893399.01)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for ry.2 (0,5)
          for yy_c.4 (0,2)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1712	GFLOPS: 6735.49 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.99, Tstamp:1669893400.88)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1713	GFLOPS: 6787.60 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.66, Tstamp:1669893402.53)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for nn_c.3 (0,2)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1714	GFLOPS: 6666.48 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.37, Tstamp:1669893404.19)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for yy_c.3 (0,2)
          for ry.2 (0,5)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1715	GFLOPS: 6736.91 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.44, Tstamp:1669893406.06)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,36)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
          pad_temp.shared = ...
      for nn_c.3 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,4)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,4)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1716	GFLOPS: 6705.89 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.59, Tstamp:1669893407.93)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,36)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for yy_c.3 (0,4)
          for rx.2 (0,5)
            for nn_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,4)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1717	GFLOPS: 6705.79 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.28, Tstamp:1669893409.59)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,36)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for nn_c.4 (0,2)
            for yy_c.4 (0,4)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,4)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1718	GFLOPS: 6753.48 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.78, Tstamp:1669893411.24)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for yy_c.3 (0,2)
          for ry.2 (0,5)
            for nn_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1719	GFLOPS: 6671.22 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.37, Tstamp:1669893413.04)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1720	GFLOPS: 6686.15 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.68, Tstamp:1669893415.01)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1721	GFLOPS: 6666.82 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.75, Tstamp:1669893416.67)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for yy_c.3 (0,2)
          for rx.2 (0,5)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1722	GFLOPS: 6663.74 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.71, Tstamp:1669893418.34)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for ry.2 (0,5)
          for yy_c.4 (0,2)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1723	GFLOPS: 6670.64 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.75, Tstamp:1669893420.05)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for yy_c.3 (0,2)
          for ry.2 (0,5)
            for xx_c.4 (0,2)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1724	GFLOPS: 6746.70 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.24, Tstamp:1669893421.91)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1725	GFLOPS: 6588.32 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.95, Tstamp:1669893423.55)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for yy_c.3 (0,2)
          for rc.2 (0,2)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1726	GFLOPS: 674.63 / 7146.00	results: MeasureResult(cost:[0.0019], error_no:0, all_cost:2.50, Tstamp:1669893425.20)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,256)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,56)
    conv2d_nchw.local auto_unroll: 64
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,83)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for nn_c.3 (0,4)
          for rx.2 (0,5)
            for yy_c.4 (0,14)
              conv2d_nchw.local = ...
    for nn.3 (0,4)
      for yy.3 (0,14)
        conv2d_nchw = ...

==================================================
No: 1727	GFLOPS: 6689.14 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.86, Tstamp:1669893426.97)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for yy_c.3 (0,2)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1728	GFLOPS: 4790.05 / 7146.00	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:3.13, Tstamp:1669893428.82)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for nn_c.3 (0,2)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for nn.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

Time elapsed for measurement: 123.92 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 1.46 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------

-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.401 ms	Trials: 896	Used time : 2120 s	Next ID: 0	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                                Conv5x5_origin |        0.182 |        7062.44 |    512 |
|    1 |                                                   Conv5x5_opt |        0.219 |        8616.58 |    448 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.400 ms	Trials: 960	Used time : 2266 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                                Conv5x5_origin |        0.182 |        7062.44 |    512 |
|    1 |                                                   Conv5x5_opt |        0.219 |        8616.58 |    512 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.400 ms	Trials: 1024	Used time : 2409 s	Next ID: 0	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                                Conv5x5_origin |        0.180 |        7119.92 |    576 |
|    1 |                                                   Conv5x5_opt |        0.219 |        8616.58 |    512 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.399 ms	Trials: 1088	Used time : 2554 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                                Conv5x5_origin |        0.180 |        7119.92 |    576 |
|    1 |                                                   Conv5x5_opt |        0.218 |        8625.46 |    576 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.399 ms	Trials: 1152	Used time : 2704 s	Next ID: 0	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                                Conv5x5_origin |        0.180 |        7119.92 |    640 |
|    1 |                                                   Conv5x5_opt |        0.218 |        8625.46 |    576 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.399 ms	Trials: 1216	Used time : 2853 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                                Conv5x5_origin |        0.180 |        7119.92 |    640 |
|    1 |                                                   Conv5x5_opt |        0.218 |        8625.46 |    640 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.399 ms	Trials: 1280	Used time : 3000 s	Next ID: 0	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                                Conv5x5_origin |        0.180 |        7119.92 |    704 |
|    1 |                                                   Conv5x5_opt |        0.218 |        8625.46 |    640 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.399 ms	Trials: 1344	Used time : 3148 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                                Conv5x5_origin |        0.180 |        7119.92 |    704 |
|    1 |                                                   Conv5x5_opt |        0.218 |        8625.46 |    704 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.399 ms	Trials: 1408	Used time : 3293 s	Next ID: 0	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                                Conv5x5_origin |        0.180 |        7138.10 |    768 |
|    1 |                                                   Conv5x5_opt |        0.218 |        8625.46 |    704 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.398 ms	Trials: 1472	Used time : 3441 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                                Conv5x5_origin |        0.180 |        7138.10 |    768 |
|    1 |                                                   Conv5x5_opt |        0.218 |        8625.46 |    768 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.398 ms	Trials: 1536	Used time : 3586 s	Next ID: 0	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                                Conv5x5_origin |        0.180 |        7146.00 |    832 |
|    1 |                                                   Conv5x5_opt |        0.218 |        8625.46 |    768 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.398 ms	Trials: 1600	Used time : 3734 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                                Conv5x5_origin |        0.180 |        7146.00 |    832 |
|    1 |                                                   Conv5x5_opt |        0.218 |        8625.46 |    832 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.398 ms	Trials: 1664	Used time : 3881 s	Next ID: 0	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 73	fail_ct: 4023	Time elapsed: 2.18
GA Iter: 0	Max score: 0.6928	Min score: 0.0833	#Pop: 73	#M+: 0	#M-: 0
GA Iter: 4	Max score: 1.0015	Min score: 0.9148	#Pop: 128	#M+: 1404	#M-: 0
EvolutionarySearch		#s: 128	Time elapsed: 15.42
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************==================================================
No: 1729	GFLOPS: 8572.57 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669893461.84)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1730	GFLOPS: 8564.17 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669893463.56)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ff_c.3 (0,4)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1731	GFLOPS: 8508.92 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.29, Tstamp:1669893465.28)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,4)
              for yy_c.3 (0,2)
                for ry.2 (0,3)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1732	GFLOPS: 8507.05 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.14, Tstamp:1669893467.05)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for rx.2 (0,3)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1733	GFLOPS: 8578.64 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669893468.77)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1734	GFLOPS: 8503.03 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.20, Tstamp:1669893470.61)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for rx.2 (0,3)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1735	GFLOPS: 8502.17 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669893472.33)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for ff_c.3 (0,4)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1736	GFLOPS: 8495.76 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.06, Tstamp:1669893474.18)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for ff_c.3 (0,4)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1737	GFLOPS: 8549.13 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.48, Tstamp:1669893476.17)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for ff_c.3 (0,4)
                for yy_c.3 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1738	GFLOPS: 8477.08 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669893477.88)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for ff_c.3 (0,4)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1739	GFLOPS: 8540.73 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.06, Tstamp:1669893479.61)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.2 (0,3)
            for nn_c.4 (0,2)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1740	GFLOPS: 8578.96 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669893481.30)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1741	GFLOPS: 8495.20 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.22, Tstamp:1669893483.22)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,4)
              for ry.2 (0,3)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1742	GFLOPS: 8584.24 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669893484.92)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for ff_c.3 (0,4)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1743	GFLOPS: 8581.00 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.11, Tstamp:1669893486.72)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1744	GFLOPS: 8540.28 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.12, Tstamp:1669893488.43)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for ff_c.3 (0,4)
            for ry.2 (0,3)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1745	GFLOPS: 8515.10 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.22, Tstamp:1669893490.30)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1746	GFLOPS: 8511.05 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.21, Tstamp:1669893492.21)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1747	GFLOPS: 8550.68 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.00, Tstamp:1669893493.93)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for yy_c.3 (0,2)
              for nn_c.4 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1748	GFLOPS: 8578.17 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.00, Tstamp:1669893495.63)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1749	GFLOPS: 8555.78 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.07, Tstamp:1669893497.36)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for yy_c.3 (0,2)
              for nn_c.4 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1750	GFLOPS: 8571.90 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.32, Tstamp:1669893499.35)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1751	GFLOPS: 8585.47 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669893501.06)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for yy_c.3 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1752	GFLOPS: 8565.36 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669893502.79)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for yy_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1753	GFLOPS: 8365.22 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669893504.50)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,3)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for nn_c.4 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 1754	GFLOPS: 8383.22 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.23, Tstamp:1669893506.35)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ff_c.3 (0,4)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for rx.2 (0,3)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1755	GFLOPS: 8376.31 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669893508.07)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1756	GFLOPS: 8350.02 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.13, Tstamp:1669893509.92)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for ff_c.3 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1757	GFLOPS: 8354.50 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669893511.64)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,4)
              for ry.2 (0,3)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1758	GFLOPS: 8375.65 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.90, Tstamp:1669893513.52)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,4)
              for ry.2 (0,3)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1759	GFLOPS: 8335.06 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.26, Tstamp:1669893515.51)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1760	GFLOPS: 8363.25 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669893517.22)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for yy_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1761	GFLOPS: 8380.90 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669893518.97)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for yy_c.3 (0,2)
              for nn_c.4 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1762	GFLOPS: 8362.92 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.99, Tstamp:1669893520.66)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1763	GFLOPS: 7987.85 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.88, Tstamp:1669893522.50)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for rx.2 (0,3)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1764	GFLOPS: 8075.94 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.80, Tstamp:1669893524.21)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for ry.2 (0,3)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1765	GFLOPS: 8051.82 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.95, Tstamp:1669893526.06)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1766	GFLOPS: 8034.78 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.84, Tstamp:1669893527.75)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1767	GFLOPS: 8046.59 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.71, Tstamp:1669893529.43)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for ff_c.3 (0,4)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1768	GFLOPS: 8069.11 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.83, Tstamp:1669893531.14)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1769	GFLOPS: 8025.38 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.84, Tstamp:1669893532.91)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1770	GFLOPS: 8051.52 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.79, Tstamp:1669893534.60)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for yy_c.3 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1771	GFLOPS: 8033.12 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.81, Tstamp:1669893536.29)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for yy_c.3 (0,2)
              for nn_c.4 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1772	GFLOPS: 8075.51 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.73, Tstamp:1669893537.98)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1773	GFLOPS: 8014.88 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.94, Tstamp:1669893539.79)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1774	GFLOPS: 8059.71 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.75, Tstamp:1669893541.50)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for nn_c.3 (0,2)
        for ff_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1775	GFLOPS: 8057.17 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.79, Tstamp:1669893543.16)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1776	GFLOPS: 8055.43 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.71, Tstamp:1669893544.87)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for yy_c.3 (0,2)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1777	GFLOPS: 8084.51 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.83, Tstamp:1669893546.67)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for nn_c.3 (0,2)
        for yy_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1778	GFLOPS: 8005.54 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.76, Tstamp:1669893548.37)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for ry.1 (0,3)
        for nn_c.3 (0,4)
          for yy_c.3 (0,2)
            for rx.2 (0,3)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1779	GFLOPS: 8048.38 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.70, Tstamp:1669893550.04)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          pad_temp.shared = ...
      for rx.1 (0,3)
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
    for nn.3 (0,4)
      for ff.3 (0,4)
        for yy.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1780	GFLOPS: 8036.77 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.64, Tstamp:1669893551.73)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rx.1 (0,3)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for nn_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1781	GFLOPS: 8042.58 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.82, Tstamp:1669893553.54)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for nn_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1782	GFLOPS: 8516.86 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.73, Tstamp:1669893555.26)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1783	GFLOPS: 8012.39 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.80, Tstamp:1669893557.15)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,4)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1784	GFLOPS: 8062.82 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.93, Tstamp:1669893559.05)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1785	GFLOPS: 8021.55 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.99, Tstamp:1669893560.76)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1786	GFLOPS: 8001.36 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.94, Tstamp:1669893562.46)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for yy_c.3 (0,2)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1787	GFLOPS: 7966.03 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.93, Tstamp:1669893564.19)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for yy_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1788	GFLOPS: 7994.56 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.24, Tstamp:1669893566.17)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for rx.2 (0,3)
                  for nn_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1789	GFLOPS: 8031.45 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.91, Tstamp:1669893567.87)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1790	GFLOPS: 3057.42 / 8625.46	results: MeasureResult(cost:[0.0006], error_no:0, all_cost:2.91, Tstamp:1669893570.05)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1808)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,226)
      for rc.0 (0,4)
        for ry.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,17)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
              pad_temp.shared = ...
          for rc.1 (0,2)
            for rc.2 (0,4)
              for rx.2 (0,3)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for ff.3 (0,4)
        conv2d_nchw = ...

==================================================
No: 1791	GFLOPS: 3009.53 / 8625.46	results: MeasureResult(cost:[0.0006], error_no:0, all_cost:2.44, Tstamp:1669893571.76)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,12769)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
      for rc.0 (0,8)
        for ax0@ax1@ax2@ax3@.0.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,32)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for ry.1 (0,3)
            for rx.1 (0,3)
              for xx_c.3 (0,2)
                for rc.2 (0,2)
                  for nn_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1792	GFLOPS: 8003.76 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.84, Tstamp:1669893573.50)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,17)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for ry.2 (0,3)
                for nn_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,2)
          for yy.3 (0,2)
            conv2d_nchw = ...

Time elapsed for measurement: 125.51 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 1.71 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Iter: 5	#Pop: 34	#Target: 50	fail_ct: 10206	Time elapsed: 4.34
Sample Initial Population	#s: 50	fail_ct: 14286	Time elapsed: 6.19
GA Iter: 0	Max score: 0.7688	Min score: -0.1574	#Pop: 50	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9879	Min score: 0.9235	#Pop: 128	#M+: 1380	#M-: 0
EvolutionarySearch		#s: 128	Time elapsed: 15.40
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************==================================================
No: 1793	GFLOPS: 7039.40 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.87, Tstamp:1669893612.48)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1794	GFLOPS: 7061.76 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.92, Tstamp:1669893614.15)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for yy_c.3 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for nn_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1795	GFLOPS: 7058.52 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.90, Tstamp:1669893615.82)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for xx_c.3 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1796	GFLOPS: 7051.62 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.24, Tstamp:1669893617.49)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1797	GFLOPS: 7066.73 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.88, Tstamp:1669893619.17)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for nn_c.3 (0,2)
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1798	GFLOPS: 7039.41 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.17, Tstamp:1669893621.07)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1799	GFLOPS: 7027.05 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.97, Tstamp:1669893622.73)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1800	GFLOPS: 7113.89 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.96, Tstamp:1669893624.40)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for yy_c.3 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for nn_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1801	GFLOPS: 6999.44 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669893626.08)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for rc.2 (0,2)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1802	GFLOPS: 7093.90 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.09, Tstamp:1669893627.95)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for xx_c.3 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1803	GFLOPS: 7087.91 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.88, Tstamp:1669893629.62)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1804	GFLOPS: 7094.65 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.88, Tstamp:1669893631.31)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1805	GFLOPS: 7087.03 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.90, Tstamp:1669893633.07)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1806	GFLOPS: 6943.44 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.26, Tstamp:1669893634.94)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for xx_c.3 (0,2)
            for rc.2 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1807	GFLOPS: 6892.86 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.96, Tstamp:1669893636.59)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1808	GFLOPS: 6189.54 / 7146.00	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.90, Tstamp:1669893638.26)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for yy_c.3 (0,4)
        for xx_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              conv2d_nchw.local = ...
    for yy.3 (0,4)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1809	GFLOPS: 7652.84 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.87, Tstamp:1669893640.12)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for yy_c.3 (0,2)
          for xx_c.3 (0,4)
            for rx.2 (0,5)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,4)
        conv2d_nchw = ...

==================================================
No: 1810	GFLOPS: 6987.80 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.19, Tstamp:1669893641.99)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              for rc.2 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1811	GFLOPS: 6921.11 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.41, Tstamp:1669893643.65)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for rx.2 (0,5)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1812	GFLOPS: 6900.96 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.90, Tstamp:1669893645.32)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for nn_c.3 (0,2)
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1813	GFLOPS: 6928.44 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.08, Tstamp:1669893647.18)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for nn_c.4 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1814	GFLOPS: 6884.55 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.09, Tstamp:1669893649.06)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1815	GFLOPS: 6914.11 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669893650.74)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1816	GFLOPS: 6900.95 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.87, Tstamp:1669893652.41)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for nn_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1817	GFLOPS: 6894.55 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.88, Tstamp:1669893654.27)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.2 (0,5)
        for rx.2 (0,5)
          for nn_c.4 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1818	GFLOPS: 6903.86 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669893656.14)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.2 (0,5)
        for rx.2 (0,5)
          for nn_c.4 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1819	GFLOPS: 6899.79 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.87, Tstamp:1669893657.80)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for xx_c.3 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1820	GFLOPS: 6910.74 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.90, Tstamp:1669893659.48)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.2 (0,5)
          for nn_c.4 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1821	GFLOPS: 5585.51 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.47, Tstamp:1669893661.31)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,36)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
          pad_temp.shared = ...
      for yy_c.3 (0,2)
        for xx_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for nn_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 1822	GFLOPS: 5595.70 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.77, Tstamp:1669893663.14)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,36)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
          pad_temp.shared = ...
      for nn_c.3 (0,2)
        for yy_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,4)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1823	GFLOPS: 6925.27 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.85, Tstamp:1669893664.82)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for nn_c.3 (0,2)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1824	GFLOPS: 6924.33 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.85, Tstamp:1669893666.50)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for nn_c.3 (0,2)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1825	GFLOPS: 6876.72 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.88, Tstamp:1669893668.31)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for yy_c.3 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for nn_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1826	GFLOPS: 6909.69 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.08, Tstamp:1669893670.18)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for nn_c.3 (0,2)
        for yy_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1827	GFLOPS: 6925.04 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.83, Tstamp:1669893671.85)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for nn_c.3 (0,2)
        for yy_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1828	GFLOPS: 6909.31 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.85, Tstamp:1669893673.52)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1829	GFLOPS: 6753.78 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.52, Tstamp:1669893675.29)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,36)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,4)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1830	GFLOPS: 4607.19 / 7652.84	results: MeasureResult(cost:[0.0003], error_no:0, all_cost:3.60, Tstamp:1669893677.15)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for nn_c.3 (0,2)
        for xx_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              conv2d_nchw.local = ...
    for nn.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1831	GFLOPS: 3111.53 / 7652.84	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:4.42, Tstamp:1669893679.25)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,20)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1832	GFLOPS: 3111.42 / 7652.84	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:4.27, Tstamp:1669893681.38)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,20)
          pad_temp.shared = ...
      for ry.2 (0,5)
        for rx.2 (0,5)
          for nn_c.4 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1833	GFLOPS: 3108.91 / 7652.84	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:4.32, Tstamp:1669893683.48)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,20)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1834	GFLOPS: 6788.91 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.95, Tstamp:1669893685.15)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1835	GFLOPS: 3109.17 / 7652.84	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:4.41, Tstamp:1669893687.29)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,20)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for nn_c.3 (0,2)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1836	GFLOPS: 6790.59 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.15, Tstamp:1669893689.14)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for rc.2 (0,2)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1837	GFLOPS: 6784.92 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.06, Tstamp:1669893691.02)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for yy_c.3 (0,2)
        for xx_c.3 (0,2)
          for rc.2 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1838	GFLOPS: 3110.12 / 7652.84	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:4.40, Tstamp:1669893693.14)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,20)
          pad_temp.shared = ...
      for nn_c.3 (0,2)
        for yy_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1839	GFLOPS: 3109.85 / 7652.84	results: MeasureResult(cost:[0.0004], error_no:0, all_cost:4.29, Tstamp:1669893695.27)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,20)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1840	GFLOPS: 6706.97 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.50, Tstamp:1669893697.04)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,9)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for yy_c.3 (0,2)
        for xx_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1841	GFLOPS: 2481.44 / 7652.84	results: MeasureResult(cost:[0.0005], error_no:0, all_cost:4.01, Tstamp:1669893699.15)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,20)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for nn_c.3 (0,2)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1842	GFLOPS: 5480.19 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.44, Tstamp:1669893700.80)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,36)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for xx_c.3 (0,2)
          for rx.2 (0,5)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                for xx_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 1843	GFLOPS: 6887.11 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.85, Tstamp:1669893702.57)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for ry.2 (0,5)
          for nn_c.4 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1844	GFLOPS: 6883.00 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.70, Tstamp:1669893704.24)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for ry.2 (0,5)
          for nn_c.4 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1845	GFLOPS: 6845.44 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.61, Tstamp:1669893705.90)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for nn_c.3 (0,2)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1846	GFLOPS: 6834.71 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.56, Tstamp:1669893707.54)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.2 (0,5)
          for nn_c.4 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1847	GFLOPS: 6826.33 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.92, Tstamp:1669893709.47)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.2 (0,5)
        for rx.2 (0,5)
          for nn_c.4 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1848	GFLOPS: 6754.93 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.65, Tstamp:1669893711.10)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,15)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for yy_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1849	GFLOPS: 6777.09 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.94, Tstamp:1669893712.76)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,15)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1850	GFLOPS: 6806.38 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.02, Tstamp:1669893714.52)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1851	GFLOPS: 6802.36 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.07, Tstamp:1669893716.38)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1852	GFLOPS: 6833.81 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.94, Tstamp:1669893718.03)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.2 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1853	GFLOPS: 6818.61 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.97, Tstamp:1669893719.69)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1854	GFLOPS: 2540.53 / 7652.84	results: MeasureResult(cost:[0.0005], error_no:0, all_cost:5.09, Tstamp:1669893721.96)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,448)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,8)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,112)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        for ry.0 (0,5)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,112)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,18)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,112)
              pad_temp.shared = ...
          for xx_c.3 (0,2)
            for rx.2 (0,5)
              conv2d_nchw.local = ...
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1855	GFLOPS: 6621.66 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.99, Tstamp:1669893723.76)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rc.2 (0,2)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1856	GFLOPS: 6791.03 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.16, Tstamp:1669893725.62)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.2 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

Time elapsed for measurement: 128.69 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 1.38 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 56	fail_ct: 4040	Time elapsed: 2.18
GA Iter: 0	Max score: 0.7404	Min score: 0.0118	#Pop: 56	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9842	Min score: 0.9292	#Pop: 128	#M+: 1399	#M-: 0
EvolutionarySearch		#s: 128	Time elapsed: 15.60
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************==================================================
No: 1857	GFLOPS: 8487.20 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.22, Tstamp:1669893759.16)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ff_c.3 (0,4)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1858	GFLOPS: 8531.35 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669893760.87)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ff_c.3 (0,4)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1859	GFLOPS: 8562.90 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669893762.56)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1860	GFLOPS: 8508.22 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.06, Tstamp:1669893764.33)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,4)
              for rx.2 (0,3)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1861	GFLOPS: 8607.73 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.21, Tstamp:1669893766.22)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for ff_c.3 (0,4)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1862	GFLOPS: 8566.64 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.08, Tstamp:1669893767.93)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for yy_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1863	GFLOPS: 8550.57 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.15, Tstamp:1669893769.63)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for yy_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1864	GFLOPS: 8573.14 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.14, Tstamp:1669893771.39)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1865	GFLOPS: 8342.61 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.25, Tstamp:1669893773.28)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,3)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ff_c.3 (0,4)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 1866	GFLOPS: 8567.52 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.07, Tstamp:1669893775.00)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ff_c.3 (0,4)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1867	GFLOPS: 8534.23 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.08, Tstamp:1669893776.72)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for ff_c.3 (0,4)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1868	GFLOPS: 8489.42 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669893778.44)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1869	GFLOPS: 8527.56 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.30, Tstamp:1669893780.35)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1870	GFLOPS: 8541.99 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669893782.09)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.4 (0,2)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1871	GFLOPS: 8553.67 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669893783.80)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.2 (0,3)
          for rx.2 (0,3)
            for nn_c.4 (0,2)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1872	GFLOPS: 8559.98 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669893785.52)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.2 (0,3)
            for nn_c.4 (0,2)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1873	GFLOPS: 8530.36 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.36, Tstamp:1669893787.51)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for ff_c.3 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1874	GFLOPS: 8494.16 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669893789.22)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ry.2 (0,3)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1875	GFLOPS: 8547.25 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.00, Tstamp:1669893790.93)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for rx.2 (0,3)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1876	GFLOPS: 8555.81 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669893792.62)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for ff_c.3 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1877	GFLOPS: 8510.15 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.27, Tstamp:1669893794.58)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for ry.2 (0,3)
            for nn_c.4 (0,2)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1878	GFLOPS: 8525.35 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.06, Tstamp:1669893796.29)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for ff_c.3 (0,4)
              for yy_c.3 (0,2)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1879	GFLOPS: 8561.62 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.00, Tstamp:1669893798.01)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for ff_c.3 (0,4)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1880	GFLOPS: 8524.63 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.00, Tstamp:1669893799.72)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for yy_c.3 (0,2)
              for nn_c.4 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1881	GFLOPS: 8525.92 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.23, Tstamp:1669893801.67)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for yy_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1882	GFLOPS: 8563.13 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669893803.37)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ff_c.3 (0,4)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1883	GFLOPS: 8602.44 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669893805.09)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for rx.2 (0,3)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1884	GFLOPS: 8588.57 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.97, Tstamp:1669893806.79)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for ff_c.3 (0,2)
                for yy_c.3 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1885	GFLOPS: 8502.38 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.22, Tstamp:1669893808.71)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for ry.2 (0,3)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1886	GFLOPS: 8504.93 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.91, Tstamp:1669893810.42)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for ry.2 (0,3)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1887	GFLOPS: 8512.69 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.16, Tstamp:1669893812.25)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1888	GFLOPS: 8573.74 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.97, Tstamp:1669893813.96)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ff_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1889	GFLOPS: 8556.15 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.14, Tstamp:1669893815.81)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1890	GFLOPS: 8531.50 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.22, Tstamp:1669893817.74)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for ff_c.3 (0,2)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1891	GFLOPS: 8554.25 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.95, Tstamp:1669893819.46)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for ff_c.3 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1892	GFLOPS: 8506.35 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.94, Tstamp:1669893821.17)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1893	GFLOPS: 8499.84 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.99, Tstamp:1669893822.88)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.2 (0,3)
            for nn_c.4 (0,2)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1894	GFLOPS: 8513.63 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.15, Tstamp:1669893824.81)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1895	GFLOPS: 8348.84 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.91, Tstamp:1669893826.52)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,3)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for ff_c.3 (0,4)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1896	GFLOPS: 8333.89 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669893828.24)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,3)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for ff_c.3 (0,4)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1897	GFLOPS: 8376.70 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.98, Tstamp:1669893829.96)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,3)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,4)
              for ry.2 (0,3)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1898	GFLOPS: 8371.75 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.23, Tstamp:1669893831.92)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,3)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for ff_c.3 (0,4)
              for yy_c.3 (0,2)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1899	GFLOPS: 8569.80 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.93, Tstamp:1669893833.65)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ff_c.3 (0,4)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for rx.2 (0,3)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1900	GFLOPS: 8413.98 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.96, Tstamp:1669893835.36)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,3)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ry.2 (0,3)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1901	GFLOPS: 8359.49 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.86, Tstamp:1669893837.07)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,3)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1902	GFLOPS: 8471.70 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.20, Tstamp:1669893839.01)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ff_c.3 (0,4)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for rx.2 (0,3)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1903	GFLOPS: 8448.69 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.97, Tstamp:1669893840.72)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,3)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for rx.2 (0,3)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1904	GFLOPS: 8371.98 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.88, Tstamp:1669893842.43)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,3)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for ff_c.3 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1905	GFLOPS: 8356.18 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.00, Tstamp:1669893844.14)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,3)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1906	GFLOPS: 8500.30 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.22, Tstamp:1669893846.06)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,4)
              for yy_c.3 (0,2)
                for rx.2 (0,3)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1907	GFLOPS: 8423.43 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.78, Tstamp:1669893847.77)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,3)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ff_c.3 (0,4)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for rx.2 (0,3)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1908	GFLOPS: 8354.85 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.80, Tstamp:1669893849.47)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for ff_c.3 (0,4)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1909	GFLOPS: 8358.97 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.93, Tstamp:1669893851.17)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ff_c.3 (0,4)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1910	GFLOPS: 8368.53 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.14, Tstamp:1669893853.03)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1911	GFLOPS: 8371.44 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.88, Tstamp:1669893854.69)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for ff_c.3 (0,4)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1912	GFLOPS: 8340.83 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669893856.58)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for ff_c.3 (0,4)
              for yy_c.3 (0,2)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1913	GFLOPS: 8387.25 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.03, Tstamp:1669893858.28)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for yy_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1914	GFLOPS: 8395.58 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.06, Tstamp:1669893860.17)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ff_c.3 (0,4)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1915	GFLOPS: 8366.82 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.32, Tstamp:1669893862.15)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.2 (0,3)
          for rx.2 (0,3)
            for nn_c.4 (0,2)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1916	GFLOPS: 8363.40 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.07, Tstamp:1669893863.85)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.4 (0,2)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1917	GFLOPS: 8365.01 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.06, Tstamp:1669893865.56)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.2 (0,3)
          for rx.2 (0,3)
            for nn_c.4 (0,2)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1918	GFLOPS: 3764.67 / 8625.46	results: MeasureResult(cost:[0.0005], error_no:0, all_cost:2.41, Tstamp:1669893867.25)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1808)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,226)
    for rc.0 (0,8)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,17)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ff_c.3 (0,2)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              for rc.2 (0,2)
                for ry.2 (0,3)
                  for rx.2 (0,3)
                    conv2d_nchw.local = ...
    for ff.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1919	GFLOPS: 2791.29 / 8625.46	results: MeasureResult(cost:[0.0007], error_no:0, all_cost:3.15, Tstamp:1669893869.21)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,452)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,4)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,226)
      conv2d_nchw.local auto_unroll: 64
      for rc.0 (0,16)
        for ry.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,32)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,226)
              pad_temp.shared = ...
          for nn_c.3 (0,2)
            for rc.2 (0,2)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for xx_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1920	GFLOPS: 8347.99 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.93, Tstamp:1669893870.91)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,6)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for rx.2 (0,3)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

Time elapsed for measurement: 126.01 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 1.76 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Iter: 5	#Pop: 38	#Target: 50	fail_ct: 10202	Time elapsed: 4.38
Sample Initial Population	#s: 51	fail_ct: 14285	Time elapsed: 6.14
GA Iter: 0	Max score: 0.5399	Min score: -0.0471	#Pop: 51	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9162	Min score: 0.8493	#Pop: 128	#M+: 1405	#M-: 0
EvolutionarySearch		#s: 128	Time elapsed: 15.27
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************==================================================
No: 1921	GFLOPS: 7073.36 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.84, Tstamp:1669893908.91)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for nn_c.3 (0,2)
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1922	GFLOPS: 6906.63 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.88, Tstamp:1669893910.57)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for nn_c.3 (0,2)
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1923	GFLOPS: 6926.43 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.85, Tstamp:1669893912.21)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1924	GFLOPS: 6899.69 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.87, Tstamp:1669893913.86)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for yy_c.3 (0,2)
          for rx.2 (0,5)
            for nn_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1925	GFLOPS: 6921.34 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.85, Tstamp:1669893915.53)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for nn_c.3 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1926	GFLOPS: 6928.29 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.88, Tstamp:1669893917.20)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1927	GFLOPS: 6922.73 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.07, Tstamp:1669893919.00)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for nn_c.4 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1928	GFLOPS: 6911.29 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.90, Tstamp:1669893920.67)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.2 (0,5)
          for nn_c.4 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1929	GFLOPS: 6928.04 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.88, Tstamp:1669893922.34)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for yy_c.3 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for nn_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1930	GFLOPS: 6890.59 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.95, Tstamp:1669893924.01)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for yy_c.3 (0,2)
          for rx.2 (0,5)
            for nn_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1931	GFLOPS: 7099.48 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.87, Tstamp:1669893925.82)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for xx_c.3 (0,2)
          for rx.2 (0,5)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1932	GFLOPS: 6768.57 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.88, Tstamp:1669893927.48)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              for ry.2 (0,5)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1933	GFLOPS: 7061.51 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.93, Tstamp:1669893929.16)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for xx_c.3 (0,2)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1934	GFLOPS: 7089.83 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.91, Tstamp:1669893930.83)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for xx_c.3 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1935	GFLOPS: 6789.96 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.16, Tstamp:1669893932.65)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for nn_c.3 (0,2)
          for xx_c.3 (0,2)
            for ry.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1936	GFLOPS: 6797.65 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.88, Tstamp:1669893934.31)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for xx_c.3 (0,2)
          for ry.2 (0,5)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1937	GFLOPS: 7107.07 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.90, Tstamp:1669893935.99)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for nn_c.3 (0,2)
          for xx_c.3 (0,2)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1938	GFLOPS: 7100.43 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.86, Tstamp:1669893937.67)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for nn_c.3 (0,2)
          for xx_c.3 (0,2)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1939	GFLOPS: 6786.05 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.15, Tstamp:1669893939.48)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for rc.2 (0,2)
              for ry.2 (0,5)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1940	GFLOPS: 6910.50 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.82, Tstamp:1669893941.15)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for ry.2 (0,5)
              for nn_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1941	GFLOPS: 7112.66 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.06, Tstamp:1669893942.82)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for yy_c.3 (0,2)
        for xx_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for nn_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1942	GFLOPS: 7030.02 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.92, Tstamp:1669893944.49)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for rx.2 (0,5)
              for nn_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1943	GFLOPS: 7115.89 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.13, Tstamp:1669893946.39)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for yy_c.3 (0,2)
        for xx_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for nn_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1944	GFLOPS: 7048.34 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.87, Tstamp:1669893948.07)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              for nn_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1945	GFLOPS: 7039.24 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.89, Tstamp:1669893949.74)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              for nn_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1946	GFLOPS: 6921.12 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669893951.41)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              for rc.2 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1947	GFLOPS: 6950.20 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.25, Tstamp:1669893953.27)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              for rc.2 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1948	GFLOPS: 6799.67 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.88, Tstamp:1669893954.94)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for xx_c.3 (0,2)
          for ry.2 (0,5)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1949	GFLOPS: 6900.29 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.94, Tstamp:1669893956.62)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for xx_c.3 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1950	GFLOPS: 6986.58 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.87, Tstamp:1669893958.38)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              for nn_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1951	GFLOPS: 6759.50 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669893960.25)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for ry.2 (0,5)
              for nn_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1952	GFLOPS: 6982.49 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.81, Tstamp:1669893961.95)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for rx.2 (0,5)
              for nn_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1953	GFLOPS: 6992.32 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.81, Tstamp:1669893963.61)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for rx.2 (0,5)
              for nn_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1954	GFLOPS: 6729.04 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.85, Tstamp:1669893965.36)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1955	GFLOPS: 6725.46 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.96, Tstamp:1669893967.23)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.2 (0,5)
        for rx.2 (0,5)
          for nn_c.4 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1956	GFLOPS: 7467.12 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.75, Tstamp:1669893968.88)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,15)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for xx_c.3 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1957	GFLOPS: 6729.90 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.66, Tstamp:1669893970.52)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,15)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1958	GFLOPS: 6729.99 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.69, Tstamp:1669893972.32)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,15)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1959	GFLOPS: 7484.89 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.88, Tstamp:1669893974.18)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,15)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for xx_c.3 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1960	GFLOPS: 6890.46 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.85, Tstamp:1669893975.84)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for yy_c.3 (0,2)
        for xx_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for nn_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1961	GFLOPS: 6761.70 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.88, Tstamp:1669893977.50)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for nn_c.3 (0,2)
          for ry.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1962	GFLOPS: 6782.68 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.87, Tstamp:1669893979.30)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1963	GFLOPS: 6799.38 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.95, Tstamp:1669893981.17)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for nn_c.3 (0,2)
          for ry.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1964	GFLOPS: 6707.20 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.87, Tstamp:1669893982.82)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for yy_c.3 (0,2)
        for xx_c.3 (0,2)
          for rc.2 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1965	GFLOPS: 6480.01 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.65, Tstamp:1669893984.46)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for nn_c.3 (0,2)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1966	GFLOPS: 6778.80 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.73, Tstamp:1669893986.26)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for nn_c.4 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1967	GFLOPS: 6718.22 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.99, Tstamp:1669893988.13)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,15)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for yy_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1968	GFLOPS: 7503.65 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.70, Tstamp:1669893989.80)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,15)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for xx_c.3 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1969	GFLOPS: 6671.88 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.85, Tstamp:1669893991.45)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for yy_c.3 (0,2)
          for rc.2 (0,2)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1970	GFLOPS: 6730.51 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.78, Tstamp:1669893993.22)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for yy_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1971	GFLOPS: 6722.00 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.92, Tstamp:1669893995.09)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,15)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for yy_c.3 (0,2)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1972	GFLOPS: 6713.70 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.70, Tstamp:1669893996.75)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for xx_c.3 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1973	GFLOPS: 6787.27 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.76, Tstamp:1669893998.42)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for nn_c.3 (0,2)
        for yy_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1974	GFLOPS: 6660.74 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.60, Tstamp:1669894000.23)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              for rc.2 (0,2)
                for rx.2 (0,5)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1975	GFLOPS: 6446.32 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.83, Tstamp:1669894002.08)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for ry.2 (0,5)
          for nn_c.4 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1976	GFLOPS: 6669.77 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.55, Tstamp:1669894003.73)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              for ry.2 (0,5)
                for rx.2 (0,5)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1977	GFLOPS: 6671.22 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.90, Tstamp:1669894005.40)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,16)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,36)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rc.1 (0,2)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              for ry.2 (0,5)
                for rx.2 (0,5)
                  conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 1978	GFLOPS: 6791.14 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.03, Tstamp:1669894007.19)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for xx_c.3 (0,2)
          for rc.2 (0,2)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1979	GFLOPS: 6770.67 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.33, Tstamp:1669894009.08)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for xx_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1980	GFLOPS: 6798.53 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.00, Tstamp:1669894010.75)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for xx_c.3 (0,2)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1981	GFLOPS: 6789.28 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.00, Tstamp:1669894012.42)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for xx_c.3 (0,2)
          for rc.2 (0,2)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1982	GFLOPS: 675.91 / 7652.84	results: MeasureResult(cost:[0.0019], error_no:0, all_cost:2.31, Tstamp:1669894014.17)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,32)
    for rc.0 (0,4)
      for rx.0 (0,5)
        for ax0@ax1@ax2@ax3@.0.0 (0,2)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,256)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,32)
            pad_temp.shared = ...
        for rc.1 (0,8)
          for ry.1 (0,5)
            for nn_c.3 (0,2)
              for yy_c.4 (0,2)
                for xx_c.4 (0,4)
                  conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 1983	GFLOPS: 6761.21 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.14, Tstamp:1669894016.00)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for xx_c.3 (0,2)
        for rc.2 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 1984	GFLOPS: 6800.72 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.97, Tstamp:1669894017.66)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for ry.1 (0,5)
          for xx_c.3 (0,2)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

Time elapsed for measurement: 123.47 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 1.74 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Initial Population	#s: 64	fail_ct: 4032	Time elapsed: 2.24
GA Iter: 0	Max score: 0.6299	Min score: -0.0086	#Pop: 64	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9945	Min score: 0.9345	#Pop: 128	#M+: 1393	#M-: 0
EvolutionarySearch		#s: 128	Time elapsed: 15.39
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************==================================================
No: 1985	GFLOPS: 8575.72 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.12, Tstamp:1669894051.27)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ff_c.3 (0,4)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1986	GFLOPS: 8517.77 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.06, Tstamp:1669894052.98)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ff_c.3 (0,4)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1987	GFLOPS: 8573.27 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.12, Tstamp:1669894054.67)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ff_c.3 (0,4)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1988	GFLOPS: 8533.35 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.33, Tstamp:1669894056.60)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,4)
              for rx.2 (0,3)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1989	GFLOPS: 8540.47 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669894058.31)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for ff_c.3 (0,4)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1990	GFLOPS: 8545.93 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.06, Tstamp:1669894060.01)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for ff_c.3 (0,4)
            for yy_c.3 (0,2)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1991	GFLOPS: 8459.48 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.14, Tstamp:1669894061.72)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,4)
              for rx.2 (0,3)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1992	GFLOPS: 8517.52 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.26, Tstamp:1669894063.64)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,4)
              for yy_c.3 (0,2)
                for ry.2 (0,3)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1993	GFLOPS: 8452.35 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.08, Tstamp:1669894065.34)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ff_c.3 (0,4)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1994	GFLOPS: 8541.63 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669894067.06)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1995	GFLOPS: 8544.07 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.13, Tstamp:1669894068.77)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1996	GFLOPS: 8438.74 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.27, Tstamp:1669894070.70)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for rx.2 (0,3)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1997	GFLOPS: 8508.58 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.09, Tstamp:1669894072.41)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for ff_c.3 (0,4)
              for yy_c.3 (0,2)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1998	GFLOPS: 8494.85 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669894074.12)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.4 (0,2)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 1999	GFLOPS: 8541.94 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669894075.82)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ry.2 (0,3)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 2000	GFLOPS: 8497.96 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.19, Tstamp:1669894077.71)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for ff_c.3 (0,4)
              for nn_c.4 (0,2)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 2001	GFLOPS: 8502.32 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.07, Tstamp:1669894079.43)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for ff_c.3 (0,4)
                for yy_c.3 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 2002	GFLOPS: 8509.81 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669894081.15)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for ff_c.4 (0,4)
                for yy_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 2003	GFLOPS: 8578.28 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669894082.86)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 2004	GFLOPS: 8521.86 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.25, Tstamp:1669894084.78)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for yy_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 2005	GFLOPS: 8511.84 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.97, Tstamp:1669894086.46)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for yy_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 2006	GFLOPS: 8490.45 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.00, Tstamp:1669894088.14)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for yy_c.3 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 2007	GFLOPS: 8542.09 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.07, Tstamp:1669894089.89)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 2008	GFLOPS: 8564.28 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.21, Tstamp:1669894091.72)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ff_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 2009	GFLOPS: 8571.79 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.98, Tstamp:1669894093.42)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 2010	GFLOPS: 8599.95 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669894095.14)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for ff_c.3 (0,2)
                for yy_c.3 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 2011	GFLOPS: 8564.94 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669894096.88)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for ff_c.3 (0,2)
              for yy_c.3 (0,2)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 2012	GFLOPS: 8466.11 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.10, Tstamp:1669894098.82)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ff_c.3 (0,4)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for rx.2 (0,3)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 2013	GFLOPS: 8526.75 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.08, Tstamp:1669894100.52)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ff_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 2014	GFLOPS: 8541.42 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.00, Tstamp:1669894102.21)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ff_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 2015	GFLOPS: 8553.63 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.97, Tstamp:1669894103.91)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for ff_c.3 (0,2)
            for yy_c.3 (0,2)
              for ry.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 2016	GFLOPS: 8509.87 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.17, Tstamp:1669894105.85)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for ff_c.3 (0,2)
                for yy_c.3 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 2017	GFLOPS: 8515.01 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.96, Tstamp:1669894107.54)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for ff_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 2018	GFLOPS: 8516.14 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.00, Tstamp:1669894109.25)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 2019	GFLOPS: 8525.25 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669894110.95)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for ff_c.3 (0,2)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 2020	GFLOPS: 8519.76 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.32, Tstamp:1669894112.88)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ff_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 2021	GFLOPS: 8525.91 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.99, Tstamp:1669894114.58)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for ff_c.3 (0,2)
              for nn_c.4 (0,2)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 2022	GFLOPS: 8452.97 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.97, Tstamp:1669894116.29)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,4)
              for yy_c.3 (0,2)
                for rx.2 (0,3)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 2023	GFLOPS: 8522.38 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.00, Tstamp:1669894118.02)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 2024	GFLOPS: 8507.98 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.12, Tstamp:1669894119.96)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  for yy_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 2025	GFLOPS: 8562.63 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669894121.67)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,2)
              for rx.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 2026	GFLOPS: 8490.05 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.95, Tstamp:1669894123.41)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,2)
              for ry.2 (0,3)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 2027	GFLOPS: 8549.69 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.89, Tstamp:1669894125.12)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for ff_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 2028	GFLOPS: 8530.52 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.22, Tstamp:1669894127.10)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ff_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 2029	GFLOPS: 8528.21 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669894128.79)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ff_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,4)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 2030	GFLOPS: 8593.27 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.91, Tstamp:1669894130.50)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,4)
              for ff_c.3 (0,2)
                for ff_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 2031	GFLOPS: 8492.92 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.81, Tstamp:1669894132.19)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,4)
              for ry.2 (0,3)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 2032	GFLOPS: 8500.40 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.01, Tstamp:1669894134.09)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,4)
              for ry.2 (0,3)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 2033	GFLOPS: 8575.22 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.90, Tstamp:1669894135.79)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for ff_c.3 (0,4)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 2034	GFLOPS: 8595.88 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.81, Tstamp:1669894137.51)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 2035	GFLOPS: 8577.65 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.85, Tstamp:1669894139.19)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,2)
              for nn_c.4 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 2036	GFLOPS: 8481.65 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.93, Tstamp:1669894141.03)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 2037	GFLOPS: 8476.70 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.81, Tstamp:1669894142.74)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ry.2 (0,3)
              for nn_c.4 (0,2)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 2038	GFLOPS: 8587.87 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.90, Tstamp:1669894144.45)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 2039	GFLOPS: 8581.33 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.84, Tstamp:1669894146.16)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,4)
          for ry.2 (0,3)
            for rx.2 (0,3)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 2040	GFLOPS: 8487.21 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.08, Tstamp:1669894148.16)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,2)
            for ff_c.3 (0,2)
              for ry.2 (0,3)
                for nn_c.4 (0,2)
                  for ff_c.4 (0,2)
                    conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 2041	GFLOPS: 8409.19 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.06, Tstamp:1669894149.88)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          vectorize ax0@ax1@ax2@ax3@.1 (0,3)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,3)
              for rx.2 (0,3)
                for ff_c.4 (0,4)
                  conv2d_nchw.local = ...
      for nn.3 (0,2)
        for ff.3 (0,4)
          for yy.3 (0,2)
            conv2d_nchw = ...

==================================================
No: 2042	GFLOPS: 8500.35 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.06, Tstamp:1669894151.57)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ry.2 (0,3)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 2043	GFLOPS: 8597.18 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.12, Tstamp:1669894153.30)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.2 (0,3)
          for rx.2 (0,3)
            for nn_c.4 (0,4)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 2044	GFLOPS: 8599.11 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.29, Tstamp:1669894155.23)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for ry.1 (0,3)
          for rx.1 (0,3)
            for nn_c.3 (0,4)
              for ff_c.3 (0,4)
                conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 2045	GFLOPS: 8489.20 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.00, Tstamp:1669894156.93)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ry.2 (0,3)
              for ff_c.4 (0,4)
                conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 2046	GFLOPS: 1757.25 / 8625.46	results: MeasureResult(cost:[0.0011], error_no:0, all_cost:3.35, Tstamp:1669894159.13)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1808)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,452)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,4)
        for rx.0 (0,3)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,452)
            kernel.shared = ...
          for ax0@ax1@ax2@ax3@.0.0 (0,9)
            threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,452)
              pad_temp.shared = ...
          for rc.1 (0,8)
            for ry.1 (0,3)
              for nn_c.3 (0,2)
                conv2d_nchw.local = ...
      for nn.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 2047	GFLOPS: 8488.46 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.01, Tstamp:1669894160.87)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,4)
              for ry.2 (0,3)
                conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 2048	GFLOPS: 8486.19 / 8625.46	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.92, Tstamp:1669894162.57)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,904)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,113)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,9)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,113)
            vectorize ax0@ax1@ax2@ax3@.1 (0,2)
              pad_temp.shared = ...
        for rx.1 (0,3)
          for nn_c.3 (0,4)
            for ff_c.3 (0,4)
              for ry.2 (0,3)
                conv2d_nchw.local = ...
      for nn.3 (0,4)
        for ff.3 (0,4)
          conv2d_nchw = ...

Time elapsed for measurement: 125.43 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 1.86 s
----------------------------------------------------------------------
------------------------------  [ Task Scheduler ]
----------------------------------------------------------------------
----------------------------------------------------------------------
------------------------------  [ Search ]
----------------------------------------------------------------------
Sample Iter: 5	#Pop: 31	#Target: 50	fail_ct: 10209	Time elapsed: 4.37
Sample Initial Population	#s: 50	fail_ct: 16334	Time elapsed: 6.98
GA Iter: 0	Max score: 0.6418	Min score: -0.1451	#Pop: 50	#M+: 0	#M-: 0
GA Iter: 4	Max score: 0.9203	Min score: 0.8523	#Pop: 128	#M+: 1403	#M-: 0
EvolutionarySearch		#s: 128	Time elapsed: 15.10
----------------------------------------------------------------------
------------------------------  [ Measure ]
----------------------------------------------------------------------
Get 64 programs to measure:
................................................................****************************************************************==================================================
No: 2049	GFLOPS: 7052.47 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.03, Tstamp:1669894200.59)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for rx.2 (0,5)
              for nn_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2050	GFLOPS: 7057.75 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.11, Tstamp:1669894202.47)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for xx_c.3 (0,2)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2051	GFLOPS: 7026.17 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.90, Tstamp:1669894204.14)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for xx_c.3 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2052	GFLOPS: 7052.78 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.90, Tstamp:1669894205.81)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for xx_c.3 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2053	GFLOPS: 7047.30 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.90, Tstamp:1669894207.69)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for xx_c.3 (0,2)
          for rx.2 (0,5)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2054	GFLOPS: 7030.69 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.14, Tstamp:1669894209.57)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for xx_c.3 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2055	GFLOPS: 7049.97 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.90, Tstamp:1669894211.24)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for nn_c.3 (0,2)
          for xx_c.3 (0,2)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2056	GFLOPS: 7074.61 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.92, Tstamp:1669894212.93)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for yy_c.3 (0,2)
        for xx_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for nn_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2057	GFLOPS: 7013.45 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.91, Tstamp:1669894214.69)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              for nn_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2058	GFLOPS: 7099.25 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.27, Tstamp:1669894216.71)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for xx_c.3 (0,2)
          for rx.2 (0,5)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2059	GFLOPS: 7089.18 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.92, Tstamp:1669894218.39)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for xx_c.3 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2060	GFLOPS: 7082.22 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.90, Tstamp:1669894220.07)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for xx_c.3 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2061	GFLOPS: 7066.73 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.94, Tstamp:1669894221.85)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for xx_c.3 (0,2)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2062	GFLOPS: 6777.91 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.19, Tstamp:1669894223.72)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              for ry.2 (0,5)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2063	GFLOPS: 7058.95 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.92, Tstamp:1669894225.39)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for nn_c.3 (0,2)
        for xx_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2064	GFLOPS: 7051.61 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.93, Tstamp:1669894227.07)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for nn_c.3 (0,2)
        for xx_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2065	GFLOPS: 6911.45 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.90, Tstamp:1669894228.95)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for xx_c.3 (0,2)
          for ry.2 (0,5)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2066	GFLOPS: 6907.58 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.07, Tstamp:1669894230.81)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              for nn_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2067	GFLOPS: 7403.84 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.84, Tstamp:1669894232.47)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for yy_c.3 (0,2)
          for xx_c.3 (0,4)
            for rx.2 (0,5)
              conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,4)
        conv2d_nchw = ...

==================================================
No: 2068	GFLOPS: 6903.84 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.93, Tstamp:1669894234.16)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for xx_c.3 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2069	GFLOPS: 6902.54 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.90, Tstamp:1669894235.93)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for xx_c.3 (0,2)
          for rx.2 (0,5)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2070	GFLOPS: 6926.41 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.20, Tstamp:1669894237.81)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for xx_c.3 (0,2)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2071	GFLOPS: 6899.64 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669894239.47)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for xx_c.3 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for nn_c.4 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2072	GFLOPS: 6790.79 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.93, Tstamp:1669894241.14)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              for ry.2 (0,5)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2073	GFLOPS: 6775.55 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.88, Tstamp:1669894243.02)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              for ry.2 (0,5)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2074	GFLOPS: 6921.36 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669894244.87)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for nn_c.3 (0,2)
          for xx_c.3 (0,2)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2075	GFLOPS: 6917.17 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.84, Tstamp:1669894246.52)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for nn_c.3 (0,2)
        for xx_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2076	GFLOPS: 6903.59 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.87, Tstamp:1669894248.20)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for nn_c.3 (0,2)
          for xx_c.3 (0,2)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2077	GFLOPS: 6895.05 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.94, Tstamp:1669894250.08)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for rx.2 (0,5)
              for nn_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2078	GFLOPS: 6904.20 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.05, Tstamp:1669894251.93)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for ry.2 (0,5)
              for nn_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2079	GFLOPS: 6929.73 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.87, Tstamp:1669894253.59)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for nn_c.3 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2080	GFLOPS: 6912.19 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.88, Tstamp:1669894255.25)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for nn_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2081	GFLOPS: 6905.84 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.86, Tstamp:1669894257.14)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for nn_c.3 (0,2)
            for yy_c.3 (0,2)
              for xx_c.3 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2082	GFLOPS: 6883.21 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.08, Tstamp:1669894259.00)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2083	GFLOPS: 6733.59 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.83, Tstamp:1669894260.66)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for ry.2 (0,5)
              for nn_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2084	GFLOPS: 6770.55 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.86, Tstamp:1669894262.31)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for ry.2 (0,5)
              for nn_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2085	GFLOPS: 6982.14 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.91, Tstamp:1669894264.10)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              for rc.2 (0,2)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 2086	GFLOPS: 6710.29 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.99, Tstamp:1669894265.95)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,15)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for yy_c.3 (0,2)
            for rx.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2087	GFLOPS: 6878.71 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.82, Tstamp:1669894267.61)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2088	GFLOPS: 6762.95 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.84, Tstamp:1669894269.28)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for xx_c.3 (0,2)
            for ry.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2089	GFLOPS: 7028.54 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.84, Tstamp:1669894271.10)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.1 (0,5)
            for xx_c.3 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2090	GFLOPS: 7027.25 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669894272.98)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.1 (0,5)
            for xx_c.3 (0,2)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2091	GFLOPS: 6733.94 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.61, Tstamp:1669894274.63)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for yy_c.3 (0,2)
        for ry.2 (0,5)
          for rx.2 (0,5)
            for nn_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2092	GFLOPS: 6729.06 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.71, Tstamp:1669894276.28)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for rx.1 (0,5)
          for nn_c.4 (0,2)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2093	GFLOPS: 6714.60 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.64, Tstamp:1669894278.16)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,2)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for nn_c.3 (0,2)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2094	GFLOPS: 6727.74 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.02, Tstamp:1669894280.03)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for rx.1 (0,5)
          for yy_c.3 (0,2)
            for xx_c.3 (0,2)
              for ry.2 (0,5)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2095	GFLOPS: 6896.30 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.81, Tstamp:1669894281.70)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2096	GFLOPS: 7012.28 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.80, Tstamp:1669894283.39)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for xx_c.3 (0,2)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2097	GFLOPS: 7028.61 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.81, Tstamp:1669894285.15)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for xx_c.3 (0,2)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2098	GFLOPS: 7021.38 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.04, Tstamp:1669894287.13)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for xx_c.3 (0,2)
          for ry.2 (0,5)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2099	GFLOPS: 6982.29 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.81, Tstamp:1669894288.80)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,2)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for xx_c.3 (0,2)
            for rx.2 (0,5)
              for yy_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2100	GFLOPS: 6725.62 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.78, Tstamp:1669894290.48)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,128)
    conv2d_nchw.local auto_unroll: 512
    for rc.0 (0,16)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,15)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,128)
          pad_temp.shared = ...
      for rc.1 (0,2)
        for yy_c.3 (0,2)
          for xx_c.3 (0,2)
            for ry.2 (0,5)
              for rx.2 (0,5)
                conv2d_nchw.local = ...
    for yy.3 (0,2)
      for xx.3 (0,2)
        conv2d_nchw = ...

==================================================
No: 2101	GFLOPS: 6874.54 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.78, Tstamp:1669894292.25)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2102	GFLOPS: 6874.52 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:4.00, Tstamp:1669894294.13)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2103	GFLOPS: 6886.23 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.66, Tstamp:1669894295.80)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.2 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2104	GFLOPS: 6871.45 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.64, Tstamp:1669894297.46)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for xx_c.3 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2105	GFLOPS: 6886.63 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.86, Tstamp:1669894299.32)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 512
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2106	GFLOPS: 6882.15 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:3.06, Tstamp:1669894301.20)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for xx_c.3 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2107	GFLOPS: 6888.13 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.95, Tstamp:1669894302.87)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2108	GFLOPS: 6909.34 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.93, Tstamp:1669894304.54)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.2 (0,5)
            for yy_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2109	GFLOPS: 6869.17 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.92, Tstamp:1669894306.39)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  vthread nn.1@ff.1@yy.1@xx.1@ (0,2)
    threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
      conv2d_nchw.local auto_unroll: 1024
      for rc.0 (0,32)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          vectorize ax0@ax1@ax2@ax3@.1 (0,10)
            kernel.shared = ...
        for ax0@ax1@ax2@ax3@.0.0 (0,18)
          threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
            pad_temp.shared = ...
        for ry.1 (0,5)
          for rx.1 (0,5)
            for yy_c.3 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2110	GFLOPS: 448.91 / 7652.84	results: MeasureResult(cost:[0.0029], error_no:0, all_cost:2.86, Tstamp:1669894308.20)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,128)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,56)
    conv2d_nchw.local auto_unroll: 16
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
        kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,138)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,56)
          pad_temp.shared = ...
      for ry.1 (0,5)
        for nn_c.3 (0,2)
          for yy_c.3 (0,7)
            for xx_c.3 (0,4)
              for rx.2 (0,5)
                for nn_c.4 (0,2)
                  conv2d_nchw.local = ...
    for nn.3 (0,4)
      for yy.3 (0,7)
        for xx.3 (0,4)
          conv2d_nchw = ...

==================================================
No: 2111	GFLOPS: 6784.72 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.86, Tstamp:1669894309.87)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for nn_c.3 (0,2)
          for yy_c.3 (0,2)
            for ry.2 (0,5)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

==================================================
No: 2112	GFLOPS: 6754.12 / 7652.84	results: MeasureResult(cost:[0.0002], error_no:0, all_cost:2.86, Tstamp:1669894311.54)
==================================================
Placeholder: data, kernel
blockIdx.x nn.0@ff.0@yy.0@xx.0@ (0,1568)
  threadIdx.x nn.2@ff.2@yy.2@xx.2@ (0,64)
    conv2d_nchw.local auto_unroll: 1024
    for rc.0 (0,32)
      threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
        vectorize ax0@ax1@ax2@ax3@.1 (0,10)
          kernel.shared = ...
      for ax0@ax1@ax2@ax3@.0.0 (0,18)
        threadIdx.x ax0@ax1@ax2@ax3@.0.1 (0,64)
          pad_temp.shared = ...
      for rx.1 (0,5)
        for yy_c.3 (0,2)
          for ry.2 (0,5)
            for nn_c.4 (0,2)
              for xx_c.4 (0,2)
                conv2d_nchw.local = ...
    for nn.3 (0,2)
      for yy.3 (0,2)
        for xx.3 (0,2)
          conv2d_nchw = ...

Time elapsed for measurement: 124.91 s
----------------------------------------------------------------------
------------------------------  [ Train cost model ]
----------------------------------------------------------------------
Time elapsed for training: 1.71 s
/home/zly/.local/lib/python3.7/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html
  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)

|    0 |                                                Conv5x5_origin |        0.180 |        7146.00 |    896 |
|    1 |                                                   Conv5x5_opt |        0.218 |        8625.46 |    832 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.398 ms	Trials: 1728	Used time : 4028 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                                Conv5x5_origin |        0.180 |        7146.00 |    896 |
|    1 |                                                   Conv5x5_opt |        0.218 |        8625.46 |    896 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.398 ms	Trials: 1792	Used time : 4172 s	Next ID: 0	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                                Conv5x5_origin |        0.168 |        7652.84 |    960 |
|    1 |                                                   Conv5x5_opt |        0.218 |        8625.46 |    896 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.386 ms	Trials: 1856	Used time : 4324 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                                Conv5x5_origin |        0.168 |        7652.84 |    960 |
|    1 |                                                   Conv5x5_opt |        0.218 |        8625.46 |    960 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.386 ms	Trials: 1920	Used time : 4470 s	Next ID: 0	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                                Conv5x5_origin |        0.168 |        7652.84 |   1024 |
|    1 |                                                   Conv5x5_opt |        0.218 |        8625.46 |    960 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.386 ms	Trials: 1984	Used time : 4617 s	Next ID: 1	

|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |
-----------------------------------------------------------------------------------------------------------------
|    0 |                                                Conv5x5_origin |        0.168 |        7652.84 |   1024 |
|    1 |                                                   Conv5x5_opt |        0.218 |        8625.46 |   1024 |
-----------------------------------------------------------------------------------------------------------------
Estimated total latency: 0.386 ms	Trials: 2048	Used time : 4762 s	Next ID: 0	

